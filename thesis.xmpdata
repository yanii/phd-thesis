% Replace the following information with your document's actual
% metadata. If you do not want to set a value for a certain parameter,
% just omit it.
%
% Symbols permitted in metadata
% =============================
% 
% Within the metadata, all printable ASCII characters except
% '\', '{', '}', and '%' represent themselves. Also, all printable
% Unicode characters from the basic multilingual plane (i.e., up to
% code point U+FFFF) can be used directly with the UTF-8 encoding. 
% Consecutive whitespace characters are combined into a single
% space. Whitespace after a macro such as \copyright, \backslash, or
% \sep is ignored. Blank lines are not permitted. Moreover, the
% following markup can be used:
%
%  '\ '         - a literal space  (for example after a macro)                  
%   \%          - a literal '%'                                                 
%   \{          - a literal '{'                                                 
%   \}          - a literal '}'                                                 
%   \backslash  - a literal '\'                                                 
%   \copyright  - the (c) copyright symbol                                      
%
% The macro \sep is only permitted within \Author, \Keywords, and
% \Org.  It is used to separate multiple authors, keywords, etc.
% 
% List of supported metadata fields
% =================================
% 
% Here is a complete list of user-definable metadata fields currently
% supported, and their meanings. More may be added in the future.
% 
% General information:
%
%  \Author           - the document's human author. Separate multiple
%                      authors with \sep.
%  \Title            - the document's title.
%  \Keywords         - list of keywords, separated with \sep.
%  \Subject          - the abstract. 
%  \Org              - publishers.
% 
% Copyright information:
%
%  \Copyright        - a copyright statement.
%  \CopyrightURL     - location of a web page describing the owner
%                      and/or rights statement for this document.
%  \Copyrighted      - 'True' if the document is copyrighted, and
%                      'False' if it isn't. This is automatically set
%                      to 'True' if either \Copyright or \CopyrightURL
%                      is specified, but can be overridden. For
%                      example, if the copyright statement is "Public
%                      Domain", this should be set to 'False'.
%
% Publication information:
%
% \PublicationType   - The type of publication. If defined, must be
%                      one of book, catalog, feed, journal, magazine,
%                      manual, newsletter, pamphlet. This is
%                      automatically set to "journal" if \Journaltitle
%                      is specified, but can be overridden.
% \Journaltitle      - The title of the journal in which the document
%                      was published. 
% \Journalnumber     - The ISSN for the publication in which the
%                      document was published.
% \Volume            - Journal volume.
% \Issue             - Journal issue/number.
% \Firstpage         - First page number of the published version of
%                      the document.
% \Lastpage          - Last page number of the published version of
%                      the document.
% \Doi               - Digital Object Identifier (DOI) for the
%                      document, without the leading "doi:".
% \CoverDisplayDate  - Date on the cover of the journal issue, as a
%                      human-readable text string.
% \CoverDate         - Date on the cover of the journal issue, in a
%                      format suitable for storing in a database field
%                      with a 'date' data type.
\Title {Structural Priors in Deep Neural Networks}
\PublicationType{journal}
\Author{Yani Andrew Ioannou}
\Publisher{University of Cambridge}
\CoverDisplayDate{September 2017}
\CoverDisplayDate{29/09/2017}
\Journaltitle{Ph.D. Thesis, Department of Engineering, University of Cambridge}
\Copyright{\copyright Yani Ioannou, 2017}
\Copyrighted{True}
\Keywords{Deep Learning\sepNeural Networks\sepComputer Vision\sepStructural Priors}
\Subject{Deep learning has in recent years come to dominate the previously separate fields of research in machine learning, computer vision, natural language understanding and speech recognition. Despite breakthroughs in training deep networks, many relatively simple questions remain unanswered about the representations learned within these networks. In addition this lack of understanding in both the optimization and structure of deep networks has meant that contemporary deep network architectures for image classification have high computational and memory complexity. This is a direct result of the inability to identify the optimal architecture for datasets. 
The approach advocated by many researchers in the field has been to train monolithic networks with excess complexity, and strong regularization - an approach that has, in practice, found success in improving the generalization of deep networks, but leaves much to desire in efficiency. Instead, within this work, we propose that carefully designing networks in consideration of our prior knowledge of the task can improve the memory and compute efficiency of state-of-the art networks, and even increase accuracy through structurally-induced regularization - what we propose to denote as structural priors.

We present two such structural priors for convolutional neural networks, and evaluate them in state-of-the-art image classification architectures. Both methods are found to improve the generalization of these architectures while also decreasing the size and increasing the efficiency of their training and test time computation.

Finally, we begin to explore auchecktomatically finding structural priors in deep networks by presenting our work on adding conditional computation to deep networks. Although results show this is not optimal, the method shows promise in producing a better compute-generalization trade-off than naive approaches to reducing computation in deep networks, hopefully leading towards a better approach to automatically learning structural priors in deep networks.}
\setRGBcolorprofile{macbook.icc}{Color LCD 2013-11-09 sRGB MQ-HQ 3xCurve+MTX}{Color LCD 2013-11-09 sRGB MQ-HQ 3xCurve+MTX}{http://www.argyllcms.com/}