\documentclass[t,xcolor=dvipsnames]{beamer}

%%%%%%%%%%%%%%%%%%%%%%%% Packages/includes
\usepackage[english]{babel}
%\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage{pgfpages}
\usepackage{pgfplotstable}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{epigraph}
% Antonio's macros
\input{../acsettings}
\input{../pgftablehighlight}

\usepackage{tikz}
\usetikzlibrary{arrows,positioning,matrix,scopes,calc,tikzmark,decorations.pathreplacing,shapes,decorations.markings,calc}
\tikzstyle{every picture}+=[remember picture]

%%%%%%%%%%%%%%%%%%%%%%%% Bibliography
\usepackage[backend=bibtex,style=authortitle]{biblatex}
\AtEveryCitekey{\iffootnote{\color{black}\tiny}{\color{black}}}
\addbibresource{../references.bib}

%%%%%%%%%%%%%%%%%%%%%%%% Beamer Mode

%\setbeameroption{hide notes}
%\setbeameroption{show notes}
%\setbeameroption{show notes on second screen=right}


%%%%%%%%%%%%%%%%%%%%%%%% Beamer Options/Settings

\mode<presentation>
{
  \usetheme{default}
  \setbeamercovered{transparent}
}

%\setbeamertemplate{note page}[plain]

% Get rid of the annoying symbol list
\beamertemplatenavigationsymbolsempty
% don't show bookmarks on initial view
\hypersetup{pdfpagemode=UseNone}

% Show page number in lower right
\setbeamertemplate{footline}{%
    \raisebox{5pt}{\makebox[\paperwidth]{\hfill\makebox[20pt]{\color{gray}
          \scriptsize\insertframenumber}}}\hspace*{5pt}}


\addtobeamertemplate{frametitle}{}{%
    \begin{tikzpicture}[remember picture,overlay]
    \node[anchor=north east,yshift=2pt] at (current page.north east) {\includegraphics[height=1em]{uc-cmyk}};
    \end{tikzpicture}}

\definecolor{filtercolor}{RGB}{255,230,153}
\definecolor{filtershade}{RGB}{205,185,123}
\definecolor{fmcolor}{RGB}{231,230,230}
\definecolor{fmshade}{RGB}{186,185,185}

\setbeamercolor{title}{fg=White}
\setbeamercolor{author}{fg=White}
\setbeamercolor{institute}{fg=White}
\setbeamercolor{date}{fg=White}
%\setbeamercolor{section in head/foot}{fg=White}
%\setbeamercolor{author in head/foot}{fg=White}
%\setbeamercolor{date in head/foot}{fg=White}


%%%%%%%%%%%%%%%%%%%%%%%% Title Page Setup
\title[Conditional Computation and Restricted Connectivity in Deep Neural Networks] % (optional, use only with long paper titles)
{Conditional Computation and Restricted Connectivity in Deep Neural Networks}
%\subtitle
%{Improving CNN
%Generalization and
%Efficiency}

\author[Yani Ioannou]
{Yani Ioannou}

\institute[University of Cambridge] % (optional, but mostly needed)
{University of Cambridge}

\date{March 21, 2017}

%\pgfdeclareimage[height=0.5cm]{university-logo}{uc-cmyk}
%\logo{\pgfuseimage{university-logo}}

% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%\AtBeginSubsection[]
%{
%  \begin{frame}<beamer>{Outline}
%    \tableofcontents[currentsection,currentsubsection]
%  \end{frame}
%}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 

%\beamerdefaultoverlayspecification{<+->}


\begin{document}

%%%%%%%%%%%%%%%%%%%% TITLE Frame

\usebackgroundtemplate{%             declare it
\tikz[overlay,remember picture] \node[opacity=0.95, at=(current page.center)] {
   \includegraphics[height=\paperheight,width=\paperwidth]{background}};
}

\begin{frame}
  \titlepage
\end{frame}


\usebackgroundtemplate{%             declare it
\tikz[overlay,remember picture] \node[opacity=0.2, at=(current page.center)] {
   \includegraphics[height=\paperheight,width=\paperwidth]{background}};
}

%\begin{frame}{Outline}
%  \tableofcontents
  % You might wish to add the option [pausesections]
%\end{frame}


% Since this a solution template for a generic talk, very little can
% be said about how it should be structured. However, the talk length
% of between 15min and 45min and the theme suggest that you stick to
% the following rules:  

% - Exactly two or three sections (other than the summary).
% - At *most* three subsections per section.
% - Talk about 30s to 2min per frame. So there should be between about
%   15 and 30 frames, all told.

\section{Introduction}
\begin{frame}{About Me}
\begin{itemize}
\item Ph.D.\ student in the Department of Engineering at the University of Cambridge.
%\item Funded by a Microsoft Research PhD Scholarship
\item Supervised by Professor Roberto Cipolla, head of the Computer Vision and Robotics group in the Machine Intelligence Lab, and Dr. Antonio Criminisi, a principal researcher at Microsoft Research.
\end{itemize}
\centering
\includegraphics[width=0.207\linewidth]{antonio}~
\includegraphics[width=0.2\linewidth]{roberto}
\end{frame}

\usebackgroundtemplate{}

\section{Areas of Research}
\begin{frame}{Collaborative Research @ MSR}
\begin{itemize}
    \item Segmentation of brain tumor tissues with convolutional neural networks.\\{\footnotesize D.\ Zikic, Y.\ Ioannou, M.\ Brown, A.\ Criminisi.\\MICCAI-BRATS 2014}
    \item Measuring Neural Net Robustness with Constraints.\\{\footnotesize O.\ Bastani, Y.\ Ioannou, L.\ Lampropoulos, D\ Vytiniotis, A.\ Nori, A.\ Criminisi.\\NIPS 2016}
    \item Refining Architectures of Deep Convolutional Neural Networks.\\{\footnotesize 
S.\ Shankar, D.\ Robertson, Y.\ Ioannou, A.\ Criminisi, R.\ Cipolla.\\CVPR 2016}
\end{itemize}
\end{frame}

\begin{frame}{Collaborative Research @ MSR}
\textbf{Segmentation of brain tumor tissues with convolutional neural networks.}\\{\footnotesize D.\ Zikic, Y.\ Ioannou, M.\ Brown, A.\ Criminisi.\\MICCAI-BRATS 2014}
\begin{itemize}
    \item One of the first papers using deep learning for volumetric/medical imagery
    \item Biggest challenge for training was the class-imbalanced training data
\end{itemize}
\end{frame}

\begin{frame}{Collaborative Research @ MSR}
\textbf{Measuring Neural Net Robustness with Constraints.}\\{\footnotesize O.\ Bastani, Y.\ Ioannou, L.\ Lampropoulos, D\ Vytiniotis, A.\ Nori, A.\ Criminisi.\\NIPS 2016}
\begin{itemize}
    \item Showed that contemporary methods of finding adversarial examples were limited in their coverage
    \item Found that not all adversarial images can be used to improve network robustness
\end{itemize}
\end{frame}

\begin{frame}{Collaborative Research @ MSR}
\textbf{Refining Architectures of Deep Convolutional Neural Networks.}\\{\footnotesize S.\ Shankar, D.\ Robertson, Y.\ Ioannou, A.\ Criminisi, R.\ Cipolla.\\CVPR 2016}
\begin{itemize}
    \item A method for adapting network architectures optimized for large datasets for smaller datasets
    \item Attempts to answer the common problem of adapting contemporary research to real-world problems
\end{itemize}
\end{frame}

\begin{frame}{First Author Publications during PhD}
\begin{itemize}
    \item Decision Forests, Convolutional Networks and the Models in-Between.\\{\footnotesize Y.\ Ioannou, D.\ Robertson, D.\ Zikic, P.\ Kontschieder, J.\ Shotton, M.\ Brown, A.\ Criminisi.\\MSR Technical Report} 
    \item Training CNNs with Low-Rank Filters for Efficient Image Classification.\\{\footnotesize Y.\ Ioannou, D.\ Robertson, J.\ Shotton, R.\ Cipolla, A.\ Criminisi. \\ICLR 2016}
    \item Deep roots: Improving CNN efficiency with hierarchical filter groups.\\{\footnotesize Y.\ Ioannou, D.\ Robertson, R.\ Cipolla, A.\ Criminisi.\\CVPR 2017}
    
\end{itemize}
\end{frame}

\section{Motivation}

%%%%%%%%%%%%%%%%%%%%

\begin{frame}{ILSVRC}{Imagenet Large-Scale Visual Recognition Challenge}

\begin{figure}
    \includegraphics[width=0.4\textwidth]{imagenetlogo}
\end{figure}
\begin{itemize}
    \item Imagenet Large-Scale Visual Recognition Challenge\footcite{ILSVRC2015}.
    \item 1.2 Million Training Images, 1000 classes.
    \item 50,000 image validiation/test set.
    \begin{itemize}
        \item In 2012 Alex Krizhevsky won challenge with CNN\footcite{Krizhevsky2012}.
        \item `AlexNet' was 26.2\% better than second best, 15.3\%.
    \end{itemize}
    \item State-of-the-art beats human error (5\%).
\end{itemize}    
\end{frame}

%%%%%%%%%%%%%%%%%%%%

\begin{frame}{AlexNet Complexity}
\includegraphics[width=\columnwidth]{alexnet}
\begin{itemize}
\item $\approx$ 61 million parameters %60,965,224
\item $\approx$ 724 million FLOPS (per-sample) % 724,417,384
\item Imagenet has 1.28 million training samples ($227 \times 227 \times 3$) %227×227×3×1281167
\item Images of dimensions  ($227 \times 227 \times 3$) $\approx$ 200 billion pixels % %227×227×3×1281167 = 198,051,763,029
\end{itemize}
\end{frame}

\begin{frame}{AlexNet Complexity - FLOPS}
\centering
\includegraphics[width=0.9\columnwidth]{alexnet}
\tikzstyle{every node}=[font=\tiny]
\pgfplotstableread[col sep=comma]{alexnetma.csv}\datatable
\begin{tikzpicture}
\begin{axis}[
    axis x line=bottom,
    axis y line=left,
    ybar=0pt,
    bar width=1em,
    width=\linewidth,
    height=0.4\linewidth,
    enlarge x limits=0.1,
    ylabel=FLOPS,
    y label style={at={(axis description cs:0.09,.5)},anchor=south},
    y tick label style={
        /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
        /tikz/.cd
    },
    ymin=0,
    xticklabels from table={\datatable}{layer},
    xticklabel style = {rotate = 90, xshift = -0.8ex, anchor = mid east, font=\tiny},
    xtick=data,
]
\addplot[ybar, draw=none, fill=red!40] table [x expr=\coordindex,y=ma]{\datatable};
\end{axis}
\end{tikzpicture}
\end{frame}

\begin{frame}{AlexNet Complexity - Parameters}
\centering
\includegraphics[width=0.9\columnwidth]{alexnet}
\tikzstyle{every node}=[font=\tiny]
\pgfplotstableread[col sep=comma]{alexnetma.csv}\datatable
\begin{tikzpicture}
\begin{axis}[
    axis x line=bottom,
    axis y line=left,
    ybar=0pt,
    bar width=1em,
    width=\linewidth,
    height=0.4\linewidth,
    enlarge x limits=0.1,
    ylabel=Parameters,
    y label style={at={(axis description cs:0.09,.5)},anchor=south},
    y tick label style={
        /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
        /tikz/.cd
    },
    ymin=0,
    xticklabels from table={\datatable}{layer},
    xticklabel style = {rotate = 90, xshift = -0.8ex, anchor = mid east, font=\tiny},
    xtick=data,
]
\addplot[ybar, draw=none, fill=red!40] table [x expr=\coordindex,y=param]{\datatable};
\end{axis}
\end{tikzpicture}
$\approx$ 96\% in fully connected layers
%0.961714567
%0.038285433
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}{AlexNet Complexity}
\begin{figure}[tbp]
\centering
\tikzstyle{every node}=[font=\tiny]
\pgfplotstableread[col sep=comma]{alexnetma.csv}\datatable
\begin{tikzpicture}
\begin{axis}[
    axis x line=bottom,
    axis y line=left,
    ybar=0pt,
    bar width=1em,
    width=0.9\linewidth,
    height=0.33\linewidth,
    enlarge x limits=0.1,
    ylabel=FLOPS,
    y label style={at={(axis description cs:0.09,.5)},anchor=south},
    y tick label style={
        /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
        /tikz/.cd
    },
    ymin=0,
    xmajorticks=false,
    legend style={at={(0.5,1.2)},
    draw=none, anchor=north,legend columns=-1},
    area legend,
]
\addplot[ybar, draw=none, fill=red!40] table [x expr=\coordindex,y=ma]{\datatable};
\end{axis}
\end{tikzpicture}
~
\begin{tikzpicture}
\begin{axis}[
    axis x line=bottom,
    axis y line=left,
    ybar=0pt,
    bar width=1em,
    width=0.9\linewidth,
    height=0.33\linewidth,
    enlarge x limits=0.1,
    ylabel=Parameters,
    y label style={at={(axis description cs:0.09,.5)},anchor=south},
    y tick label style={
        /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
        /tikz/.cd
    },
    ymin=0,
    xticklabels from table={\datatable}{layer},
    xticklabel style = {rotate = 90, xshift = -0.8ex, anchor = mid east, font=\tiny},
    xtick=data,
]
\addplot[ybar, draw=none, fill=red!40] table [x expr=\coordindex,y=param]{\datatable};
\end{axis}
\end{tikzpicture}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Hinton Philosophy}
% http://sms.cam.ac.uk/media/2017973?format=mpeg4&quality=720p
    %\item Standard practice: Create a model of appropriate size for our data and the problem at hand.
    %\item Hinton philosophy on neural network design: Create a massively overparameterized network, and regularize like crazy to prevent overfitting.
    %\item Why is the latter needed in practice?
    %\item ``If you give me any sized dataset, what you ought to do if you want good generalization is get yourself into the small data regime. That is, however big your dataset, you ought to make a much bigger model so that that's small data. So, I think what the brain is doing is making sure that big data is small data and then regularizing the hell out of it, and that's a better thing to do than what statisticians used to think you should do, which is have a small model. And you can think of it as, it's just and efficient way of averaging many, many small models, which is a good way to use argback??, but something a bit more efficient than just doing a normal ensemble. So you should always, if you can, be in the small data regime by having a really big computer. And that's certainly where we live, because we have a limited lifetime so the amount of data we get is limited.''

\begin{quote}
``If you give me any sized dataset, what you ought to do \textbf{if you want good generalization} is get yourself into the small data regime. That is, however big your dataset, you ought to \textbf{make a much bigger model} so that that's small data.''\\

``So, I think what the brain is doing is \ldots \textbf{regularizing the hell} out of it, and that's a better thing to do than what statisticians used to think you should do, which is have a small model.'' \\
\flushright{\normalfont Geoffery Hinton\\\footnote{\url{http://sms.cam.ac.uk/media/2017973}}Cambridge, June 2015}
\end{quote}
\end{frame}

%%%%%%%%%%%%%%%%%%%%

\pgfplotstableread[col sep=comma]{../lrdata/bigpicture.csv}\datatable
\pgfplotstableread[col sep=comma]{../lrdata/bigpicture_ours.csv}\datatableours
\pgfplotstableread[col sep=comma]{../lrdata/bigpicture_aug.csv}\datatableaug
\pgfplotsset{major grid style={dotted,red}}
\pgfplotsset{minor grid style={dotted,red}}

\begin{frame}{}
\resizebox {\textwidth} {!} {
\begin{tikzpicture}
\begin{axis}[
  width=1.2\textwidth,
  height=1.1\textheight,
  axis x line=bottom,
  ylabel=Top-5 Error,
  xlabel=$\log_{10}$(Multiply-Accumulate Operations),
  axis lines=left,
  enlarge x limits=0.05,
  enlarge y limits=0.05,
  grid=both,
  ytick={0.00,0.02,...,0.22},
  xmode=log, 
  %ymin=0,ymax=0.2,
  %xmin=10e8,xmax=10e10,
  yticklabel={\pgfmathparse{\tick*100}\pgfmathprintnumber{\pgfmathresult}\%},style={
        /pgf/number format/fixed,
        /pgf/number format/precision=1
  },
  legend style={at={(0.01,0.01)},anchor=south west},
]
%\addplot[mark=*,mark options={fill=green!70!black},nodes near coords,only marks,
%   point meta=explicit symbolic,
%   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
%] table[meta=Network,x=Multiply-Acc.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatableours};
\addplot[mark=*,mark options={fill=blue},nodes near coords,only marks,
   point meta=explicit symbolic,
   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
] table[meta=Network,x=Multiply-Acc.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatable};
\addplot[mark=square*,mark options={fill=red},nodes near coords,only marks,
   point meta=explicit symbolic,
   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
] table[meta=Network,x=Test Multiply-Acc.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatableaug};
\legend{Crop \& Mirror Aug., Extra Augmentation}
\end{axis}
\end{tikzpicture}
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%

\begin{frame}{}
\begin{figure}
\centering
\pgfplotstableread[col sep=comma]{../lrdata/bigpicture.csv}\datatable
\pgfplotstableread[col sep=comma]{../lrdata/bigpicture_ours.csv}\datatableours
\pgfplotstableread[col sep=comma]{../lrdata/bigpicture_aug.csv}\datatableaug
\pgfplotsset{major grid style={dotted,red}}
\pgfplotsset{minor grid style={dotted,red}}

\resizebox {\textwidth} {!} {
\begin{tikzpicture}
\begin{axis}[
  width=1.2\textwidth,
  height=1.1\textheight,
  axis x line=bottom,
  ylabel=Top-5 Error,
  xlabel=$\log_{10}$(Number of Parameters),
  axis lines=left,
  enlarge y limits=0.05,
  grid=both,
  ytick={0.01,0.02,...,0.2},
  xmode=log,
  xmin=10e5,xmax=10e8,
  yticklabel={\pgfmathparse{\tick*100}\pgfmathprintnumber{\pgfmathresult}\%},style={
        /pgf/number format/fixed,
        /pgf/number format/precision=1
  },
  legend style={at={(0.01,0.01)},anchor=south west},
]
%\addplot[mark=*,mark options={fill=green!70!black},
%   nodes near coords,
%   only marks,
%   point meta=explicit symbolic,
%   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
%] table[meta=Network,x=Param.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatableours};
\addplot[mark=*,mark options={fill=blue},
   nodes near coords,
   only marks,
   point meta=explicit symbolic,
   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
] table[meta=Network,x=Param.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatable};
\addplot[mark=square*,mark options={fill=red},
   nodes near coords,
   only marks,
   point meta=explicit symbolic,
   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
   every node near coord/.append style={font=\tiny},
] table[meta=Network,x=Param.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatableaug};
\legend{Crop \& Mirror Aug., Extra Augmentation}
\end{axis}
\end{tikzpicture}
}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Hinton Philosophy}
% http://sms.cam.ac.uk/media/2017973?format=mpeg4&quality=720p
\begin{itemize}
    \item Hinton philosophy on neural network design: Create a massively over-parameterized network, and ``regularize like hell'' to prevent over-fitting.
    \item This seems to work well in practice for networks such as AlexNet, VGG, and MSRA
    \item But there is something that feels fundamentally wrong about this (and I'm not a statistician!)
    \item This feeling is backed up not only by my work, but by recent work showing that regularization in deep networks is not as effective as the community thought.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Alternative Philosophy}
\begin{columns}[onlytextwidth,T]
\begin{column}{0.65\textwidth}
\begin{itemize}
    \item Deep networks need many more parameters than data points because they aren't just learning to model data, but also learning what \emph{\color{blue}not} to learn. 
    %\item At each level in a CNN, the transformed data exists both as weights and spatial/channel information.
    %\begin{itemize}
    %    \item Isn't this what regularization is for? Yes, but it's more like using a sledgehammer on a small nail.
    %\end{itemize}
    % Sort of like regularization, but regularization is like using a sledge hammer on a nail, it affect a lot more than what you want
    \item Idea: Why don't we help the network, through structural priors, not to learn things it doesn't need to?
    %\item Let's separate learning into two distinct functions:
    %\begin{itemize}
    %    \item Learning weights such that a filter responds to a stimulus
    %    \item Learning what to ignore
    %\end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.34\textwidth}
\includegraphics[width=\textwidth]{statue}\\
\tiny \flushright 
The Atlas Slave\\(Accademia, Florence)
\end{column}
\end{columns}
\end{frame}
%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Typical Convolutional Layer}
\begin{figure}
   \includegraphics[width=0.9\textwidth, page=1]{../Figs/PDF/groupfig}
%   \caption{A full rank convolutional layer.}
\end{figure}
    Best example of this idea is a CNN!
    \begin{itemize}
        \item Shared parameters: filter learned for one part of the image should apply to the whole image.
        \item Filters: natural images are highly correlated locally
    \end{itemize}
    %\item A fully-connected network could theoretically learn the same thing, but in practice it never will.

    \note[item]{We show here a typical convolutional layer.}
    \note[item]{Yellow blocks are filters.}
    \note[item]{Grey blocks are images or feature maps.}
\end{frame}

%%%%%%%%%%%%%%%%%

\begin{frame}{Typical Convolutional Layer}
\begin{figure}
   \includegraphics[width=0.9\textwidth, page=1]{../Figs/PDF/groupfig}
%   \caption{A full rank convolutional layer.}
\end{figure}
\begin{figure}

   \resizebox {0.5\textwidth} {!} {
   \begin{tikzpicture}[
       decoration={
          markings,
          mark=at position 1 with {\arrow[scale=2,gray]{latex}};
        }]
        % draw featuremap
        \pgfmathsetmacro{\cubex}{2}
        \pgfmathsetmacro{\cubey}{2}
        \pgfmathsetmacro{\cubez}{1.5}
        \draw[black,fill=fmcolor] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
        \draw[black,fill=fmshade] (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
        \draw[black,fill=fmcolor] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
        \draw (-0.7, -2.5) node {image/feature map};
        
        \draw (1, -0.7) node {\LARGE$*$};
        
        % draw filter
        \pgfmathsetmacro{\cubex}{0.3}
        \pgfmathsetmacro{\cubey}{0.3}
        \pgfmathsetmacro{\cubez}{1.5}
        \draw[black,fill=filtercolor] (2,-0.7,0) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
        \draw[black,fill=filtershade] (2,-0.7,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
        \draw[black,fill=filtercolor] (2,-0.7,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
        \draw (2.2, -2.5) node {filter};
            
        % draw output featuremap
        \pgfmathsetmacro{\cubex}{2}
        \pgfmathsetmacro{\cubey}{2}
        \pgfmathsetmacro{\cubez}{0.1}
        \draw[black,fill=fmcolor] (6,0,-0.75) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
        \draw[black,fill=fmshade] (6,0,-0.75) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
        \draw[black,fill=fmcolor] (6,0,-0.75) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
        \draw (-0.7, -2.5) node {image/feature map};
        
        \draw[gray,postaction={decorate}] (2.7,-0.7) -- (3.8, -0.7);
        
        \draw (5.2, -2.5) node {output featuremap};
    \end{tikzpicture}
    }
\end{figure}
    \note[item]{We show here a typical convolutional layer.}
    \note[item]{Yellow blocks are filters.}
    \note[item]{Grey blocks are images or feature maps.}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Sparsity of Convolution}
\begin{figure}
    \includegraphics[width=\textwidth]{../Figs/PDF/sparseconn3}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\section{Spatial Structural Priors}

\usebackgroundtemplate{%             declare it
\tikz[overlay,remember picture] \node[opacity=0.7, at=(current page.center), yshift=-7cm] {
   \setlength{\fboxsep}{0pt}\fbox{
   \includegraphics[width=0.9\paperwidth,page=1]{lrpaper.pdf}
   }
   };
}

\begin{frame}
\vfill
\centering
%\begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
\usebeamerfont{title}Spatial Structural Priors\par%
%\end{beamercolorbox}
\vfill
%Chiseling out the spatial
\end{frame}

\usebackgroundtemplate{}

%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Convolution is Expensive}{Can we use Separable Filters?}
\begin{itemize}
    \item In image processing, we can often use separable filters instead.
    \begin{itemize}
        \item Factorizes a $k\times k$ filter into $1\times k$ and $k \times 1$ smaller filters.
        \item Only works if we can factorize a matrix into such an outer product $\Rightarrow$ rank-1 matrix    \item \eg $\begin{bmatrix} 1 \\ 2 \\ 1\end{bmatrix} \begin{bmatrix} -1 & 0 & 1\end{bmatrix}  = \begin{bmatrix} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1\end{bmatrix}$.
    \end{itemize}
    
    % The following outlook is optional.
    \vskip0pt plus.5fill
    \item Much faster - $\mathcal{O}(k+k)$ \vs $\mathcal{O}(k^2)$.
    \note[item]{We can do this because convolution is associative, \ie $f*(v*h) = (f*v)*h$.}
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Separable Layer}
\vspace{-1em}
\begin{itemize}
    \item Use sequential filters of differing orientation\footcite{mamalet2012simplifying,journals/corr/JaderbergVZ14}.
\end{itemize}
\begin{figure}
    \begin{tikzpicture}
        \node at (0, 4.2) (n1) {
            \includegraphics[width=0.4\textwidth, page=1]{../Figs/PDF/sparsification}
        };
        \node at (0, 1) (t1) {
            \includegraphics[width=0.9\textwidth, page=2]{../Figs/PDF/sparsification}
        };
        \path[->]<1-> (n1) edge (t1);
    \end{tikzpicture}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Separable Layer -- Issues}
\begin{figure}
\includegraphics[width=\textwidth, page=2]{../Figs/PDF/sparsification}
%   \caption{Sequential separable filters.}
%   \label{fig:separableseq}
\end{figure}
\begin{itemize}
    \item Order of filter orientations gives different results (!).
    \note[item]{Vertical first was more accurate\footcite{mamalet2012simplifying}}
    \note[item]{Second orientation has $64\times$ the param, overfits differently?}
    \item Complexity of each filter:
    \begin{itemize}
        \item before: filter is $\mathcal{O}(d \times [h\times w \times c])$.
        \item after: $\mathcal{O}(d\times [h\times \mathbf{m}] + \mathbf{m} [w \times c])$,
    \end{itemize}
    \item However, in practice was not trainable for large networks.
    \item New initialization proposed allows us to train this.
    \note[item]{Get vanishing/exploding gradients if we tried to train these networks.}
    \note[item]{in most CNNs, $d \geq m \gg c$, so still not that cheap.}
    \note[item]{Second orientation has $64\times$ the param, so much more expensive that you'd think.}
    \note[item]{Still has compute savings.}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Proposed Method}{Learning a basis space for filters.}
\begin{figure}
   \includegraphics[width=\textwidth, page=4]{../Figs/PDF/sparsification}
\end{figure}
\begin{itemize}
    \item A learned basis space of vertical/horizontal rectangular filters and square filters!
    \item Shape of learned filters is a full $w \times h \times c$.
    \item But what can be effectively learned is limited by the number and complexity of the components.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\pgfplotstableread[col sep=comma]{../lrdata/bigpicture.csv}\datatable
\pgfplotstableread[col sep=comma]{../lrdata/bigpicture_ours.csv}\datatableours
\pgfplotstableread[col sep=comma]{../lrdata/bigpicture_aug.csv}\datatableaug
\pgfplotsset{major grid style={dotted,red}}
\pgfplotsset{minor grid style={dotted,red}}

\begin{frame}{State-of-the-Art Models}%{The Cost of Increasing Accuracy}
\resizebox {\textwidth} {!} {
\begin{tikzpicture}
\begin{axis}[
  width=1.2\textwidth,
  height=1.1\textheight,
  axis x line=bottom,
  ylabel=Top-5 Error,
  xlabel=$\log_{10}$(Multiply-Accumulate Operations),
  axis lines=left,
  enlarge x limits=0.05,
  enlarge y limits=0.05,
  grid=both,
  ytick={0.00,0.02,...,0.22},
  xmode=log, 
  %ymin=0,ymax=0.2,
  %xmin=10e8,xmax=10e10,
  yticklabel={\pgfmathparse{\tick*100}\pgfmathprintnumber{\pgfmathresult}\%},style={
        /pgf/number format/fixed,
        /pgf/number format/precision=1
  },
  legend style={at={(0.01,0.01)},anchor=south west},
]
\addplot[mark=*,mark options={fill=blue},nodes near coords,only marks,
   point meta=explicit symbolic,
   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
] table[meta=Network,x=Multiply-Acc.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatable};
\addplot[mark=square*,mark options={fill=red},nodes near coords,only marks,
   point meta=explicit symbolic,
   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
] table[meta=Network,x=Test Multiply-Acc.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatableaug};
\addplot[mark=*,mark options={fill=green!70!black},nodes near coords,only marks,
   point meta=explicit symbolic,
   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
] table[meta=Network,x=Multiply-Acc.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatableours};
\legend{Crop \& Mirror Aug., Extra Augmentation, Our Results}
\end{axis}
\end{tikzpicture}
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%

\begin{frame}{}
\begin{figure}
\centering
\pgfplotstableread[col sep=comma]{../lrdata/bigpicture.csv}\datatable
\pgfplotstableread[col sep=comma]{../lrdata/bigpicture_ours.csv}\datatableours
\pgfplotstableread[col sep=comma]{../lrdata/bigpicture_aug.csv}\datatableaug
\pgfplotsset{major grid style={dotted,red}}
\pgfplotsset{minor grid style={dotted,red}}

\resizebox {\textwidth} {!} {
\begin{tikzpicture}
\begin{axis}[
  width=1.2\textwidth,
  height=1.1\textheight,
  axis x line=bottom,
  ylabel=Top-5 Error,
  xlabel=$\log_{10}$(Number of Parameters),
  axis lines=left,
  enlarge y limits=0.05,
  grid=both,
  ytick={0.01,0.02,...,0.2},
  xmode=log,
  xmin=10e5,xmax=10e8,
  yticklabel={\pgfmathparse{\tick*100}\pgfmathprintnumber{\pgfmathresult}\%},style={
        /pgf/number format/fixed,
        /pgf/number format/precision=1
  },
  legend style={at={(0.01,0.01)},anchor=south west},
]
\addplot[mark=*,mark options={fill=blue},
   nodes near coords,
   only marks,
   point meta=explicit symbolic,
   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
] table[meta=Network,x=Param.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatable};
\addplot[mark=square*,mark options={fill=red},
   nodes near coords,
   only marks,
   point meta=explicit symbolic,
   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
   every node near coord/.append style={font=\tiny},
] table[meta=Network,x=Param.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatableaug};
\addplot[mark=*,mark options={fill=green!70!black},
   nodes near coords,
   only marks,
   point meta=explicit symbolic,
   every node near coord/.append style={xshift=0.01em, anchor=west, font=\tiny},
] table[meta=Network,x=Param.,y expr={1 - \thisrow{Top-5 Acc.} }]{\datatableours};
\legend{Crop \& Mirror Aug., Extra Augmentation, Our Results}
\end{axis}
\end{tikzpicture}
}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%

\section{Channel-wise Structural Priors}

\usebackgroundtemplate{%             declare it
\tikz[overlay,remember picture] \node[opacity=0.7, at=(current page.center), yshift=-7cm] {
   \setlength{\fboxsep}{0pt}\fbox{
   \includegraphics[width=0.9\paperwidth,page=1]{deeproots.pdf}
   }
   };
}

\begin{frame}
\vfill
\centering
%\begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
\usebeamerfont{title}Channel-wise Structural Priors\par%
%\end{beamercolorbox}
\vfill
\end{frame}

\usebackgroundtemplate{}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Typical Convolutional Layer}
\begin{figure}
   \includegraphics[width=0.9\textwidth, page=1]{../Figs/PDF/groupfig}
%   \caption{A full rank convolutional layer.}
\end{figure}
\begin{figure}

   \resizebox {0.5\textwidth} {!} {
   \begin{tikzpicture}[
       decoration={
          markings,
          mark=at position 1 with {\arrow[scale=2,gray]{latex}};
        }]
        % draw featuremap
        \pgfmathsetmacro{\cubex}{2}
        \pgfmathsetmacro{\cubey}{2}
        \pgfmathsetmacro{\cubez}{1.5}
        \draw[black,fill=fmcolor] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
        \draw[black,fill=fmshade] (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
        \draw[black,fill=fmcolor] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
        \draw (-0.7, -2.5) node {image/feature map};
        
        \draw (1, -0.7) node {\LARGE$*$};
        
        % draw filter
        \pgfmathsetmacro{\cubex}{0.3}
        \pgfmathsetmacro{\cubey}{0.3}
        \pgfmathsetmacro{\cubez}{1.5}
        \draw[black,fill=filtercolor] (2,-0.7,0) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
        \draw[black,fill=filtershade] (2,-0.7,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
        \draw[black,fill=filtercolor] (2,-0.7,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
        \draw (2.2, -2.5) node {filter};
            
        % draw output featuremap
        \pgfmathsetmacro{\cubex}{2}
        \pgfmathsetmacro{\cubey}{2}
        \pgfmathsetmacro{\cubez}{0.1}
        \draw[black,fill=fmcolor] (6,0,-0.75) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
        \draw[black,fill=fmshade] (6,0,-0.75) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
        \draw[black,fill=fmcolor] (6,0,-0.75) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
        \draw (-0.7, -2.5) node {image/feature map};
        
        \draw[gray,postaction={decorate}] (2.7,-0.7) -- (3.8, -0.7);
        
        \draw (5.2, -2.5) node {output featuremap};
    \end{tikzpicture}
    }
\end{figure}
    \note[item]{We show here a typical convolutional layer.}
    \note[item]{Yellow blocks are filters.}
    \note[item]{Grey blocks are images or feature maps.}
\end{frame}

%%%%%%%%%%%%%%%%%%%
\begin{frame}{AlexNet Filter Grouping}
\begin{figure}
\includegraphics[width=\columnwidth]{alexnet}
\end{figure}
\begin{itemize}
\item Uses 2 filter groups in most of the convolutional layers
\item Allowed training across two GPUs (model parallelism)
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%

\begin{frame}{AlexNet Filter Grouping}
	\centering
    \begin{figure}
    \includegraphics[width=0.9\columnwidth]{alexnet}
    \end{figure}
	\pgfplotstableread[col sep=comma]{../rootdata/alexnetma.csv}\datatable
	\pgfplotsset{major grid style={dotted,red}}
	
	\begin{tikzpicture}
	\begin{axis}[
	width=\linewidth,
	height=0.4\linewidth,
	axis x line=bottom,
	ylabel=Top-5 Val.\ Error,
	xlabel=Model Parameters (\# floats),
	axis lines=left,
	enlarge x limits=0.12,
	enlarge y limits=0.1,
	grid=major,
	%xmin=0,
	ytick={0.01,0.02,...,0.21},
	ymin=0.18,ymax=0.2,
	yticklabel={\pgfmathparse{\tick*100}\pgfmathprintnumber{\pgfmathresult}\%},style={
		/pgf/number format/fixed,
		/pgf/number format/precision=1
	},
	legend style={at={(0.98,0.98)}, anchor=north east, column sep=0.5em},
	legend columns=2,
	]
	\addplot[mark=*,mark options={fill=black},nodes near coords,only marks,
	point meta=explicit symbolic,
	] table[meta=Network,x=Param.,y expr={1 - \thisrow{Top-5 Acc.} },]{\datatable};
	\end{axis}
	\end{tikzpicture}
	\end{frame}

%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Grouped Convolutional Layer}
\begin{figure}
   \includegraphics[width=0.9\textwidth, page=2]{../Figs/PDF/groupfig}
%   \caption{A full rank convolutional layer.}
\end{figure}
\begin{figure}

   \resizebox {0.5\textwidth} {!} {
   \begin{tikzpicture}[
       decoration={
          markings,
          mark=at position 1 with {\arrow[scale=2,gray]{latex}};
        }]
        % draw featuremap
        \pgfmathsetmacro{\cubex}{2}
        \pgfmathsetmacro{\cubey}{2}
        \pgfmathsetmacro{\cubez}{1.5}
        \draw[black,fill=fmcolor] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
        \draw[black,fill=fmshade] (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
        \draw[black,fill=fmcolor] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
        \draw (-0.7, -2.5) node {image/feature map};
        
        \draw (1, -0.7) node {\LARGE$*$};
        
        % draw filter
        \pgfmathsetmacro{\cubex}{0.3}
        \pgfmathsetmacro{\cubey}{0.3}
        \pgfmathsetmacro{\cubez}{1.5}
        \draw[black,fill=filtercolor] (2,-0.7,0) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
        \draw[black,fill=filtershade] (2,-0.7,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
        \draw[black,fill=filtercolor] (2,-0.7,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
        \draw (2.2, -2.5) node {filter};
            
        % draw output featuremap
        \pgfmathsetmacro{\cubex}{2}
        \pgfmathsetmacro{\cubey}{2}
        \pgfmathsetmacro{\cubez}{0.1}
        \draw[black,fill=fmcolor] (6,0,-0.75) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
        \draw[black,fill=fmshade] (6,0,-0.75) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
        \draw[black,fill=fmcolor] (6,0,-0.75) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
        \draw (-0.7, -2.5) node {image/feature map};
        
        \draw[gray,postaction={decorate}] (2.7,-0.7) -- (3.8, -0.7);
        
        \draw (5.2, -2.5) node {output featuremap};
    \end{tikzpicture}
    }
\end{figure}
    \note[item]{We show here a grouped convolutional layer.}
    \note[item]{Yellow blocks are filters.}
    \note[item]{Grey blocks are images or feature maps.}
\end{frame}

%%%%%%%%%%%%%%%%%%%
\begin{frame}{Network-in-Network}
            \centering
			\begin{tikzpicture}[ampersand replacement=\&]
			\begin{scope}[]
			\matrix[column sep=0em]{
				\node (1a) {
					\includegraphics[height=0.12\linewidth, page=15]{../Figs/PDF/groupfig}
				};\&
				\node (1b) {
					\includegraphics[height=0.145\linewidth, page=17]{../Figs/PDF/groupfig}
				};\& 
				\node (1c) {
					\includegraphics[height=0.145\linewidth, page=17]{../Figs/PDF/groupfig}
				};\&
				\node (1cdots) {
					{\Large $\cdots$}
				};\\
				\draw node{{\footnotesize \textit{input}} \hspace{0.05em} {\footnotesize \textit{conv1a}}};\&
				\draw node{\footnotesize \textit{conv1b}};\&
				\draw node{\footnotesize \textit{conv1c}};\\
				\node (2adots) {
					{\Large $\cdots$}
				};\&
				\node (2a) {
				    \includegraphics[height=0.12\linewidth, page=16]{../Figs/PDF/groupfig}
				};\&
				\node (2b) {
					\includegraphics[height=0.145\linewidth, page=17]{../Figs/PDF/groupfig}
				};\&
				\node (2c) {
					\includegraphics[height=0.145\linewidth, page=17]{../Figs/PDF/groupfig}
				};\&
				\node (2cdots) {
					{\Large $\cdots$}
				};\\
				\&
				\draw node{\footnotesize \textit{conv2a}};\&
				\draw node{\footnotesize \textit{conv2b}};\&
				\draw node{\footnotesize \textit{conv2c}};\\
				\node (3adots) {
					{\Large $\cdots$}
				};\&
				\node (3a) {
				    \includegraphics[height=0.12\linewidth, page=16]{../Figs/PDF/groupfig}
				};\&
				\node (3b) {
					\includegraphics[height=0.145\linewidth, page=17]{../Figs/PDF/groupfig}
				};\&
				\node (3c) {
					\includegraphics[height=0.145\linewidth, page=17]{../Figs/PDF/groupfig}
				};\&
				\node (1a) {
					\includegraphics[height=0.12\linewidth, page=20]{../Figs/PDF/groupfig}
				};\\
				\&
				\draw node{\footnotesize \textit{conv3a}};\&
				\draw node{\footnotesize \textit{conv3b}};\&
				\draw node{\footnotesize \textit{conv3c}};\&
				\draw node{\footnotesize \textit{pool} \hspace{0.08em} {\footnotesize \textit{output}}};\\
			};
			\end{scope}
			\end{tikzpicture}
\end{frame}

%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Root Modules}
%\includegraphics[width=0.7\linewidth, page=4]{../Figs/PDF/groupfig}
%   \caption{Convolution with $d$ filters of shape $h\times w\times c$.}
%   \label{fig:normalresnet}
%\end{subfigure}\\
%\begin{subfigure}[t]{\linewidth}
\centering
\includegraphics[width=0.9\linewidth, page=5]{../Figs/PDF/groupfig}\\
Root-2 Module: $d$ filters in $g = 2$ filter groups, of shape $h\times w\times c/2$.
\includegraphics[width=0.9\linewidth, page=6]{../Figs/PDF/groupfig}\\
Root-4 Module: $d$ filters in $g = 4$ filter groups, of shape $h\times w\times c/4$.
\end{frame}
%%%%%%%%%%%%%%%%%%%%

\begin{frame}{NiN Root Architectures}
			\begin{tikzpicture}[ampersand replacement=\&]
			\begin{scope}[]
			\matrix[column sep=0em, row sep=2em]{
				\node (1) {
					{\Large $\cdots$}
				};\&
				\node (1c) {
					\includegraphics[height=0.11\linewidth, page=17]{../Figs/PDF/groupfig}
				};\& 
				\node (2a) {
				    \includegraphics[height=0.09\linewidth, page=16]{../Figs/PDF/groupfig}
				};\&
				\node (2b) {
					\includegraphics[height=0.11\linewidth, page=17]{../Figs/PDF/groupfig}
				};\&
				\node (2c) {
					\includegraphics[height=0.11\linewidth, page=17]{../Figs/PDF/groupfig}
				};\&
				\node (3a) {
				    \includegraphics[height=0.09\linewidth, page=16]{../Figs/PDF/groupfig}
				};\&
				\node (3b) {
					\includegraphics[height=0.11\linewidth, page=17]{../Figs/PDF/groupfig}
				};\&
				\node (4) {
					{\Large $\cdots$}
				};\\
				%%%%%%%%%%%%%%%%%%
			    
				%%%%%%%%%%%%%%%%%%
				\node (r1) {
					{\Large $\cdots$}
				};\&
				\node (r1c) {
					\includegraphics[height=0.11\linewidth, page=17]{../Figs/PDF/groupfig}
				};\& 
				\node (r2a) {
					\includegraphics[height=0.15\linewidth, page=19]{../Figs/PDF/groupfig}
				};\&
				\node (r2b) {
					\includegraphics[height=0.11\linewidth, page=17]{../Figs/PDF/groupfig}
				};\&
				\node (r2c) {
					\includegraphics[height=0.11\linewidth, page=17]{../Figs/PDF/groupfig}
				};\&
				\node (r3a) {
					\includegraphics[height=0.09\linewidth, page=18]{../Figs/PDF/groupfig}
				};\&
				\node (r3b) {
					\includegraphics[height=0.11\linewidth, page=17]{../Figs/PDF/groupfig}
				};\&
				\node (r4) {
					{\Large $\cdots$}
				};\\
			};
			\draw[->] (2a) edge (r2a);
			\draw[->] (3a) edge (r3a);
			
			\draw[decorate,decoration={brace,mirror},](r2a.south west) -- node[below=3pt] {\small root-4 module} ++(2.3, 0);
			\draw[decorate,decoration={brace,mirror},yshift=-2em](r3a.south west) + (0, -0.4) -- node[below=3pt] {\small root-2 module} ++(2.5, -0.4);
			\end{scope}
			\end{tikzpicture}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\section*{Results}

\begin{frame}{NiN Root Architectures}
\centering
\textbf{Network-in-Network}. Filter groups in each convolutional layer.
\vfill
%\resizebox{\linewidth}{!}{
\begin{tabular}{@{}lm{1.5em}m{1.5em}m{1.5em}m{1.5em}m{1.5em}m{1.5em}m{1.5em}m{1.5em}m{1.5em}@{}}
\toprule
    Model & \multicolumn{3}{c}{conv1} & \multicolumn{3}{c}{conv2} & \multicolumn{3}{c}{conv3} \\
     & \textit{\footnotesize a} & \textit{\footnotesize b} & \textit{\footnotesize c} & \textit{\footnotesize a} & \textit{\footnotesize b} & \textit{\footnotesize c} & \textit{\footnotesize a} & \textit{\footnotesize b} & \textit{\footnotesize c} \\
     & \textit{\footnotesize5$\times$5} & \textit{\footnotesize1$\times$1} & \textit{\footnotesize1$\times$1} & \textit{\footnotesize5$\times$5} & \textit{\footnotesize1$\times$1} & \textit{\footnotesize1$\times$1} & \textit{\footnotesize3$\times$3} & \textit{\footnotesize1$\times$1} & \textit{\footnotesize1$\times$1} \\
    Orig. & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\
    \midrule
    root-2 & 1 & 1 & 1 & 2 & 1 & 1 & 1 & 1 & 1\\
    root-4 & 1 & 1 & 1 & 4 & 1 & 1 & 2 & 1 & 1\\
    root-8 & 1 & 1 & 1 & 8 & 1 & 1 & 4 & 1 & 1\\
    root-16 & 1 & 1 & 1 & 16 & 1 & 1 & 8 & 1 & 1\\
    \bottomrule
\end{tabular}
\vfill
\end{frame}


%%%%%%%%%%%%%%%%%%%%
\begin{frame}{CIFAR10: Model Parameters \vs Error}
\footnotesize
\pgfplotstableread[col sep=comma]{../rootdata/nincifar.csv}\datatable
\pgfplotstableread[col sep=comma]{../rootdata/nincifar_root_s.csv}\rdatatable
\pgfplotstableread[col sep=comma]{../rootdata/nincifar_tree_s.csv}\tdatatable
\pgfplotstableread[col sep=comma]{../rootdata/nincifar_col_s.csv}\cdatatable
\pgfplotsset{major grid style={dotted,red}}

\centering
\begin{tikzpicture}
%\tikzstyle{every node}=[font=\footnotesize]
\begin{axis}[
  width=\linewidth,
  height=0.66\linewidth,
  axis x line=bottom,
  ylabel=Error,
  xlabel=Model Parameters,
  axis lines=left,
  enlarge x limits=0.05,
  enlarge y limits=0.05,
  grid=major,
  %xmin=0,
  ytick={0.002,0.004,...,1.0},
  %ymin=0.075,ymax=0.09,
  xticklabel style={
        /pgf/number format/fixed,
        /pgf/number format/precision=1
  },
  yticklabel={\pgfmathparse{\tick*1}\pgfmathprintnumber{\pgfmathresult}\%},style={
        /pgf/number format/fixed zerofill,
        /pgf/number format/precision=1
  },
  legend style={at={(1,0.95)}, anchor=south east, column sep=0.2em, font=\small},
  legend columns=4,
]
\addplot[mark=*,mark options={fill=red},
   %nodes near coords,
   only marks,
   point meta=explicit symbolic,
   error bars/y dir=both,
   error bars/y fixed=0.00131497782,
] table[meta=name,x=param,y expr={1 - \thisrow{accuracy} },]{\datatable};
\addplot[mark=square*,mark options={fill=green},
   nodes near coords, only marks,
   every node near coord/.append style={inner sep=4pt},
   point meta=explicit symbolic,
] table[meta=name,x=param,y expr={1 - \thisrow{accuracy} },]{\rdatatable};
\addplot[mark=triangle*,mark options={fill=blue},
   nodes near coords, nodes near coords align = {below}, only marks,
   every node near coord/.append style={inner sep=4pt},
   point meta=explicit symbolic,
] table[meta=name,x=param,y expr={1 - \thisrow{accuracy} },]{\tdatatable};
\addplot[mark=diamond*,mark options={fill=yellow},
   nodes near coords, nodes near coords align = {below}, only marks,
   every node near coord/.append style={inner sep=4pt},
   point meta=explicit symbolic,
] table[meta=name,x=param,y expr={1 - \thisrow{accuracy} },]{\cdatatable};
\legend{NiN, Root, Tree, Column}
\end{axis}
\end{tikzpicture}
%\caption{\textbf{Model Parameters \vs Error.}}
%\label{fig:nincifarparamconvonly}
{\tiny NiN: mean and standard deviation (error bars) are shown over 5 different random initializations.}
\end{frame}

\begin{frame}{CIFAR10: FLOPS (Multiply-Add) \vs Error.}
\pgfplotstableread[col sep=comma]{../rootdata/nincifar.csv}\datatable
\pgfplotstableread[col sep=comma]{../rootdata/nincifar_root_s.csv}\rdatatable
\pgfplotstableread[col sep=comma]{../rootdata/nincifar_tree_s.csv}\tdatatable
\pgfplotstableread[col sep=comma]{../rootdata/nincifar_col_s.csv}\cdatatable
\pgfplotsset{major grid style={dotted,red}}
\footnotesize

\centering
\begin{tikzpicture}
%\tikzstyle{every node}=[font=\footnotesize]
\begin{axis}[
  width=\linewidth,
  height=0.66\linewidth,
  axis x line=bottom,
  ylabel=Error,
  xlabel=FLOPS (Multiply-Add),
  axis lines=left,
  enlarge x limits=0.05,
  enlarge y limits=0.05,
  grid=major,
  %xmin=0,
  ytick={0.002,0.004,...,1.0},
  %ymin=0.075,ymax=0.09,
  xticklabel style={
        /pgf/number format/fixed zerofill,
        /pgf/number format/precision=1
  },
  yticklabel={\pgfmathparse{\tick*1}\pgfmathprintnumber{\pgfmathresult}\%},style={
        /pgf/number format/fixed zerofill,
        /pgf/number format/precision=1
  },
  legend style={at={(1,0.95)}, anchor=south east, column sep=0.2em, font=\small},
  legend columns=4,
]
\addplot[mark=*,mark options={fill=red},
   %nodes near coords,
   only marks,
   point meta=explicit symbolic,
   error bars/y dir=both,
   error bars/y fixed=0.00131497782,
] table[meta=name,x=ma,y expr={1 - \thisrow{accuracy} },]{\datatable};
\addplot[mark=square*,mark options={fill=green},
   nodes near coords, only marks,
   every node near coord/.append style={inner sep=4pt},
   point meta=explicit symbolic,
] table[meta=name,x=ma,y expr={1 - \thisrow{accuracy} },]{\rdatatable};
\addplot[mark=triangle*,mark options={fill=blue},
   nodes near coords, nodes near coords align = {below}, only marks,
   every node near coord/.append style={inner sep=4pt},
   point meta=explicit symbolic,
] table[meta=name,x=ma,y expr={1 - \thisrow{accuracy} },]{\tdatatable};
\addplot[mark=diamond*,mark options={fill=yellow},
   nodes near coords, nodes near coords align = {below}, only marks,
   every node near coord/.append style={inner sep=4pt},
   point meta=explicit symbolic,
] table[meta=name,x=ma,y expr={1 - \thisrow{accuracy} },]{\cdatatable};
\legend{NiN, Root, Tree, Column}
\end{axis}
\end{tikzpicture}
{\tiny NiN: mean and standard deviation (error bars) are shown over 5 different random initializations.}
%\caption{\textbf{FLOPS (Multiply-Add) \vs Error.}}
%\label{fig:nincifarmaconvonly}

%\caption{\textbf{Network-in-Network CIFAR10 Results.} Spatial filters (3$\times$3, 5$\times$5) are grouped hierarchically. The best models are closest to the origin. 
%(left) Parameters \vs Error, (right) FLOPS \vs Error.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%


\newcommand{\covarlabels}[1]{%
\tiny
\vspace{0.75em}
\begin{tikzpicture}[remember picture]
\node [anchor=south west, inner sep=0pt] (c)
    {
        #1
    };
    \path[use as bounding box] (c.south west) rectangle (c.north east);
    \node [anchor=south west, xshift=-0.5em, yshift=-0.5em, rotate=45] at (c.north west) {\footnotesize 0};
    \node [anchor=south east, xshift=\linewidth, yshift=-0.2em] at (c.north west) {\footnotesize 192};
    \node [anchor=south west, xshift=0.25em, yshift=-1.05\linewidth, rotate=90] at (c.north west) {\footnotesize 192};
    \node [anchor=south, xshift=0.5\linewidth] at (c.north west) {\footnotesize\texttt{conv3a}};
    \node [anchor=south, xshift=0.2em, yshift=-0.5\linewidth, rotate=90] at (c.north west) {\footnotesize \texttt{conv2c}};
\end{tikzpicture}%
}

\begin{frame}{Inter-layer Filter Covariance}

\begin{figure}[tb]
\centering
\begin{subfigure}[b]{0.31\linewidth}
\centering
    \covarlabels{\includegraphics[width=\linewidth]{../Figs/Raster/msrc-cifar-nin-4pad-conv8-corr}}
    \caption{\textbf{Standard:} $g=1$}
    \label{fig:normalcovartest}
\end{subfigure}
~
\begin{subfigure}[b]{0.31\linewidth}
\centering
    \covarlabels{\includegraphics[width=\linewidth]{../Figs/Raster/msrc-cifar-nin-4pad-funnel4-convonly-conv8-corr}}
    \caption{\textbf{Root-4:} $g=2$}
    \label{fig:root4}
\end{subfigure}
%~
%\begin{subfigure}[b]{0.24\linewidth}
%\centering
%    \covarlabels{\includegraphics[width=\linewidth]{../Figs/Raster/msrc-cifar-nin-4pad-funnel8-convonly-conv8-corr}}
%    \caption{\textbf{Root-8:} 4 filter groups}
%    \label{fig:root8corr}
%\end{subfigure}
~
\begin{subfigure}[b]{0.31\linewidth}
\centering
    \covarlabels{\includegraphics[width=\linewidth]{../Figs/Raster/msrc-cifar-nin-4pad-funnel32-convonly-conv8-corr}}
    \caption{\textbf{Root-32:} $g=16$}
    \label{fig:root32corr}
\end{subfigure}
\caption{The block-diagonal sparsity learned by a root-module is visible in the correlation of filters on layers \texttt{conv3a} and \texttt{conv2c} in the NiN network.}
\label{fig:covar}
\end{figure}
%Figure~\ref{fig:covar} shows the inter-layer correlation between the adjacent filter layers \texttt{conv2c} and \texttt{conv3a} in the network architectures outlined in Table~\ref{table:ninconfig} as evaluated on the CIFAR test set. The block-diagonalization enforced by the filter group structure (as illustrated in Fig.~\ref{fig:groupconfig}) is visible, more so with larger number of filter groups. This shows that the network learns an organization of filters such that the sparsely distributed strong filter relations, visible in \ref{fig:normalcovartest} as brighter pixels, are grouped into a denser block-diagonal structure, leaving a visibly darker, low-correlated background. See \S\ref{interlayercovar} for more images, and an explanation of their derivation.
\end{frame}

%%%%%%%%%%%%%%%%%%%%

\renewcommand{\covarlabels}[5]{%
\begin{tikzpicture}[anchor=south west]
    \tikzstyle{every node}=[font=\tiny]
    \node [inner sep=0pt] (c)
    {
        #5
    };
    \ifx\covarwidth\undefined
    \newlength{\covarwidth}
    \newlength{\covarheight}
    \fi
    \settowidth{\covarwidth}{#5}
    \settoheight{\covarheight}{#5}
    \path[use as bounding box] (c.south west) rectangle (c.north east);
    %\node [anchor=south west, xshift=-0.5em, yshift=-0.5em, rotate=45] at (c.north west) {\footnotesize 0};
    %\node [anchor=south east, xshift=\covarwidth, yshift=-0.2em] at (c.north west) {\footnotesize #4};
    %\node [anchor=south west, xshift=0.25em, yshift=-1.05\covarheight, rotate=90] at (c.north west) {\footnotesize #2};
    \node [anchor=south, xshift=0.5\covarwidth] at (c.north west) {\tiny\texttt{#3}};
    %\node [anchor=south, xshift=0.2em, yshift=-0.5\covarheight, rotate=90] at (c.north west) {\footnotesize \texttt{#1}};
\end{tikzpicture}%
}

\begin{frame}[fragile]{Intra-layer Filter Correlation}
\centering
\tiny
%    \covarlabels{conv1a}{192}{conv1a}{192}{\includegraphics[width=0.1\textwidth]{../Figs/Raster/nin/corrcoef_conv0.png}}
%~
%    \covarlabels{conv1b}{160}{conv1b}{160}{\includegraphics[width=0.1\textwidth]{../Figs/Raster/nin/corrcoef_conv1.png}}
%~
    \covarlabels{conv1c}{96}{conv1c}{96}{\includegraphics[width=0.15\textwidth]{../Figs/Raster/nin/corrcoef_conv2.png}}
    \covarlabels{conv2a}{192}{conv2a}{192}{\includegraphics[width=0.15\textwidth]{../Figs/Raster/nin/corrcoef_conv4.png}}
    \covarlabels{conv2b}{192}{conv2b}{192}{\includegraphics[width=0.15\textwidth]{../Figs/Raster/nin/corrcoef_conv5.png}}
    \covarlabels{conv2c}{192}{conv2c}{192}{\includegraphics[width=0.15\textwidth]{../Figs/Raster/nin/corrcoef_conv6.png}}
    \covarlabels{conv3a}{192}{conv3a}{192}{\includegraphics[width=0.15\textwidth]{../Figs/Raster/nin/corrcoef_conv8.png}}
    \covarlabels{conv3b}{192}{conv3b}{192}{\includegraphics[width=0.15\textwidth]{../Figs/Raster/nin/corrcoef_conv9.png}}\\
\textbf{Network-in-Network.}\\
%    \covarlabels{conv1a}{192}{conv1a}{192}{\includegraphics[width=0.11\textwidth]{../Figs/Raster/ninroot4/corrcoef_conv0.png}}
%~
%    \covarlabels{conv1b}{160}{conv1b}{160}{\includegraphics[width=0.11\textwidth]{../Figs/Raster/ninroot4/corrcoef_conv1.png}}
%~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot4/corrcoef_conv2.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot4/corrcoef_conv4.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot4/corrcoef_conv5.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot4/corrcoef_conv6.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot4/corrcoef_conv8.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot4/corrcoef_conv9.png}\\
\textbf{Root-4.}\\
%    \covarlabels{conv1a}{192}{conv1a}{192}{\includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot8/corrcoef_conv0.png}}
%~
%    \covarlabels{conv1b}{160}{conv1b}{160}{\includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot8/corrcoef_conv1.png}}
%~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot8/corrcoef_conv2.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot8/corrcoef_conv4.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot8/corrcoef_conv5.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot8/corrcoef_conv6.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot8/corrcoef_conv8.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot8/corrcoef_conv9.png}\\
\textbf{Root-8.}\\
%    \covarlabels{conv1a}{192}{conv1a}{192}{\includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot32/corrcoef_conv0.png}}
%~
%    \covarlabels{conv1b}{160}{conv1b}{160}{\includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot32/corrcoef_conv1.png}}
%~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot32/corrcoef_conv2.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot32/corrcoef_conv4.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot32/corrcoef_conv5.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot32/corrcoef_conv6.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot32/corrcoef_conv8.png}
~
    \includegraphics[width=0.15\textwidth]{../Figs/Raster/ninroot32/corrcoef_conv9.png}\\
\textbf{Root-32.}\\
\includegraphics[width=0.4\linewidth]{../Figs/PDF/colorbar}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}{ILSVRC GoogLeNet}

\begin{itemize}
    \item Root-8 (s) GoogLeNet
    \begin{itemize}
        \item 7\% fewer parameters 
        \item 21\% faster CPU timings
        \item 16\% faster GPU timings
    \end{itemize}
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%

%\input{resnet50maplot}
%%%%%%%%%%%%%%%%%%%%
\begin{frame}{ILSVRC ResNet 50}

%\input{resnet50matable}
\begin{itemize}
\item Root-64 (s) ResNet 50
    \begin{itemize}
        \item 40\% fewer parameters
        \item 45\% fewer floating point operations
        \item 31\% faster CPU timings
        \item 12\% faster GPU timings
    \end{itemize}
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%

\section*{Summary}

\begin{frame}{Summary}

  % Keep the summary *very short*.
  \begin{itemize}
	%\item Separable filter model show surprisingly high accuracy on what are considered challenging problems -- approx.\ 88\% top-5 accuracy on ILSVRC.
	\item Using a hierarchical set of grouped convolutions:
	\begin{itemize}
    	\item Our models are \alert{less computationally complex}.
	    \item They also use \alert{less parameters}.
	    \item Initial results indicate they may significantly help generalization in deeper residual networks.
	\end{itemize}
	\item Are amenable to \alert{model parallelization} (as with original AlexNet), for better parallelism across gpus/nodes
	\begin{itemize}
	    \item State of the art models require 4-8 GPUs to train
	    \item Commerical services using CNNs for inference are distributed.
	\end{itemize}
	%\item The restriction on what filter responses may be combined is an effective form of regularization, and helps prevent over-fitting.
  \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\centering
Extra Slides
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Vanishing/Exploding Gradients}{}
\vspace{-2em}
\begin{figure}
%\begin{tikzpicture}
    %\node[inner sep=0pt] (1) at (0,0)
    %{
    \includegraphics[width=0.2\textwidth, page=1, bb = 0 0 440 600, clip=true]{../Figs/PDF/sparsification}
    \includegraphics[width=0.2\textwidth, page=1, bb = 0 0 440 600, clip=true]{../Figs/PDF/sparsification}
    \includegraphics[width=0.2\textwidth, page=1, bb = 0 0 440 600, clip=true]{../Figs/PDF/sparsification}
    %};
    %\node[inner sep=0pt] (2) at (5,0)
    %{
    $\cdots$
    \includegraphics[width=0.2\textwidth, page=1, bb = 0 0 400 600, clip=true]{../Figs/PDF/sparsification}
    %};
    %\node at ($(1.south east)!.5!(2.south west)$) {\ldots};
%\end{tikzpicture}
\end{figure}

\begin{itemize}
    \item Incorrect initialization scales the forward signal by $\beta$.
    \item After $L$ layers, this becomes a scaling of $\beta^L$.
    \item For \alert{$\beta > 1$}, signal $\rightarrow\infty$, training \alert{diverges}.
    \item For \alert{$\beta < 1$}, signal $\rightarrow 0$, training \alert{stalls}.
    \item Want $\beta\approx 1$ in all layers to prevent this.
\end{itemize}
\note[item]{at the end of this layer, have a softmax!}
\note[item]{$\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}$.}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Convolutional Layer - Initialization}
\begin{figure}
\includegraphics[width=0.7\textwidth, page=1]{../Figs/PDF/sparsification}
\end{figure}
For a sigmoid non-linearity\footcite{glorot2010understanding}:
\begin{eqnarray*}
\sigma &=& \sqrt{\frac{1}{n^{\textrm{ out}}}} = \sqrt{\frac{2}{w^{[i]} h^{[i]} d^{[i]}}}.
\end{eqnarray*}
For a ReLU non-linearity\footcite{journals/corr/HeZR015}:
\begin{equation*}
\sigma = \sqrt{\frac{2}{n^{\textrm{ out}}}}.
\end{equation*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%

\renewcommand{\covarlabels}[5]{%
\begin{tikzpicture}[anchor=south west]
    \node [inner sep=0pt] (c)
    {
        #5
    };
    \ifx\covarwidth\undefined
    \newlength{\covarwidth}
    \newlength{\covarheight}
    \fi
    \settowidth{\covarwidth}{#5}
    \settoheight{\covarheight}{#5}
    \path[use as bounding box] (c.south west) rectangle (c.north east);
    \node [anchor=south west, xshift=-0.5em, yshift=-0.5em, rotate=45] at (c.north west) {\footnotesize 0};
    \node [anchor=south east, xshift=\covarwidth, yshift=-0.2em] at (c.north west) {\footnotesize #4};
    \node [anchor=south west, xshift=0.25em, yshift=-1.05\covarheight, rotate=90] at (c.north west) {\footnotesize #2};
    \node [anchor=south, xshift=0.5\covarwidth] at (c.north west) {\footnotesize\texttt{#3}};
    \node [anchor=south, xshift=0.2em, yshift=-0.5\covarheight, rotate=90] at (c.north west) {\footnotesize \texttt{#1}};
\end{tikzpicture}%
}

\begin{frame}{Whitening for Covariance Analysis}

\begin{figure}[tbp]
\centering
\begin{subfigure}[b]{0.45\linewidth}
\centering
    \covarlabels{conv2c}{192}{conv3a}{192}{\includegraphics[width=\linewidth]{../Figs/Raster/ninroot32/layercovar_conv8.png}}
    \caption{Non-whitened responses}
    \label{fig:notwhitened}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\linewidth}
\centering
    \covarlabels{conv2c}{192}{conv3a}{192}{\includegraphics[width=\linewidth]{../Figs/Raster/ninroot32/layercovarwhite_conv8.png}}
    \caption{Whitened responses}
    \label{fig:whitened}
\end{subfigure}
\caption{Covariance between layers is conflated with the inherent covariances within $X_1$ and $X_2$ from the data. We can more clearly show the covariance between layers by first whitening (using ZCA~\cite{CIFAR10}) the samples in $X_1$ and $X_2$.}
\label{fig:whitevsnot}
\end{figure}
\end{frame}
\end{document}


