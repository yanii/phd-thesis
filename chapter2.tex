\documentclass[thesis]{subfiles}

\begin{document}
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

%Background
%• Trees, DAGs and deep neural networks (DNNs)
%• Motivation: Representing trees/DAGs as MLPs
%• Motivation: ReLUs and data routing

\chapter{Background}
\label{background}

\section{Neural Networks}
Artificial neural networks (or simply neural networks) are a broad range of statistical models characterized by consisting of a set of inter-connected nodes with non-linear \emph{activation functions} with learnable parameters, or \emph{weights}). Although initially biologically inspired, Neural Networks (NN) within the field of machine learning have moved away from biologically-plausible models and towards practical models for applications as diverse as computer vision, speech recognition, and general classifiers.

Neural networks with at least one \emph{hidden} layer have been proven to be a universal approximator - \ie such a neural network can theoretically represent any function~\cite{journals/mcss/Cybenko92,hornik89a}. Representing complex functions requires a very large number of nodes in the hidden layer, however less nodes are required when more hidden layers are used, and thus in practice networks with many hidden layers achieve better accuracy than networks with few hidden layers. 

We will not cover the long and colourful history of neural networks, but attempt to instead provide an overview of research directly relevant to this work. For a comprehensive overview of neural networks we refer the reader to \cite{Bishop1995}.

\subsection{Convolutional Neural Networks}
The earliest work on what are now termed \emph{Convolutional Neural Networks} (CNNs) was by Fukushima et al.~\cite{Fuk80,fukushima2013artificial}, on the \emph{Neocognitron}. The neocognitron was a biologically motivated architecture, motivated by what are typically called simple and complex cells in the primary visual cortex (V1). To model simple cells; cells whose response correlated with simple oriented edges in a translation invariant manner, the neocognitron used shared weights which were connected to local image patches of the input image (and were not simply described as convolution of a filter). Complex cells were modelled by a ``blurring'' operation, what we now term more generally as \emph{pooling}. The neocognitron network consisted of alternating layers of simple and complex cells, \ie alternating convolution and pooling layers, much as seen in state of the art convolutional networks.

Despite the pioneering novelty of the work on neocognitrons, it was only following the simplification and improvement in both the description of the network and it's operation that it gained wider acknowledgement as a breakthrough for image recognition, notably in the work of LeCun, Bottou, Bengio and Haffner~\cite{Lecun1998}. In their work the local shared weights of the neocognitron are put in the context of convolution, and the averaging operation replaces with max-pooling. Their application to handwritten digit recognition gave state of the art results, and would result in the \emph{LeNet5} network, still used today in commercial applications. 

The application of the LeNet-style CNN architecture to more complex problems however proved infeasible, these problems required a deeper hierarchy of representation, which implied a large number of layers. Networks with a large number of layers proved to be untrainable with due to numerous issues with the model itself, notably vanishing gradients, never mind the lack of large datasets and computational power at the time. Convolutional neural networks fell out of favour, and were passed over in favour of the more successful paradigm of using hand-crafted local features for many tasks, and in particular the problem of object instance recognition was well addressed by such solutions. Meanwhile object class recognition remained a difficult problem, for which the best solutions were deformable parts models, also based on local features.

\subsection{Supervision - Alex Krizhevsky \etal}
Training \emph{deep} networks, that is neural networks with many (\eg 4 or more) hidden layers, had proven difficult due to the high computational complexity, and the so called ``vanishing gradient'' problem~\cite{bengio:ieeenn94}. However, recent advances have made training \emph{deep} neural networks possible. A combination of these advances were used in the work of Krizhevsky \etal\cite{Krizhevsky2012imanet} to show that a deep convolutional network with appropriate initialization~\cite{Sutskever2013momentum}, weight decay, ReLU activation functions~\cite{conf/icml/NairH10} and drop out regularisation~\cite{1207.0580v1} could beat state of the art methods on large scale object class recognition methods based on hand-crafted features by a large margin. 

AlexNet notably used training-time and test-time augmentation to achieve it's state of the art accuracy. During training random $224 \times 224$ crops of a $256 \times 256$ image are used, along with random mirroring of these crops. In addition \emph{relighting augmentation} is used, where the PCA components over all RGB pixels in the image are used to perturbe the ``brightness'' of the image, and give some robustness to photometric variations in the test images. At test time ``$`0\times$ oversampling'' is used, that is for each $256\times 256$ image and it's mirror, 4 corner and one centre crop are pushed through the network, and the prediction is simply the averaged over these 10 crops. Finally, for the best results reported (Top-5 error of 15.4\%), an \emph{ensemble} of 7 models is used, where the prediction is the average of all these mdoels. 

AlexNet uses two filter groups throughout most of the layers of the model in order to split computation across two GPUs. The authors observed that the filters on each GPU appeared to specialize to learn fundamentally different features regardless of initialization~\cite{Krizhevsky2012imanet}. This interesting observation has mostly been ignored in subsequent networks where GPU memory has increased enough that such a split of the network is not required, but the original observation is a fundamental motivation of our work.

%There have now been three major eras of artificial neural network research, of which we are in the third. The history of connectionist learning has been punctuated by findings resulting in a loss of confidence of the model.

%\subsection{The Perceptron}
%The first connectionist model, the perceptron, was typically only a two layer network - one layer of input neurons, and one layer of output neurons. Such networks were shown by Minksy et al.~to not be able to represent a simple XOR function. Given such a lack of representational power, research and funding in the field went into severe decline.

%This fundamental restriction was shown, 10 years later, to be solved by the addition of at least one `hidden' layer, \ie a layer not having any direct connection to the input or outputs of the network. At the limit, a hidden layer with an infinite number of neurons was in fact shown to be a universal approximator - it could approximate any function.

%\subsection{Back-propagation}
%The addition of a hidden layer, however, added new problems for training - the so called credit problem. This too was solved with the introduction of back-propagation - applying the chain rule to propagate error derivatives from the output layer throughout the network.

%Although networks with arbitrary numbers of hidden neurons could now be trained from a random initialization, such an optimization is very sensitive to the initialization, and would usually find a local optimum. Optimization methods that settle in local minima are typically not desirable, global minima being the objective of any reasonable optimization. combined with the difficulty of `vanishing gradients' when training a network with many hidden layers (\ie deep neural networks), neural networks fell out of favour yet again.

\subsection{VGG Networks}
Since AlexNet, there have been many improvements to state of the art models on the ILSVRC challenge. One particular models that has lended itself to both high accuracy and being a natural extension of the original network has been the models proposed by Simonyan \etal of the Visual Geometry Group (VGG) at Oxford. The primary contribution of the VGG network is showing that very deep networks improve accuracy. VGG is an evolution of the AlexNet models, with the same number of max-pooling layers, however using very small convolutional filters ($3 \times 3$), and many more of these layers between pooling, instead of the relatively large single layers of convolutional filters in AlexNet ($7\times 7$). In addition VGG uses small non-overlapping max-pooling ($2\times 2$), and the fully convolutional trick of Overfeat to do test-time oversampling. VGG uses extensive training augmentation, extending the augmentation used in AlexNet by adding scale augmentation, where crops are taken from images of different rescaled sizes. 

\subsection{GoogLeNet}

\section{Decision Forests}
\mynote{TODO}

\section{The relationship between Decision Forests and Neural Networks}
There has been work in the past exploring the relationship between decision forests and neural networks. Although this work has identified that neural networks are a generalisation of decision forests, it focused on exploiting tree training towards either the initialization or training of neural networks, rather than creating hybrid models of classifier exploiting the conditional computation in neural networks, while preserving end-to-end training found in state of the art deep neural networks.


\subsection{Entropy Nets}
The relationship between decision forests and neural networks was first described by Sethi~\cite{Sethi1990}, the primary intuition of which is that the decision boundaries which are explicitly expressed in a decision tree can also be represented by a three-layer neural network, where the decision nodes of the tree are on the first layer, the leaf nodes on the second layer.

\subsection{Casting Random Forests as Artificial Neural Networks}
More recently this relationship was rediscovered~\cite{Welbl2014casting}, in a very similair manner a method of initializing a neural network with a trained Random Forest in described. The primary motivation of this is to use the trained random forest as a good initialization of a neural network in order to avoid the neural network from over-fitting during stochastic gradient descent.


Over-fitting however is not the main issue at present with neural networks, already state of the art deep neural networks are


\section{Object Recognition Datasets}
\subsection{CIFAR 10/100}
\subsection{Imagenet}

\section{Object Category Recognition}
\section[Bag of Words]{Bag of Words Approaches to Object Class Recognition}

\section{Convolutional Neural Networks}
\subsection{Imagenet}
\end{document}
