\documentclass[thesis]{subfiles}

\begin{document}
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

%Background
%• Trees, DAGs and deep neural networks (DNNs)
%• Motivation: Representing trees/DAGs as MLPs
%• Motivation: ReLUs and data routing

\chapter{Background}
\label{background}

\section{Neural Networks}
Artificial neural networks (or simply neural networks) are a broad range of statistical models characterized by consisting of a set of inter-connected nodes with non-linear \emph{activation functions} with learnable parameters, or \emph{weights}). Although initially biologically inspired, Neural Networks (NN) within the field of machine learning have moved away from biologically-plausible models and towards practical models for applications as diverse as computer vision, speech recognition, and general tools for regression and classification.

Neural networks with at least one \emph{hidden} layer have been proven to be a universal approximator - \ie such a neural network can theoretically represent any function~\citep{journals/mcss/Cybenko92,hornik89a}. Representing complex functions requires a very large number of nodes in the hidden layer, in practice however, fewer nodes are required when more hidden layers are used. Networks with many hidden layers empirically achieve better accuracy than networks with few hidden layers. 

We will not cover the long and colourful history of neural networks, but attempt to instead provide an overview of contemporary models and methods directly relevant to this work. For a comprehensive overview of neural networks we refer the reader to the excellent reference of~\citet{Bishop1995}.

\subsection{The Neuron}
At its simplest a neuron is a function of the weighted aggregation of many inputs,

\begin{equation}
	y = \sum_{i}^{N} w_i x_i + b,
\end{equation}

where $i=0,\ldots,N$, $w_i$ is the weight of the input $x_i$, $b$ is the \emph{bias} and $f$ is an \emph{activation function}. This is usually expressed more simply in matrix notation, where each neuron consists of an input vector $\mathbf{x}=(x_0,\ldots,x_N)$, activation function $f$, weights $\mathbf{w}$ and a bias $b$, the output of which is,

\begin{equation}
y = \mathbf{w}^T\mathbf{x} + b.
\end{equation}

This function also has a geometric interpretation. In 2D, for example, this is equivalent to the equation of the line. If we define $a=w_0, x=x_0, c=b$,

\begin{equation}
y = a x + c.
\end{equation}

In general a neuron defines a \emph{hyperplane}, a separating manifold of dimension $d - 1$ for an input space of dimension $d$, a line in two dimensions, or plane in 3 dimensions.
Neural networks are a discriminative classifier, and each neuron is a hyperplane functioning as a single decision boundary.

\subsection{Activation Functions}
Without an activation function, even a large multi-layer neural network would have the representational ability of a linear classifier. The \emph{activation function} $f$ is a non-linear function applied to the output of a neuron to allow multi-layer networks to learn complex non-linear functions. %Although each neuron is still representing a hyperplane, the space in which it operates is non-linear

\begin{equation}
y = f\left(\mathbf{w}^T\mathbf{x} + b\right).
\end{equation}

Activations were classically chosen to be a \emph{sigmoid} function, \ie a function mapping negative inputs to negative outputs and positive inputs to positive outputs with a smooth transition around $a = 0$, examples of sigmoid functions commonly used include the logistic function: 

\begin{equation}
	f(x) = \frac{1}{1+e^{-x}},
\end{equation}

hyperbolic tangent:
\begin{equation}
	f(x) = \tanh(x).
\end{equation}

\subsection{Convolutional Neural Networks}
The earliest work on what are now termed \emph{Convolutional Neural Networks} (CNNs) was by~\citet{Fuk80}, on the \emph{Neocognitron}~\citep{fukushima2013artificial}. The neocognitron was a biologically motivated architecture, motivated by what are typically called simple and complex cells in the primary visual cortex (V1). To model simple cells; cells whose response correlated with simple oriented edges in a translation invariant manner, the neocognitron used shared weights which were connected to local image patches of the input image (and were not simply described as convolution of a filter). Complex cells were modelled by a ``blurring'' operation, what we now term more generally as \emph{pooling}. The neocognitron network consisted of alternating layers of simple and complex cells, \ie alternating convolution and pooling layers, much as seen in state of the art convolutional networks.

Despite the pioneering novelty of the work on neocognitrons, it was only following the simplification and improvement of~\citet{Lecun1998} in both the description of the network and its operation that it gained wider acknowledgement as a breakthrough for image recognition. In their work the local shared weights of the neocognitron are put in the context of convolution, and the averaging operation replaces with max-pooling. The application to handwritten digit recognition gave state of the art results, and would result in the \emph{LeNet5} network, still used today in commercial applications.

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.66\columnwidth, page=1]{groupfig}
	\caption[Illustration of convolutional layer]{\textbf{Convolution with $d$ filters of shape $h\times w\times c$.} Convolutional filters (yellow) have the same channel dimension $c$ as the input feature maps (gray) on which they operate. Each filter is convolved across the entire set of input featuremaps to produce a single output feature map. In this illustration we assume padding appropriate to preserve the spatial dimensions.}
	\label{fig:convlayer}
\end{figure}

Figure \ref{fig:convlayer} illustrates a typical convolutional layer. If we denote the $d^{\text{th}}$ feature map for the given layer as $h^d$, where the associated filter has weights $W^d$, bias $b_d$, and activation function $f$, then the single \emph{pixel} of the feature map $h^d$ at spatial location $i, j$ is given by,

\begin{equation}
	h^d_{i,j} = f\left( \left(W^d \convolution x \right)_{i,j} + b_d \right),
\end{equation}

where $\convolution$ is the convolution operator. The discrete convolution operator $(f \convolution g)$ is defined~\citep{damelin2011} for two 1D sequences $f, g$ as,
\begin{equation}
	(f \convolution g)[n] = \sum_{i=-\infty}^\infty f[i]\, g[n - i].
\end{equation}

This can be extended to 2D,
\begin{equation}
(f \convolution g)[m, n] = \sum_{i=-\infty}^\infty \sum_{j=-\infty}^\infty f[i, j]\, g[m - i][n - j].
\end{equation}

The application of the LeNet-style CNN architecture to more complex problems, however, proved infeasible. These problems required a deeper hierarchy of representation, which implied a large number of layers. Networks with a large number of layers proved to be un-trainable due to numerous issues with the model itself, notably vanishing gradients~\citep{hochreiter1991untersuchungen}, and the lack of large datasets and computational power at the time. Convolutional neural networks fell out of favour, and were passed over in favour of the more successful paradigm of using hand-crafted local features for many tasks, and in particular the problem of object instance recognition was well addressed by such solutions. Meanwhile object class recognition remained a difficult problem, for which the best solutions were deformable parts models, also based on local features.

\section{Contemporary Methods of Training Deep Neural Networks}
\subsection{Rectified Linear Activation Function}
An integral part of any useful neuron in a neural network is a non-linear activation function. With a linear activation function, even the deepest network would only be able to represent a linear function. Historically, neural networks have used sigmoidal activation functions. These functions include the hyperbolic tangent ($f(x) = \tanh(x)$), as plotted in Fig.~\ref{fig:tanh} and logistic function ($f(x) = \frac{1}{1+e^{-x}}$). Both of these functions have the nice properties that their range is normalized in ($[-1,1]$ in the case of the hyperbolic tangent), and they are smooth functions with easy to calculate derivatives.

\begin{figure}[tbp]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\begin{tikzpicture}
		\begin{axis}%
		[
		thick,
		width=\textwidth,
		height=0.85\textwidth,
		grid=major,
		xmin=-5,
		xmax=5,
		axis x line=bottom,
		ytick={-1,0,1},
		ymax=1,
		ymin=-1,
		axis y line=middle,
		]
		\addplot%
		[
		blue,%
		mark=none,
		samples=500,
		domain=-5:5,
		]
		(x,{tanh(x)});
		\end{axis}
		\end{tikzpicture}
		\caption{Hyperbolic Tangent $y = \tanh(x)$}
		\label{fig:tanh}
	\end{subfigure}
	~
	\begin{subfigure}[t]{0.48\textwidth}
		\begin{tikzpicture}
		\begin{axis}%
		[
		thick,
		width=\textwidth,
		height=0.85\textwidth,
		grid=major,
		xmin=-1,
		xmax=1,
		axis x line=bottom,
		ytick={-1,0,1},
		ymax=1,
		ymin=-1,
		axis y line=middle,
		]
		\addplot%
		[
		blue,%
		mark=none,
		samples=500,
		domain=-5:5,
		]
		(x,{max(x,0)});
		\end{axis}
		\end{tikzpicture}
		\caption{ReLU activation function $y = \max(0,x)$}
		\label{fig:relu}
	\end{subfigure}\\
	\begin{subfigure}[t]{0.48\textwidth}
		\begin{tikzpicture}
		\begin{axis}%
		[
		thick,
		width=\textwidth,
		height=0.85\textwidth,
		grid=major,
		xmin=-5,
		xmax=5,
		axis x line=bottom,
		ytick={-1,0,1},
		ymax=1,
		ymin=-1,
		axis y line=middle,
		]
		\addplot%
		[
		blue,%
		mark=none,
		samples=500,
		domain=-5:5,
		]
		(x,{1-tanh(x)*tanh(x)});
		\end{axis}
		\end{tikzpicture}
		\caption{Derivative $\frac{d\tanh(x)}{dx}$}
		\label{fig:tanhgradients}
	\end{subfigure}
	~
	\begin{subfigure}[t]{0.48\textwidth}
		\begin{tikzpicture}[
		declare function={
		  	func(\x) = (\x<=0) * (0) + and(\x>0) * (1);
		 }
		]
		\begin{axis}%
		[
		thick,
		width=\textwidth,
		height=0.85\textwidth,
		grid=major,
		xmin=-1,
		xmax=1,
		axis x line=bottom,
		ytick={-1,0,1},
		ymax=1,
		ymin=-1,
		axis y line=middle,
		]
		\addplot%
		[
		blue,%
		mark=none,
		samples=500,
		domain=0:5,
		]
		(x, {1});
		\end{axis}
		\begin{axis}%
		[
		thick,
		width=\textwidth,
		height=0.85\textwidth,
		grid=major,
		xmin=-1,
		xmax=1,
		axis x line=bottom,
		ytick={-1,0,1},
		ymax=1,
		ymin=-1,
		axis y line=middle,
		]
		\addplot%
		[
		blue,%
		mark=none,
		samples=500,
		domain=-5:0,
		]
		(x, {0});
		\end{axis}
		\end{tikzpicture}
		\caption{Derivative $\frac{d \max(0,x)}{dx}$}
		\label{fig:relugradient}
	\end{subfigure}
	\caption[Activation Functions]{Activation functions for Neural Networks}
	\label{fig:afunctions}
\end{figure}

A major issue with sigmoidal activation functions however, is that gradients outside of a relatively narrow region of the function domain (close to $x=0$) are very small. When training with back propagation, this means that most gradients are of very small magnitude, and training can take a very long time, or even stall altogether -- a situation that is often called the \emph{vanishing gradient} problem, first identified by \citet{hochreiter1991untersuchungen}. This term has also been conflated with numerical precision issues caused by incorrect initialization, as identified by \citet{glorot2010understanding}.

Rectified linear units (ReLU) were proposed as a solution, first for restricted Boltzmann machines~\citep{conf/icml/NairH10}, and later for neural networks~\citep{glorot2010understanding}, where empirically they were shown to allow easier training with backpropogation. These neurons have a piecewise activation function, $f(x) = \max(0,x)$ (Fig.~\ref{fig:relu}). ReLUs do not exhibit the `saturation' of sigmoidal functions, always giving a gradient of either 0 or 1. In practice this can greatly speed up training with back propagation, or even allow training networks that are not trainable in practice with sigmoidal activation functions, such as the deep network of \citet{Krizhevsky2012imanet}.

\subsection{Methods of Network Initialization}\label{ssec:init}
Until relatively recently, pre-training was considered necessary for the feasibility of training deep neural networks~\citep{hinton2006reducing}. The vanishing gradient problem was first addressed through better methods of random initialization which considered the geometric effect of propagating gradients through very deep networks. Without careful initialization, gradients can either surpass numerical representation (exploding gradients) or be reduced to close to zero (vanishing gradients). This effect is further exacerbated by the use of the softmax function at the end of the network, containing the exponential function.

For example, consider a deep network consisting of $L$ identical layers as shown in Fig.~\ref{fig:manylayers}, and assume that our initialization results in the first pass through the network scaling the signal (i.e~gradients) by a factor of $\beta$ for each layer.

After propagating through $L$ layers, this becomes a scaling of $\beta^L$, greatly magnifying the effect of the discrepancy. For example, the output of a trivial deep network  where each layer $l$ only maps the identity function, $f_{l}(x) = x$, with $L$ layers, and each layer initialized such that the output response is scaled by $\beta$:

\begin{figure}[tbp]
	\includegraphics[width=0.23\textwidth, page=1, viewport = 0 0 440 600, clip=true]{layer}
	\includegraphics[width=0.23\textwidth, page=1, viewport = 0 0 440 600, clip=true]{layer}
	\includegraphics[width=0.23\textwidth, page=1, viewport = 0 0 440 600, clip=true]{layer}
	$\cdots$
	\includegraphics[width=0.23\textwidth, page=1, viewport = 0 0 400 600, clip=true]{layer}
	\caption[Vanishing Gradients]{\textbf{Vanishing Gradients.} For networks with many layers, even small deviations from a unit gradient are quickly geometrically magnified by propagation through all layers.}
	\label{fig:manylayers}
\end{figure}
\begin{equation}
\begin{aligned}
	f_L(x) & = (f_1 \circ f_2 \ldots \circ f_L) (x)\\
	& = x \, \prod^{L}_{l} \beta = x\, \beta^L
\end{aligned}
\end{equation}

\[
\lim_{L\to\infty} f_L(x) = 
\begin{cases}
\infty & \text{if } \beta > 1, \text{ training loss \emph{diverges}}\\
0 & \text{if } \beta < 1, \text{ training loss \emph{stalls}},
\end{cases}
\]

where $(f \circ g)(x)$ is the composition $f(g(x))$.

Thus we want a random initialization of layers such that $\beta\approx 1$ to minimize the 'vanishing gradient' effect. For sigmoidal activation functions, such an initialization was proposed by Glorot et al.~\cite{glorot2010understanding}. For random Gaussiaan initialization, and given the number of outgoing/incoming connections to each neuron, we can carefully choose the standard deviation $\sigma$ such that $\mathbf{E}(\beta) = 1$. 

In practice however, most networks have layers of different numbers of neurons. In this case, since there are different numbers of incoming connections and outgoing connections for each neuron, there are two possible initializations, one for the expected forward pass (response) scaling, and one for the backwards pass (gradient). As a compromise, Glorot et al.~\cite{glorot2010understanding} proposed to use the average number of outgoing and incoming connections to the neuron:

\begin{equation}
\begin{aligned}
	\sigma_{\textrm{forwards}} &= \frac{1}{\sqrt{n_{\text{out}}}},\\
	\sigma_{\textrm{backwards}} &= \frac{1}{\sqrt{n_{\text{in}}}},\\
	\sigma_{\textrm{average}} &= \frac{1}{\sqrt{(n_{\text{out}} + n_{\text{in}})/2}}.
\end{aligned}
\end{equation}

For the more typically used rectified linear unit, a variation of this initialization was proposed by He et al.~\cite{He2015b}:

\begin{equation}
\begin{aligned}
	\sigma_{\textrm{forwards}} &= \frac{2}{\sqrt{n_{\text{out}}}},\\
	\sigma_{\textrm{backwards}} &= \frac{2}{\sqrt{n_{\text{in}}}},\\
	\sigma_{\textrm{average}} &= \frac{2}{\sqrt{(n_{\text{out}} + n_{\text{in}})/2}}.
\end{aligned}
\end{equation}

\subsection{Batch Normalization}
Some network architectures are sufficiently complex, e.g.~networks with neurons with drastically different number of outgoing/incoming connections, that even careful initialization will not prevent vanishing gradients. Instead, a more direct approach of managing gradient amplitude during training was introduced by \citet{Ioffe2015}. Most state-of-the-art supervised deep networks now use both of these methods.

Batch normalization uses batch statistics to whiten the responses of layers it is applied to during training. Batch normalization calculates the the mini-batch mean and variance, and prevents vanishing gradients by normalizing responses/gradients according to the batch statistics. 

\section{Contemporary Deep Neural Networks}

\subsection{AlexNet}
Training \emph{deep} networks, that is neural networks with many (\eg 2 or more) hidden layers, had proven difficult due to the high computational complexity, and the so called ``vanishing gradient'' problem~\citep{bengio:ieeenn94}. However, recent advances have made training deep neural networks possible. \citet{Krizhevsky2012imanet} used a combination of these advances to show that a deep convolutional network (since referred to as AlexNet) with appropriate initialization~\citep{Sutskever2013momentum}, weight decay, ReLU activation functions~\citep{conf/icml/NairH10} and drop out regularisation~\citep{Hinton2012} could beat state of the art methods on large scale object class recognition methods based on hand-crafted features by a large margin. 

AlexNet notably used training-time and test-time augmentation to achieve its state of the art accuracy. During training random $224 \times 224$ crops of a $256 \times 256$ image are used, along with random mirroring of these crops. In addition \emph{relighting augmentation} is used, where the PCA components over all RGB pixels in the image are used to perturb the ``brightness'' of the image, and give some robustness to photometric variations in the test images. At test time ``$10\times$ oversampling'' is used, that is for each $256\times 256$ test image, and its mirrored image, 4 corner and one centre crop are pushed through the network, and the prediction is simply the averaged over these 10 crops. Finally, for the best results reported (Top-5 error of 15.4\%), an \emph{ensemble} of 7 models is used, where the prediction is the average of all these models. 

AlexNet uses two filter groups throughout most of the layers of the model in order to split computation and model parameters across two GPUs, the motivation being that at the time GPUs did not have enough memory to fit such a large model. The authors observed that the filters on each GPU appeared to specialize to learn fundamentally different features regardless of initialization~\citep{Krizhevsky2012imanet}. This interesting observation has mostly been ignored in subsequent networks where GPU memory has increased enough that such a split of the network is not required, but the original observation is a fundamental motivation of our work.

\subsection{Network in Network}
\citet{Lin2013NiN} introduced \emph{Network in Network} (NiN), in which the main contribution was the use of so called `micro networks', consisting of increased non-linearity between convolutions using 1$\times$1 convolutions. The authors claimed extra the non-linearities introduced allow the network to capture more complex functions. These 1$\times $1 convolutions have since been referred to as \emph{low dimensional embeddings}, \ie a reduction in the number of filters by a mapping of a high-dimensional feature map onto a lower-dimensional feature map. This can be used to reduce the computation and parameters of convolutional layers significantly. 

\citet{Lin2013NiN} also introduced \emph{global average pooling}, in which the spatial extents at the end of the convolutional layers (\ie pool5 for NiN/AlexNet) are aggregated such that there is only a single scalar output response for each filter in the pooled layer. After global average pooling a layer with $f$ filters/feature maps, the input to the classification layer is simply a vector of $f$ responses. This reduces the parameters network dramatically since the majority of the parameters in the network are typically between the last convolutional layer and the fully-connected classification layer. Lin \etal showed that on CIFAR-10 global average pooling by itself achieved a lower error than having a fully connected layer with dropout.

\subsection{VGG}
Since AlexNet, there have been many improvements to the state of the art on the ILSVRC challenge, every one of which has been an improved CNN architecture. One particular model that has lended itself to both high accuracy and being a natural extension of the original network has been the models proposed by \citet{Simonyan2014verydeep} of the Visual Geometry Group (VGG) at Oxford. The primary contribution of the VGG network is showing that very deep networks improve accuracy. VGG is an evolution of the AlexNet models, with the same number of max-pooling layers, however using very small convolutional filters ($3 \times 3$) in the convolutional layers, and many more of these convolutional layers between pooling, instead of the relatively large single layers of convolutional filters in AlexNet ($7\times 7$). In addition VGG uses small non-overlapping max-pooling ($2\times 2$), and the \emph{fully convolutional trick} introduced by \citet{Sermanet2013overfeat} to do test-time oversampling more efficiently. VGG uses extensive training augmentation, extending the augmentation used in AlexNet~\citep{Krizhevsky2012imanet} by adding scale augmentation, where crops are taken from images of different rescaled sizes. 


\subsection{Inception}
The winner of the ILSVRC2014 challenge, as measured by classification accuracy alone, was the Inception architecture (GoogLeNet)~\citep{Szegedy2014going}. The Inception architecture is particularly interesting, in that it was created explicitly to minimize computation. Although it uses the low dimensional embeddings of Network in Network, this is combined with a novel combination of filters of different spatial size, within what is called an `inception' unit. While large filters are advantageous in the problem of image class recognition, most of the important correlations in natural images are very localized, so much so that even 3$\times$3 filters can learn most of the important features, for example image gradients and edges -- as demonstrated by the VGG networks~\citep{Simonyan2014verydeep}. However, a few of the correlations are less localized and better captured by 5$\times$5 or even 7$\times$7 filters. Instead of learning a lot of large and computationally expensive 7$\times$7 filters, the inception unit learns mostly 3$\times$3 filters, with fewer 5$\times$5 and small number of 7$\times$7 filters. 

%As will be explained in \S{\ref{googlenetasbasis}}, the specific method inception does this is better explained by a basis than the `factorization' explanation used by the authors.
%\mynote{Add section on how inception is learning basis for spatial dimension of filters}

\subsection{Residual Networks}
\citet{He2015} introduced residual networks, with the insight that training extra layers to deep networks could in fact \emph{increase} training error. Yet it is not clear why this should be so -- a trivial set of parameters could be learned by the optimization to at least maintain the accuracy of a shallow network, namely the identity. In order to aid optimization, a residual layer is instead trained. Assuming our desired, but difficult to optimize, mapping from one layer to the next is $H(x)$, the residual function learned is simply:
\begin{equation}
	F(x) = H(x) + x.
\end{equation}

In practice these residual layers greatly help optimize very deep networks, and have pushed state of the art accuracy in many datasets.
\mynote{TODO: add figure for resnets}

\section{Decision Forests}
\subsection{Decision Trees}
Decision trees have played a part in statistics, and machine learning, for a long time. They have their roots in classification trees, human generated versions of which have been commonly used for hierarchical classification of animal and plant species. As such they are conceptually amongst the simplest classification methods to understand. 

In machine learning we are interested in automatically learning classifiers from training data. However, despite the simplicity of decision trees, in general learning an optimal decision tree for a given training set has been shown to be NP-hard~\cite{journals/iandc/HancockJLT96}. For this reason greedy training algorithms are used to train and grow trees from training data, based on various heuristics, typically measures of entropy or information gain at split nodes~\cite{breiman84}. 

Like nearest neighbours, deep decision trees can easily achieve perfect training set accuracy, but do not generalise well. 

\subsection{Random Forests/Decision Forests}
Interest in decision trees has recently been revived in machine learning since new methods for training decision trees can result in good generalization. In particular Decision Forests (Random Forests)~\cite{journals/neco/AmitG97,breiman2001random}, were introduced as a form of ensemble for decision trees.

\section{The relationship between Decision Forests and Neural Networks}
There has been work in the past exploring the relationship between decision forests and neural networks. Although this work has identified that neural networks are a generalisation of decision forests, it focused on exploiting tree training towards either the initialization or training of neural networks, rather than creating hybrid models exploiting the conditional computation in random forests, while preserving end-to-end training found in state of the art deep neural networks.


\subsection{Entropy Nets}
The relationship between decision forests and neural networks was first described by Sethi~\cite{Sethi1990}, the primary intuition of which is that the decision boundaries which are explicitly expressed in a decision tree can also be represented by a three-layer neural network, where the decision nodes of the tree are on the first layer, the leaf nodes on the second layer.

\subsection{Casting Random Forests as Artificial Neural Networks}
More recently this relationship was rediscovered~\cite{Welbl2014casting}, in a very similar manner a method of initializing a neural network with a trained Random Forest in described. The primary motivation of this is to use the trained random forest as a good initialization of a neural network in order to avoid the neural network from over-fitting during stochastic gradient descent.

\end{document}
