% !TEX root = thesis.tex
\documentclass[thesis]{subfiles}

\begin{document}
%*******************************************************************************
%*********************************** Conclusion *****************************
%*******************************************************************************

\chapter{Conclusion}
\label{conclusion}
%********************************** %First Section  **************************************
In this work, we propose that carefully designing networks in consideration of our prior knowledge of the task can improve the memory and compute efficiency of state-of-the art networks, and even increase accuracy through structurally induced regularization. While this philosophy defines our approach, deep neural networks\index{neural network} have a large number of degrees of freedom, and there are many facets of deep neural networks\index{neural network} that warrant such analysis. We have attempted to present each of these in isolation:

\Cref{lowrankfilters} proposed to exploit our knowledge of the low-rank nature of most filters learned for natural images by structuring a deep network to learn a collection of mostly small 1$\times$h and w$\times$1 basis filters, while only learning a few full w$\times$h filters. Our results showed similar or higher accuracy than conventional \glspl{cnn} requiring much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41\% less compute and only 24\% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point {\em increase} in accuracy over our improved VGG-11 model, giving a top-5 \emph{center-crop} validation accuracy of 89.7\% while reducing computation by 16\% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for \gls{ilsvrc}, we achieved comparable accuracy with 26\% less compute and 41\% fewer model parameters. Applying our method to a near state-of-the-art network for CIFAR, we achieved comparable accuracy with 46\% less compute and 55\% fewer parameters. 
	
\Cref{deeproots} addresses the filter/channel extents of convolutional filters, by learning filters with limited channel extents. When followed by a 1$\times$1 convolution, these can also be interpreted as learning a set of basis filters, but in the channel extents. 
Unlike in \cref{lowrankfilters}, the size of these channel-wise basis filters increased with the depth of the model, giving a novel sparse connection structure that resembles a tree root. This allows a significant reduction in computational cost and number of parameters of state-of-the-art deep \glspl{cnn} without compromising accuracy. Our results showed similar or higher accuracy than the baseline architectures with much less compute, as measured by \gls{cpu} and \gls{gpu} timings. For example, for ResNet 50, our model has 40\% fewer parameters, 45\% fewer floating point operations, and is 31\% (12\%) faster on a \gls{cpu} (\gls{gpu}). For the deeper ResNet 200 our model has 25\% fewer floating point operations and 44\% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7\% fewer parameters and is 21\% (16\%) faster on a \gls{cpu} (\gls{gpu}).

\Cref{lowrankfilters,deeproots} proposed similar methods for reducing the computation and number of parameters in the spatial and channel (filter-wise) extents of convolutional filters respectively. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of smaller basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. This means that at both training and test time our models are more efficient. Overall, the approach of learning a set of basis filters was not only effective for reducing both computation and model complexity (parameters), but in many of the results in both \cref{lowrankfilters,deeproots}, the models trained with this approach generalized better than the original state-of-the-art models they were based on.

\Cref{conditionalnetworks} presented work towards conditional computation in deep neural networks\index{neural network}. We proposed a new discriminative learning model, \emph{conditional networks}, 
that jointly exploits the accurate \emph{representation learning} capabilities of deep neural networks\index{neural network} with the efficient \emph{conditional computation} of decision trees and directed acyclic graphs (DAGs). In addition to allowing for faster inference, conditional networks yield smaller models, are highly interpretable, and offer test-time flexibility in the trade-off of compute \vs accuracy.

\end{document}
