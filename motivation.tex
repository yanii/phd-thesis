\documentclass[thesis]{subfiles}

\begin{document}
	\chapter{The Effect of Structure on Learning in Neural Networks}
	\label{motivation}
	
	It is well known that the design of a neural network architecture can have a large effect on the generalization of a learned model. And yet network design itself remains poorly understood, with intuition and experience being the cited motivation behind most common architectures, rather than theory. This, more than perhaps any other factor, has been a barrier to access for the practical use of neural networks by people who are not experts in the field.
	
	Beyond hyper-parameters used for tuning the optimization method, such as learning rate, momentum and weight decay, the architecture of a network has a profound effect on the learning. Nowhere is this effect more pronounced than in the case of using neural networks with highly structured inputs, such as natural images. Although neural networks are usually posed as general learning machines, time and again it has been demonstrated that neural networks only truly stand out as a learning method when we encode our prior knowledge of the task in the architecture itself -- a concept that we will throughout this work refer to as \emph{structural priors}. Examples of structural priors include common network architectures for images, convolutional neural networks (CNNs), and sequences, recurrent neural networks (RNNs).
	
	Neural Networks with structural priors still differ significantly from hand-tuned local features, as popularized in computer vision in the early 2000s, such as SIFT~\citep{Lowe2004}. As compared with neural networks, such local features are rigidly defined in terms of structure and weights, and the learning system is restricted to finding and cataloging the pre-determined features in images. Neural networks with structural priors on the other hand, while restricting the structure of the network somewhat, still allow the network to learn more fine-grained structure, and have no effect on the latitude given to learning weights.
	
	% This is why NN are not good at general learning, and we are now only good at expert systems
	
	The history of understanding the role of neural network architecture in learning is long, arguably going back to the Hebbian rule of learning~\citep{hebb1949organization}, and yet our understanding is still far from complete. In this section we will review a select number of the most important previous works relevant to understanding the role that structural priors play, and how they emerged to dominate the practical use of neural networks today.
	
	\section{Network Architecture}
    A persistent question in training artificial neural networks has been in the design of the networks. Specifically the question of how many parameters should be learned, and in what way they should be connected,  so as to be suitable for good generalization from a given size dataset. Notable steps in the theoretical answers to this question include findings showing the limitations of single layer networks~\citep{minsky1988perceptrons}, information-theoretic measures of the representational capacity of a network~\citep{vapnik2015uniform}, the proof that single hidden-layer networks are universal approximators~\citep{hornik89a}, and the theoretical number of nodes required for generalization from a dataset of given size~\citep{baum1989size}. 
    
    Empirical results have, however, shown that the realities of training neural networks do not match what theory predicts. Deep networks of many hidden layers have been shown time and again to out-perform shallow networks~\citep{Krizhevsky2012,Simonyan2014verydeep,He2015,He2016}, perhaps due to our limited method of optimization~\citep{NIPS2014_5484}. Networks with many more parameters than training samples, that use early-stopping or are regularized strongly, generalize better in practice than networks with the theoretically sufficient capacity~\citep{caruana2001overfitting, Krizhevsky2012, HintonTalk2015}. Networks designed with a specialized connectivity structure closer reflecting the underlying solution have consistently generalized better than fully-connected networks with higher learning capacity~\citep{lecun1989backpropagation,He2016}. In fact these seemingly theoretically defying design strategies can claim to have been responsible for recent breakthroughs in generalization on previously difficult tasks such as image class recognition~\citep{Krizhevsky2012, HintonTalk2015}.
    
    \section{Model Capacity and Representational Power}
    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{Figs/PDF/vcdim_r2lines}
        \caption{All possible labellings of 3 points in $\mathbb{R}^2$ can be separated by a oriented line (2D hyperplane). This is not possible for all labellings of 4 points in $\mathbb{R}^2$ however, and thus the VC dimension of oriented hyperplanes in $\mathbb{R}^2$ is 3. From \citet{burges1998tutorial}.}
        \label{fig:vcdim_r2line}
    \end{figure}
    The information theoretic notion of capacity, that is the expressive power of a classification algorithm, gives important insights to the learning ability of a classification algorithm. Analysis is typically based on the Vapnikâ€“Chervonenkis (VC) dimension~\citep{vapnik2015uniform} of the class of functions used as discriminators, \eg hyperplanes in the case of neural networks. Intuitively, for a discriminative classifier, the VC dimension measures the largest number of points that can be classified without error. In such a case, the set of points is said to be \emph{shattered} by the classifier. A good overview of VC-dimension is given by \citet{burges1998tutorial}.
    
    \subsection{Vapnik-Chervonenkis Dimension}
    More formally, a classification model $f(\theta)$, parameterized by $\theta$ is said to \emph{shatter} a set of data points $(x_0, x_1, \ldots, x_h)$ if for for all possible labelings of the points, the classification model can perfectly learn the points. The Vapnik-Chervonenkis (VC) dimension is the largest number of (any) points that can be shattered by such a classifier. For a classifier of VC-dimension $h$, it is sufficient that there exists a \emph{single} set of $h$ points which can be shattered. It is important to note that in general a classifier with a VC-dimension of $h$ will not necessarily shatter every possible set of $h$ points. 
    %Note that, if the VC dimension is h, then there exists at least one set of h points that can be shattered, but it in general it will not be true that every set of h points can be shattered
    
    For example, in Fig.\ \ref{fig:vcdim_r2line}, the function class of oriented hyperplanes, \ie lines in 2D, can separate all possible labellings of 3 points in $\mathbb{R}^2$ -- oriented hyperplanes in $\mathbb{R}^2$ shatter 3 points. However, for 4 points, this is no longer true. It can be proven (see~\citet{burges1998tutorial}) that in general for $\mathbb{R}^n$, the set of oriented hyperplanes shatters any set of $n+1$ points.

    The VC dimension gives us a measure of the theoretical learning capacity of a classifier, however it can also be somewhat counter-intuitive. While models with large numbers of parameters usually will have a higher VC dimension, there are examples of small single parameter models with infinite VC dimension for more specific sets of points. For example, if we have a set of evenly spaced points in 2D, a simple sinusoidal curve with the appropriate phase can shatter any labeling of an infinite number of such points. In general, however, such a classifier would be poor at classifying more general sets of points, despite the impressive theoretical VC dimension.
    
    \subsection{VC Dimension of Neural Networks}
    In the case of neural networks, early work showed that the capacity of neural networks to be quite large~\citep{hornik89a,baum1989size}. \citet{baum1989size} looked at feed-forward networks of \emph{threshold units}, \ie perceptrons. %showing the VC dimension of a single-layer network with k units and w weights (including biases) to be bounded by,
    %\begin{equation}
    %    d_{\textrm{VC}} \leq 2 w \log_2(e\,k),
    %\end{equation}
    
    %where $e$ is the mathematical constant, the base of natural logarithms. 
    The authors prove a lower bound on the VC dimension for a single hidden layer network of $k$ units and $n$ inputs,
    \begin{equation}
        d_{\textrm{VC}} \geq 2 \lfloor k/2 \rfloor n,
    \end{equation}
    where $\lfloor \ \ \rfloor$ is the floor operation, \ie largest integer less than or equal to the operand, and $d$ is the number of inputs. For a single hidden layer network with a large number of $n$ inputs and $k$ units, the authors make the assumption that $kn\approx w$, \ie the number of weights in the first layer alone is approximately that of the whole network $w$,
    \begin{equation}
        d_{\textrm{VC}} \geq w.
    \end{equation}

    \citet{baum1989size} use this lower bound on the VC dimension of a hidden layer to bound the number of training samples required to achieve an error rate of $\epsilon$, showing that for a network with $w$ weights, and a desired error rate $\epsilon$ the minimum number of training samples required is given by,
    
    \begin{equation}
        N_{\min} \approx w/\epsilon.
    \end{equation}
    
    For an error rate of $\epsilon=0.1$, this gives the rule of thumb that for a network with a total of $w$ weights, approximately $10\times w$, or 10 times the number of training samples as weights in the network, are required to guarantee an error rate of 10\%. 
    
    At first glance this work may seem to have solved a major problem in the design and training of neural networks, however the work of \citet{baum1989size} is to show a worst-case lower bound on the number of training samples required -- in practice this can be far from what is empirically required. \citet{baum1989size} themselves point out that this is likely be far more than necessary in networks where the learning algorithm seeks to minimize the number of non-zero weights (such as networks using weight decay, pruning, etc). Indeed, in practice neural networks were found to generalize much better than this worst case bound would indicate, to the point where modern deep neural networks are trained with far fewer samples than weights, in the case of AlexNet~\citep{Krizhevsky2012} approximately $250\times$ \emph{fewer} training samples than weights allows good generalization on the ILSVRC~\citep{ILSVRC2015} dataset.
    
    \citet{bartlett1997} later showed that rather than capacity being based on solely the number of weights, a bound more in-line with empirical results could be found by using the number of \emph{large weights}. To show this they moved to a scale sensitive form of the VC dimension, the \emph{fat-shattering} dimension. The author shows that the error rate for an $\ell$-layer sigmoidal (rather than threshold unit) neural network with $n$ inputs and $m$ training samples,
    
    \begin{equation}
        \epsilon \approx (cA)^{\ell(\ell+1)/2} \sqrt{(\log n)/m},
    \end{equation}
    
    and $c$ is a constant factor, and the $\ell_1$ norm of each unit's weight vector $w$ is bounded by $A$, 
    \begin{equation}
    \|\mathbf{w}\|_1 = \sum_i | w_i | \leq A.
    \end{equation}
    Surprisingly there is no term for the number of units for any layer in this equation, but rather it is the bounds on the weights themselves that determine ability of the network to generalize. In general, for a network where the inputs $\mathbf{x}$ are also bounded,
    \begin{equation}
        \|\mathbf{x}\|_\infty = \max(|x_0|, |x_1|, \ldots, |x_n|) \leq B,
    \end{equation}
    \citet{bartlett1997} show that for a given error $\epsilon$, the number of training samples $m$ required grows roughly as,
    \begin{align}
        m \approx \frac{B^2\,A^{\ell^2}}{\epsilon^2}.
    \end{align}
    
     For a single hidden layer network, \ie $\ell=2$, and an error rate of $\epsilon=0.05$, 
    
    \begin{align}
        m \approx 400\,B^2\,A^6.
    \end{align}
    
    This result, while surprising given the analysis based on the VC dimension of neural networks, supports empirical results in using contemporary training methods such as weight decay~\citep{hinton1987learning}, early stopping~\citep{Bishop1995}, and even more recently batch normalization~\citep{Ioffe2015}, all of which can keep weight magnitudes low.
%    For a single hidden layer network, \ie $\ell=2$, and an error rate of $\epsilon=0.05$, 
    
%    \begin{align}
%        m &\approx  \frac{\log n \, (cA)^{\ell(\ell+1)}}{\epsilon^2}\\
%        &\approx  \frac{\log n \, (cA)^{6}}{0.0025},\\
%    \end{align}
%    we can see that the number of training samples required to guarantee this error rate are proportional to the logarithm of the number of inputs and the , 
	
    \subsection{Model Size}
    \todo{Include references to \citet{caruana2001overfitting, baum1989size, rethinking2016}}
    
	\begin{figure}[tb]
		\centering
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit3rd.pgf}}
			\caption{\engordnumber{3}-order polynomial fitting 3 points}
			\label{fig:polyfit3rd}
		\end{subfigure}
		~
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit20th.pgf}}
			\caption{\engordnumber{20}-order polynomial fitting 3 points}
			\label{fig:polyfit20th}
		\end{subfigure}\\	
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit3rdlots.pgf}}
			\caption{\engordnumber{3}-order polynomial fitting 10 points}
			\label{fig:polyfit3rdlots}
		\end{subfigure}
		~
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit20thlots.pgf}}
			\caption{\engordnumber{20}-order polynomial fitting 10 points}
			\label{fig:polyfit20thlots}
		\end{subfigure}
		\caption[Polynomial Data Fitting]{Polynomial fits of samples from a 3rd order function. Polynomials of high order, like neural networks of many parameters, easily overfit a small number of samples as compared to polynomials of a more suitable order for the sampled function. While generalization is helped by more data, the higher order polynomial still tends to overfit.}
		\label{fig:polyfits}
	\end{figure}

	\citet{denker1987large} explored the relationship of network architecture to generalization in a more empirical manner. The work was particularly motivating in the later design of convolutional neural networks~\citep{lecun1989generalization, lecun1989backpropagation}. The authors make the intuitive analogy between the affect of the size of a neural network on its generalization, and a simple least-squares polynomial fit. Fig.~\ref{fig:polyfits} shows various polynomial fits to samples from a \engordnumber{3}-order polynomial function. When using a \engordnumber{3}-order polynomial to fit even a small number of samples (Fig.~\ref{fig:polyfit3rd}), the fit extrapolates, \ie is closer to the desired function outside the range of training samples, better than when we use a \engordnumber{20}-order polynomial to fit the same data (Fig.~\ref{fig:polyfit20th}). While the number of samples can help the fit of the higher order function, even with a large number of samples the \engordnumber{20}-order polynomial fit  (Fig.~\ref{fig:polyfit20thlots}) will not extrapolate as well as the polynomial with a more appropriate lower number of parameters (Fig.~\ref{fig:polyfit3rdlots}). Similarly, a neural network with a large number of parameters may not generalize as well as a neural network with fewer more salient parameters.
    
    \citet{caruana2001overfitting} further explored the analogy by training neural networks to fit polynomials, showing that overfitting in neural networks does not seem to be as serious a problem as in polynomials. The greatly over-parameterized neural networks still found relatively good fits. The authors suggest that neural networks trained with backpropogration may be biased towards ``smoother approximations''.
	
	%% NOTE: Polynomial fits are a special case of linear regression where we've used a polynomial basis
	%% Use example of sin wave? We can fit it with a polynomial, but better to reparameterize into a basis
	
	\subsection{Generalization and Parameters in Neural Networks}
	\begin{figure}[tb]
		\centering
		\large
        \renewcommand{\ttdefault}{pcr}
		\begin{subfigure}[t]{0.45\textwidth}
			\begin{center}
			\texttt{00\textbf{11111}00}\\
			\texttt{00\textbf{111}0000}\\
			\texttt{0000\textbf{1}0000}
			\end{center}
			\caption{Binary sequences with one clump}
			\label{fig:oneclump}
		\end{subfigure}
		~
		\begin{subfigure}[t]{0.45\textwidth}
			\begin{center}
			\texttt{00\textbf{11}0\textbf{11}00}\\
			\texttt{00\textbf{1}0\textbf{1}0\textbf{1}00}\\
			\texttt{00\textbf{111}0\textbf{1}00}
			\end{center}
			\caption{Binary sequences with two or more clumps}
			\label{fig:polyfit20th}
		\end{subfigure}
		
        \renewcommand{\ttdefault}{lmodern}
		\caption[Two-or-more Clump Predicate]{The two-or-more clumps predicate asks for the network to classify (padded) binary input sequences as having one or two or more contiguous strings of ones.}
		\label{fig:tomclumps}
	\end{figure}
	
	\citet{denker1987large,giles1987learning} explore the relationship between network architecture and generalization by evaluating networks for solving the \emph{two-or-more clumps} predicate (truth statement). The two-or-more clumps predicate asks for the network to classify binary input sequences as having one or two or more contiguous strings of ones, some examples of which are shown in Fig.~\ref{fig:tomclumps}. The predicate is largely based on the more general predicate of \emph{connectedness} explored by \citet{minsky1988perceptrons}, and is shown to be a problem not linearly separable, or more specifically solvable by a locally connected perceptron. 
	
	The authors illustrate some surprising properties of the generalization of fully-connected neural networks learned with backpropogation. First, a human-preferred `geometric' solution is manually hard-coded into the weights of a fully-connected network. While this weight configuration is a valid solution, and is intuitive to humans, the authors show that is is not a solution that the network would ever settle upon when trained with backpropogation. By using the geometric solution as an initialization, and training the network further, the authors show that the error-surface around the region is not stable,
	
	\begin{figure}[tb]
		\centering
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/tomplotmintrainloss.pgf}}
			\caption{Training Loss}
			\label{fig:tomplotmintrainloss}
		\end{subfigure}
		~
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/tomplotminvalloss.pgf}}
			\caption{Validation Loss}
			\label{fig:tomplotminvalloss}
		\end{subfigure}\\
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/tomplotmaxtrainacc.pgf}}
			\caption{Training Accuracy}
			\label{fig:tomplotmaxtrainacc}
		\end{subfigure}
		~
		\begin{subfigure}[t]{0.49\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/tomplotmaxvalacc.pgf}}
			\caption{Validation Accuracy}
			\label{fig:tomplotmaxvalacc}
		\end{subfigure}
		\caption[Two-or-more Clumps Problem and Structural Priors]{TODO. Class imbalance responsible for cycle in loss?}
		\label{fig:tomplot}
	\end{figure}

\subsection{No Free Lunch Theorem}
\begin{chapquote}{Zoubin Ghahramani, \textit{Microsoft Research AI Summer School, 2017}}
	``Everything, but the data itself, is an assumption.''
\end{chapquote}

The \emph{No Free Lunch} theorem (NFL)~\citep{wolpert1996lack} states that no algorithm performs better than any other when averaged over all possible problems (\ie all possible data-generating distributions) of a particular type, as each algorithm makes assumptions which will bias it towards different types of data distributions. Intuitively, if you are tasked with predicting the unobserved members of a large set, only from a few samples, and without using any assumptions whatsoever, you cannot do better than random on average. Any assumptions you do make on the pattern or relationship between members of the set will give you better performance for some data types, and worse for others. The NFL theorem simply formalizes the notion that we cannot infer a rule describing an entire set without either information or assumptions on every member of the set.

\subsubsection{Formal Definition}
Here we present a formal definition of the theorem specialized for supervised learning, as described by \citet{lattimore2013no}: Let $X$ and $Y$ be the input and label sets representing the input and outpus space of the classification problem respectively. The classification itself is defined as a mapping $f: X \to Y$, where $y=f(x)$ is the desired class label of input $x$. Let the training set $X_m$ be the subset of the input space $X$ which our classification algorithm is trained on, and the unseen input space is $X_u = X - X_m$. Let the classification algorithm be defined $A(f_{X_{m}}, x)$, where for $x \in X_u$, $A(f_{X_{m}}, x)$ is the guess for the class label of input $x$, and $f_{X_{m}}(x) = f(x)$ where $x\in X_m$. Let the loss function be defined,

\begin{align}
    L_A(f, X_m) = \frac{1}{X_u} \sum_{x\in X_u} 1_{A(f_{X_m}, x) \neq f(x)},
\end{align}

where $1_{t}$ is the indicator function, returning 1 when $t$ is true, and 0 otherwise. The loss function measures the number of misclassifications on the training data of the classification algorithm. The expected loss on all functions $\mathcal{M}: X \to Y$ is then,

\begin{align}
    E\left[ L_A\left(P, X_m\right) \right] = \sum_{f\in \mathcal{M}} P(f)L_A(f, X_m)
\end{align} 

where $P$ is a probability distribution on $\mathcal{M}$.

\begin{theorem}[No Free Lunch]\label{NFL}
Let $P$ be a uniform probability distribution on $\mathcal{M}$. For any algorithm $A$ and training data $X_m\subseteq X$,
\begin{align}
    E\left[ L_A\left(P, X_m\right) \right] = | Y - 1 |/|Y|,
\end{align}
\end{theorem}

\ie no algorithm can do better than random on all possible problems, given only the training data with no other biases.

The NFL theorem leads many to claim that there is no such thing as a universal learner, however it is important to note that in practice the data distributions that we are interested in represent a small number of all possible data distributions, so it's not inconceivable that a learning algorithm with assumptions may be able to do well on many different real world datasets. Formalizing this argument, \citet{lattimore2013no} claim that without contradicting NFL, biased learning algorithms may exist that do well on universal problems. Their main insight is that the assumption of a uniform probability function $P$ on data distributions may be unrealistic, and under other distributions a biased learner may do well even averaged over all distributions.

\subsubsection{No Free Lunch and Structural Priors}
The NFL theorem highlights the need to focus on designing learning algorithms that work well for the real-world data distributions we are interested in. This may be considered to be the \emph{raison d'\^{e}tre} of structural priors, however this discounts the generality of the theorem. All neural networks makes implicit assumptions even without structural priors. For example, using a single-hidden  layer neural network assumes that the problem being learned is not linear. If the problem is linear, instead a single layer network (perceptron) would suffice, and likely generalize better. The NFL is, however, important in pointing out that structural priors in neural networks are not a `hack'. In fact well-informed assumptions are the reason machine learning works in practice.

\section{Growing Neural Network Architectures}
	%http://blog.otoro.net/2016/09/28/hyper-networks/
	Given the effect of structure and network design on generalization, what if instead of designing a neural network before training, we could build networks from the ground up based on data? This appealing direction of research has been the approach taken by \citet{mezard1989learning,Fahlman1989,MacKay91,mackay1992practical,hypernetworks} amongst others. A survey of such methods is presented by \citet{parekh2000constructive}.
	
	The earliest proposals for building neural network structures from the data itself were proposed for \emph{boolean classification} networks, \ie classifying binary patterns and returning a binary result. The \emph{tiling algorithm}~\citep{mezard1989learning} is guaranteed to build a network that can achieve perfect classification of the training set. For each layer in the network, a master neuron is fist used to provide the best linear separation of the data, and then ancillary neurons are added until perfect classification of the training data is achieved. In practice this is not necessarily good for generalization to the validation set however. 
	
	Cascade Correlation~\citep{Fahlman1989} is a constructive algorithm for building general neural networks from only a basic two-layer network of input and output nodes. It is based on two key ideas, a \emph{cascade} where hidden units are added to the network one at a time, and \emph{correlation}\footnote{In fact it is covariance that is used rather than correlation, since no normalizing factors are used, as noted by \citet{Fahlman1989}.} where the output of the new hidden unit is learned such that it is highly correlated with the error residual being minimized in the existing network.
	
	\begin{figure}[tbp]
		\centering
		\includegraphics{Figs/PDF/fahlmancascade}
		\caption[Cascade Correlation]{Cascade Correlation. Connections marked with a $\square$ are frozen, while $\times$ connections are learned continously. \copyright~\citet{Fahlman1989}}
		\label{fig:cascadecorrelation}
	\end{figure}
	
	At the beginning of training only the input and output layers exist, as shown in Fig.~\ref{fig:cascadecorrelation} and since this is a single layer network with no hidden units, any of the simpler perceptron training rules can be used. After sufficient training, which is heuristically determined, the network is expanded by a single new hidden unit at a time.
	Each new (candidate) hidden unit that is added to the network is connected to all the network inputs, in addition to all of the outputs of the previous hidden units. The candidate unit is treated as a new hidden layer, and since all other connections are frozen, can be trained as if it was a single layer - \ie with the delta rule, or other similar single layer training algorithm, rather than backpropogation. The candidate unit's input weights are adjusted so as to maximize the covariance of the candidate unit's output with the error output of the network:
	% http://www.ra.cs.uni-tuebingen.de/SNNS/UserManual/node167.html#SECTION001081200000000000000
	\begin{equation}
	    C = \sum_o \left|\sum_p (y_{po} - \overline{y_o})(E_{po} - \overline{E_o})\right|,
	\end{equation}
	where $y_{po}$ is the output of the output unit o with input pattern p, $E_{po}$ is the error The trained unit's input weights are then frozen after training, and if an error threshold hasn't been met, a new layer is again added.
	
	Since each new neuron produces a new layer, cascade correlation leads to very deep networks with low throughput, since each layer is only one neuron wide, making them very inefficient computationally. Cascade correlation networks do not seem to generalize as well as standard neural networks, likely due to their greedy training strategy, as compared to deep networks trained with backpropogation where no weights are frozen during training.
	
	
	\subsection{Hyper-Networks}
	More recently \citet{hypernetworks} looked at trying to learn a compact representation of a network, much like weight sharing in an RNN allows the concise summary of a potentially infinite depth RNN.
	\mynote{todo talk about MaKay's work, hyper-networks}
	\section{Pruning Neural Networks}
    Rather than the progressive construction of networks, an alternative approach to learning efficient networks is \emph{pruning}, the removal of unimportant network weights after training. There have been many proposals of methods of pruning network connections~\citep{lecun1989optimal,sietsma1988neural,Xing2009,han2015deep,ullrich2017soft,} differing mostly on the method of evaluating the saliency of weights, \ie how important each weight is to maintaining generalization. 
    
    In a typical pruning algorithm, after having trained the network, a saliency measure for each network weight is calculated. The weights are then sorted by saliency, and the lowest saliency parameters are deleted. After this pruning step, the network must again re-trained from scratch. This iterative re-training means that such pruning methods do not scale well to contemporary neural networks where training times can be measured in weeks.

    The simplest saliency measure would be to simply measure the effect of removing a weight on the training error of the network. For each weight in the network, the training error can be evaluated, and the difference in error used as the saliency measure for that weight. In practice this straightforward method is infeasible however, since we must evaluate over the training set for each weight of the network. For a network with $n$ weights, and $p$ training samples, calculating the saliencies for the whole network in this manner is $\mathcal{O}(np)$~\citep{hanson1989comparing}.
%    A simpler naive saliency measure, weight magnitude, is overly simplistic in practice since it does not account for the larger distributed representation learned in a neural networks and can lead to a large reduction in accuracy if used for pruning.
    
    \citet{lecun1989optimal} proposed Optimal Brain Damage (OBD) to iteratively remove neurons after training based on a saliency measure that judges the parameters that have the least effect on training error. The authors point out that simply using the magnitude of the weights themselves is equivalent in the limit to having trained with a form of non-proportional weight decay, instead the authors propose a more theoretically justified saliency measure -- they propose measuring the change in the objective function caused by deleting a parameter. Rather than performing the computationally intensive task of re-evaluating the objective for every possible parameter deletion, the second derivative of the objective with respect to the weights is instead used. The authors approximate the derivative of objective function with respect to the weights using the second-order Taylor series expansion, $f(a) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x - a)^n$, of the error:
       
    % From optimal brain surgeon paper
    % https://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf
    % also look at:
    % http://cslt.riit.tsinghua.edu.cn/mediawiki/images/9/9e/131125-OBD-LC-01.pdf
    % http://web.engr.oregonstate.edu/~tgd/classes/534/slides/part10.pdf

    \begin{align}
       \delta E &= \sum_i \frac{\partial E}{\partial w_i} \delta w_i + \frac{1}{2} \sum_i \frac{\partial^2 E}{\partial w_i^2} \delta w_i^2 + \frac{1}{2} \sum_{i\neq j} \frac{\partial^2 E}{\partial w_i \partial w_j} \delta w_i \delta w_j +\mathcal{O}(\|\delta \mathbf{w}\|^3),
    \end{align}

    where $E$ is the objective function, $w_i$ is the $i$th weight. We can write this more compactly in matrix notation,
    \begin{align}
       \delta E &= \left( \frac{\partial E}{\partial \mathbf{w}} \right)^T \delta \mathbf{w} +\frac{1}{2}\delta \mathbf{w}^T \mathbf{H}\, \delta \mathbf{w} +\mathcal{O}(\|\delta \mathbf{w}\|^3),
    \end{align}
    where $\mathbf{w}$ is the vector of network weights, and $h_{ij}=\partial^2 E / \partial w_i \partial w_j$ is the Hessian matrix of second order derivatives. 

    This approximation is infeasible to calculate in practice, mostly due to the size of the Hessian matrix which is square in the number of weights. The authors use several assumptions to approximate this in a more computationally efficient manner. Most importantly they use the ``diagonal approximation'' of the Hessian, using only the diagonal terms, $h_{ii}=\partial^2 E / \partial w_i^2$. Further, the ``extremal approximation'' assumes that pruning is only done once the training has converged, and so it can be assumed that the first term is zero. Finally they assume that the function is approximately quadratic, and thus higher order terms (\ie $\mathcal{O}(\|\delta \mathbf{w}\|^3)$) can be ignored. In all, the final approximation of the objective function's change is simply:

    \begin{align}
       \delta E &\approx \frac{1}{2} \sum_i \frac{\partial^2 E}{\partial w_i^2} \delta w_i^2.
    \end{align}

    To calculate the second derivative, chain rule is used, and the second derviatives are back-propogated through the network. This approximation is used directly as the saliency measure for each network weight $w_i$, and after having trained the network, the weights are sorted by saliency, and the lowest saliency parameters are deleted.
    
    \citet{hanson1989comparing} proposed using a per-neuron gating mechanism to measure saliency
    
	\mynote{TODO}
	\citet{lecun1989optimal,sietsma1988neural,Xing2009,han2015deep,ullrich2017soft,}
    \section{Compression \& Quantization}
    In order to use deep neural networks on embedded devices, and in many applications, first a compact representation is required. There have been many papers on compressing deep networks, notably the work of ~\citep{han2016dsd} looked at combining a naive weight magnitude-based pruning method with both quantization and Huffman coding. In all the method can take a trained network
	\mynote{TODO}
	
\section{The Neocognitron}
	In an era in which fully connected networks were used to learn any input type, \citet{Fuk80} showed that for structured inputs a drastically different architecture could make a big difference in generalization. The \emph{Neocognitron}~\citep{Fuk80, fukushima2013artificial} was a biologically motivated architecture, motivated by what are typically called simple and complex cells in the primary visual cortex (V1), as found by \citet{Hubel1959a}. To model simple cells; cells whose response correlated with simple oriented edges in a translation invariant manner, the neocognitron used shared weights which were connected to local image patches of the input image (and were not simply described as convolution of a filter). Complex cells were modelled by a ``blurring'' operation, what we now term more generally as \emph{pooling}. The neocognitron network consisted of alternating layers of simple and complex cells, \ie alternating convolution and pooling layers, much as seen in state of the art convolutional networks.

	While the Neocognitron was ahead of its time, and is now recognized as the first iteration of what were to become convolutional neural networks, the article's neurological focus and explanation, timing, and to some degree country of origin, meant that it was somewhat unnoticed in the mainstream field of connectionist research. %In fact Yann LeCun recounts specifically his interest in Japan, rather than any particular citation, having lead to his discovery of the work\footnote{As related in a Q\&A session at the 2016 International Computer Vision Summer School (ICVSS)}.
	
	\section{Convolutional Neural Networks}
	
	Despite the pioneering novelty of the work on neocognitrons, it was only following the simplification and improvement of~\citet{lecun1989backpropagation,Lecun1998} in both the description of the network and its operation that it gained wider acknowledgement as a breakthrough for image recognition. In their work the local shared weights of the neocognitron are put in the context of convolution, and the averaging operation replaces with max-pooling. The application to handwritten digit recognition gave state of the art results, and would result in the \emph{LeNet5} network, still used today in commercial applications.
	
	The application of the LeNet-style CNN architecture to more complex problems, however, proved infeasible. These problems required a deeper hierarchy of representation, which implied a large number of layers. Networks with a large number of layers proved to be un-trainable due to numerous issues with the model itself, notably vanishing gradients~\citep{hochreiter1991untersuchungen}, and the lack of large datasets and computational power at the time. Convolutional neural networks fell out of favour, and were passed over in favour of the more successful paradigm of using hand-crafted local features for many tasks, and in particular the problem of object instance recognition was well addressed by such solutions. Meanwhile object class recognition remained a difficult problem, for which the best solutions were deformable parts models, also based on local features.
	
\section{Deep Networks}
	% deep boltzmann networks
	\citep{Krizhevsky2012}
	\citep{Simonyan2014verydeep}
	\citep{He2015}
	\citep{He2016}


\section{Structural Priors}
    This section has shown that the concept we have named `structural priors' is not new, and has varied interpretations. Here we will attempt to summarize the concepts covered by this large body of work in a compact nomenclature. While, like with any compact nomenclature, there is a danger of overly simplifying the details, it can also help in seeing the big picture.
    
	In practice when fitting a curve, we have little idea of what order polynomial would best fit the data. Necessarily, we must use a relatively higher order curve to fit the data. Similarly, with a neural network we rarely have knowledge of the underlying structure of the solution (but when we do, we should use it to parameterize our models appropriately~\citep{jain2016structural}). Instead we must use networks with more parameters than necessary to ensure that there is enough capacity to learn the underlying, but likely sparse solution. The problem with this approach is that over-parameterization of a model generally leads to poor generalization due to overfitting. To prevent this, there are two types of methods in which we can relate our prior knowledge that the model is over-parameterized to the optimization:
	\begin{description}
	\item[Weak Priors]
	Knowing only that our model is over-parameterized is a relatively weak prior, however we can encode this into the fit by using a regularization penalty. This restricts the model to effectively use only a small number of the parameters by adding a penalty, for example on the L2 norm of the model weights. For polynomial regression, this is called \emph{ridge regression}, while for neural networks it is called \emph{weight decay}~\cite{hinton1987learning}. Early stopping is another method for doing this.
	
	\item[Strong Priors]
	With more prior information on the task, \eg from the convexity of the polynomial, we may imply that a certain order polynomial is more appropriate, and restrict learning to that order. For example, given samples from a polynomial appear to be convex, we can surmise that the polynomial is likely to be of an even or \engordnumber{2}-order, and restrict our fit to be of that order. 
    \end{description}
    
	Although neural networks are usually posed as general learning machines, time and again it has been demonstrated that neural networks only truly stand out as a learning method when we use strong structural priors, encoding our prior knowledge of the task in the architecture itself. As observed by \citet{denker1987large} this may be considered closer to modifying the problem to be solved itself, rather than changing the learning method alone. For example, by asking a CNN to learn to classify a dataset, we are asking the network to ``Classify these types of images'', whereas by asking a fully-connected network to classify the same dataset, we are asking the network ``Classify these types of data''. The first task is inherently easier.

    Structural priors may be considered one of the greatest contributions to the success of deep learning, but arguably can also be considered the cause of its greatest failure. Structural priors allow deep models to more easily learn specific tasks, \eg image classification or speech recognition. At the same time, by specializing our network models, we are moving further away from the goal of general artificial intelligence, \ie learning models that can do both tasks.
    
    \section{Quotes}
    
	\begin{chapquote}{David Rumelhart, \textit{in personal communication with \citet{hanson1989comparing}, 1987}}
	``\ldots the simplest most robust network which accounts for a data set will, on average, lead to the best generalization to the population from which the training set has been drawn''
	\end{chapquote}
	
	
	\begin{chapquote}{John Denker \etal, \textit{Large Automatic Learning, Complex Systems, 1987}}
		``A general tabula rasa network is a fine subject for the abstract, formal studies, but one should not try to use it to solve practical problems. \ldots One should pre-program the network with all available information about the structure of the problem, especially information about the symmetry and topology of the data.''
	\end{chapquote}
	
	
	\begin{chapquote}{Mozer \etal, \textit{Using Relevance to Reduce Network Size Automatically, 1989}}
	``One thing that connectionist networks have in common with brains is that if you open them up and peer inside, all you can see is a big pile of goo.''
	\end{chapquote}
	
	
	\begin{chapquote}{David MacKay, \textit{A Practical Bayesian Framework for Backprop Networks, 1991}}
		``There are many knobs on the black box of 'backprop' (learning by back-propagation of
		errors). Generally these knobs are set by rules of thumb, trial and error, and the use of reserved test data to assess generalization ability (or more sophisticated cross-validation).''
	\end{chapquote}
	
    
	\begin{chapquote}{Kunihiko Fukushima, \textit{Neocognitron, 
				%A Self-organizing Neural Network Model
				%for a Mechanism of Pattern Recognition
				%Unaffected by Shift in Position , 
				Biol.\ Cybernetics, 1980}}
		`` One of the largest and long-standing difficulties in designing a pattern-recognizing machine has been the problem how to cope with the shift in position and the distortion in shape of the input patterns. The neocognitron proposed in this paper gives a drastic solution to this difficulty.
		''
	\end{chapquote}
	
	
	\begin{chapquote}{Yann LeCun, \textit{Backpropagation Applied to Handwritten Zip Code Recognition, 1989}}
		``Classical work in visual pattern recognition has demonstrated the advantage of extracting local features and combining them to form higher order features. Such knowledge can be easily built into the network by forcing the hidden units to combine only local sources of information. Distinctive features of an object can appear at various location on the input image. Therefore it seems judicious to have a set of feature detectors that can detect a particular instance of a feature anywhere on the input place. Since the precise location of a feature is not relevant to the classification, we can afford to lose some position information in the process. Nevertheless, approximate position information must be preserved, to allow the next levels to detect higher order, more complex features.''
	\end{chapquote}
\end{document}
