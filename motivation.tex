% !TEX root = thesis.tex
\documentclass[thesis]{subfiles}

\begin{document}
	\chapter{The Effect of Structure on Learning}\label{motivation}
	%\chapter{The Effect of Structure on Learning in Neural Networks}\label{motivation}
	\begin{chapquote}{Zoubin Ghahramani, \textit{Microsoft Research AI Summer School, 2017}}
		``Everything, but the data itself, is an assumption.''
	\end{chapquote}
	It is well known that the design of a neural network\index{neural network} architecture can have a large effect on the generalization of a learned model; and yet network design itself remains poorly understood, with intuition and experience being the cited motivation behind most common architectures, rather than theory. This, more than perhaps any other factor, has been a barrier to access for the practical use of neural networks\index{neural network} by people who are not experts in the field.
	
	Beyond hyper-parameters used for tuning the optimization method, such as learning rate, momentum and weight decay\index{weight decay}, the architecture of a network has a profound effect on the learning. Nowhere is this effect more pronounced than in the case of using neural networks\index{neural network} with highly structured inputs, such as natural images. Although neural networks\index{neural network} are usually posed as general learning machines, time and again it has been demonstrated that neural networks\index{neural network} only truly stand out as a learning method when we encode our prior knowledge of the task in the architecture itself --- a concept that we will, throughout this work, refer to as \emph{structural priors}. Examples of structural priors include common network architectures for images (\glspl{cnn}\index{CNN}), and sequences (\glspl{rnn}).
	
	Neural Networks with structural priors still differ significantly from hand-tuned local features, as popularized in computer vision in the early 2000s, such as \gls{sift}~\citep{Lowe2004}. As compared with neural networks\index{neural network}, such local features are rigidly defined in terms of structure and weights, and the learning system is restricted to finding and cataloguing the pre-determined features in images. Neural networks with structural priors on the other hand, while restricting the structure of the network somewhat, still allow the network to learn more fine-grained structure, and have no effect on the latitude given to learning weights.
	
	% This is why NN are not good at general learning, and we are now only good at expert systems
	
	The history of understanding the role of neural network\index{neural network} architecture in learning is long, arguably going back to the Hebbian rule of learning~\citep{hebb1949organization}, and yet our understanding is still far from complete. In this section we will review a select number of the most important previous works relevant to understanding the role that structural priors play, and how they emerged to dominate the practical use of neural networks\index{neural network} today.
	
	\section{Network Architecture}
    A persistent question in training artificial neural networks\index{neural network} has been in the design of the networks. Specifically the question of how many parameters should be learned, and in what way they should be connected,  so as to be suitable for good generalization from a given size dataset. Notable steps in the theoretical answers to this question include findings showing the limitations of single-layer networks~\citep{minsky1988perceptrons}, information-theoretic measures of the representational capacity of a network~\citep{vapnik2015uniform}, the proof that single hidden-layer networks are universal approximators~\citep{hornik89a}, and the theoretical number of nodes required for generalization from a dataset of given size~\citep{baum1989size}. 
    
    Empirical results have, however, shown that the realities of training neural networks\index{neural network} do not match what theory predicts. Deep networks of many hidden layers have been shown time and again to out-perform shallow networks~\citep{Krizhevsky2012,Simonyan2014verydeep,He2015,He2016}, perhaps due to our limited method of optimization~\citep{NIPS2014_5484}. Networks with many more parameters than training samples, that use early-stopping or are regularized strongly, generalize better in practice than networks with the theoretically sufficient capacity~\citep{caruana2001overfitting, Krizhevsky2012, HintonTalk2015}. Networks designed with a specialized connectivity structure closer reflecting the underlying solution have consistently generalized better than fully-connected networks with higher learning capacity~\citep{lecun1989backpropagation,He2016}. In fact these seemingly theory-defying design strategies can claim to have been responsible for recent breakthroughs in generalization on previously difficult tasks such as image class recognition~\citep{Krizhevsky2012, HintonTalk2015}.
    
    \section{Model Capacity and Representational Power}
    \begin{figure}[tbp]
        \centering
        \includegraphics[width=0.85\textwidth]{Figs/PDF/allpossibler3}
        \caption[Possible labellings of 3 points in $\mathbb{R}^2$. ]{\textbf{Possible labellings of 3 points in $\mathbb{R}^2$.} All possible labellings of 3 points in $\mathbb{R}^2$ can be separated by a oriented line (2D hyperplane). This is not possible for all labellings of 4 points in $\mathbb{R}^2$ however, and thus the \gls{vc}\index{VC dimension} dimension of oriented hyperplanes in $\mathbb{R}^2$ is 3. Inspired by figure in \citet{burges1998tutorial}.}\label{fig:vcdim_r2line}
    \end{figure}
    The information-theoretic notion of capacity, that is the expressive power of a classification algorithm, gives important insights to the learning ability of a classification algorithm. Analysis is typically based on the Vapnikâ€“Chervonenkis (\gls{vc}\index{VC dimension}) dimension~\citep{vapnik2015uniform} of the class of functions used as discriminators, \eg hyperplanes in the case of neural networks\index{neural network}. Intuitively, for a discriminative classifier, the \gls{vc}\index{VC dimension} dimension measures the largest number of points that can be classified without error. In such a case, the set of points is said to be \emph{shattered} by the classifier. A good overview of \gls{vc}\index{VC dimension} dimension is given by \citet{burges1998tutorial}.
    
    \subsection{Vapnik-Chervonenkis Dimension}
    More formally, a classification model $f(\theta)$, parametrized by $\theta$ is said to \emph{shatter} a set of data points $(\gls{x}_0, \gls{x}_1, \ldots, \gls{x}_h)$ if for all possible labellings of the points, the classification model can perfectly learn the points. The \gls{vc}\index{VC dimension} dimension is the largest number of (any) points that can be shattered by such a classifier. For a classifier of \gls{vc}\index{VC dimension} dimension $h$, it is sufficient that there exists a \emph{single} set of $h$ points which can be shattered. It is important to note that in general a classifier with a \gls{vc}\index{VC dimension} dimension of $h$ will not necessarily shatter every possible set of $h$ points. 
    %Note that, if the \gls{vc}\index{VC dimension} dimension is h, then there exists at least one set of h points that can be shattered, but it in general it will not be true that every set of h points can be shattered
    
    For example, in \cref{fig:vcdim_r2line}, the function class of oriented hyperplanes, \ie lines in 2D, can separate all possible labellings of 3 points in $\mathbb{R}^2$ --- oriented hyperplanes in $\mathbb{R}^2$ shatter 3 points. However, for 4 points, this is no longer true. It can be proven~\citep{burges1998tutorial} that in general for $\mathbb{R}^n$, the set of oriented hyperplanes shatters any set of $n+1$ points.

    The \gls{vc}\index{VC dimension} dimension gives us a measure of the theoretical learning capacity of a classifier, however it can also be somewhat counter-intuitive. While models with large numbers of parameters usually will have a higher \gls{vc}\index{VC dimension} dimension, there are examples of small single parameter models with infinite \gls{vc}\index{VC dimension} dimension for more specific sets of points. For example, if we have a set of evenly spaced points in 2D, a simple sinusoidal curve with the appropriate phase can shatter any labelling of an infinite number of such points. However, such a classifier would be poor at classifying more general sets of points, despite the impressive theoretical \gls{vc}\index{VC dimension} dimension.
    
    \subsection{VC Dimension of Neural Networks}\label{vcdim}
    In the case of neural networks\index{neural network}, early work showed the capacity of neural networks\index{neural network} to be quite large~\citep{hornik89a,baum1989size}. \citet{baum1989size} looked at feed-forward networks of \emph{threshold units}, \ie perceptrons\index{perceptron}. %showing the VC dimension of a single-layer network with k units and w weights (including biases) to be bounded by,
    %\begin{equation}
    %    d_{\textrm{VC}} \leq 2 w \log_2(e\,k),
    %\end{equation}
    
    %where $e$ is the mathematical constant, the base of natural logarithms. 
    The authors prove a lower bound on the \gls{vc}\index{VC dimension} dimension for a single hidden layer network of $k$ units and $n$ inputs,
    \begin{equation}
        d_{\textrm{\gls{vc}\index{VC dimension}}} \geq 2 \lfloor k/2 \rfloor n,
    \end{equation}
    where $\lfloor \ \ \rfloor$ is the floor operation, \ie largest integer less than or equal to the operand, and $d$ is the number of inputs. For a single hidden layer network with a large number of $n$ inputs and $k$ units, the authors make the assumption that $kn\approx w$, \ie the number of weights in the first layer alone is approximately that of the whole network $w$,
    \begin{equation}
        d_{\textrm{\gls{vc}\index{VC dimension}}} \geq w.
    \end{equation}

    \citet{baum1989size} use this lower bound on the \gls{vc}\index{VC dimension} dimension of a hidden layer to bound the number of training samples required to achieve an error rate of $\epsilon$, showing that for a network with $w$ weights, and a desired error rate $\epsilon$ the minimum number of training samples required is given by,
    
    \begin{equation}
        N_{\min} \approx w/\epsilon.
    \end{equation}
    
    For an error rate of $\epsilon=0.1$, this gives the rule of thumb that for a network with a total of $w$ weights, approximately $10\times w$, or 10 times the number of training samples as weights in the network, are required to guarantee an error rate of 10\%. 
    
    At first glance this work may seem to have solved a major problem in the design and training of neural networks\index{neural network}, however the work of \citet{baum1989size} is to show a worst-case lower bound on the number of training samples required --- in practice this can be far from what is empirically required. \citet{baum1989size} themselves point out that this is likely be far more than necessary in networks where the learning algorithm seeks to minimize the number of non-zero weights (such as networks using weight decay, pruning, etc). Indeed, in practice neural networks\index{neural network} were found to generalize much better than this worst case bound would indicate, to the point where modern deep neural networks\index{neural network} are trained with far fewer samples than weights, in the case of AlexNet~\citep{Krizhevsky2012} approximately $250\times$ \emph{fewer} training samples than weights allows good generalization on the \gls{ilsvrc}~\citep{ILSVRC2015} dataset.
    
    \citet{bartlett1997} later showed that rather than capacity being based solely on the number of weights, a bound more in-line with empirical results could be found by using the number of \emph{large weights}. To show this, they moved to a scale sensitive form of the \gls{vc}\index{VC dimension} dimension: the \emph{fat-shattering} dimension. The author shows that the error rate for an $\ell$-layer sigmoidal (rather than threshold unit) neural network\index{neural network} with $n$ inputs and $m$ training samples,
    %
    \begin{equation}
        \epsilon \approx (cA)^{\ell(\ell+1)/2} \sqrt{(\log n)/m},
    \end{equation}
    %
    and $c$ is a constant factor, and the $\ell_1$ norm of each unit's weight vector $w$ is bounded by $A$, 
    \begin{equation}
    \|\gls{vectorw}\|_1 = \sum_i | \gls{w}_i | \leq A.
    \end{equation}
    Surprisingly there is no term for the number of units for any layer in this equation, but rather it is the bounds on the weights themselves that determine ability of the network to generalize. In general, for a network where the inputs $\mathbf{x}$ are also bounded,
    \begin{equation}
        \|\mathbf{x}\|_\infty = \max(|\gls{x}_0|, |\gls{x}_1|, \ldots, |\gls{x}_n|) \leq B,
    \end{equation}
    \citet{bartlett1997} show that for a given error $\epsilon$, the number of training samples $m$ required grows roughly as,
    \begin{align}
        m \approx \frac{B^2\,A^{\ell^2}}{\epsilon^2}.
    \end{align}
    %
    For a single hidden layer network, \ie $\ell=2$, and an error rate of $\epsilon=0.05$, 
    %
    \begin{align}
        m \approx 400\,B^2\,A^6.
    \end{align}
    %
    This result, while surprising given the analysis based on the \gls{vc}\index{VC dimension} dimension of neural networks\index{neural network}, supports empirical results in using contemporary training methods such as weight decay~\citep{hinton1987learning}\index{weight decay|textbf}, early stopping~\citep{Bishop1995}\index{early stopping|textbf}, and even more recently batch normalization~\citep{Ioffe2015}\index{batch normalization}, all of which can keep weight magnitudes low.
%    For a single hidden layer network, \ie $\ell=2$, and an error rate of $\epsilon=0.05$, 
    
%    \begin{align}
%        m &\approx  \frac{\log n \, (cA)^{\ell(\ell+1)}}{\epsilon^2}\\
%        &\approx  \frac{\log n \, (cA)^{6}}{0.0025},\\
%    \end{align}
%    we can see that the number of training samples required to guarantee this error rate are proportional to the logarithm of the number of inputs and the , 
	
    \subsection{Model Size}\label{modelsize}
	\begin{figure}[tbp]
		\centering
		\begin{subfigure}[t]{0.48\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit3rd.pgf}}
			\caption{\engordnumber{3}-order polynomial fitting 3 points}
			\label{fig:polyfit3rd}
		\end{subfigure}
		~
		\begin{subfigure}[t]{0.48\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit20th.pgf}}
			\caption{\engordnumber{20}-order polynomial fitting 3 points}
			\label{fig:polyfit20th}
		\end{subfigure}\\	
		\begin{subfigure}[t]{0.48\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit3rdlots.pgf}}
			\caption{\engordnumber{3}-order polynomial fitting 10 points}
			\label{fig:polyfit3rdlots}
		\end{subfigure}
		~
		\begin{subfigure}[t]{0.48\textwidth}
			\resizebox{\linewidth}{!}{\input{Figs/matplotlib/polyfit20thlots.pgf}}
			\caption{\engordnumber{20}-order polynomial fitting 10 points}
			\label{fig:polyfit20thlots}
		\end{subfigure}
		\caption[Polynomial fits of samples from a \engordnumber{3} order function]{\textbf{Polynomial fits of samples from a 3rd order function.} Polynomials of high order, like neural networks\index{neural network} of many parameters, easily overfit a small number of samples as compared to polynomials of a more suitable order for the sampled function. While generalization is helped by more data, the higher-order polynomial still tends to overfit.}
		\label{fig:polyfits}
	\end{figure}

	\citet{denker1987large} explored the relationship of network architecture to generalization in a more empirical manner. The work was particularly motivating in the later design of \glspl{cnn}\index{CNN}~\citep{lecun1989generalization, lecun1989backpropagation}. The authors make the intuitive analogy between the affect of the size of a neural network\index{neural network} on its generalization, and a simple least-squares polynomial fit. \Cref{fig:polyfits} shows various polynomial fits to samples from a \engordnumber{3}-order polynomial function. When using a \engordnumber{3}-order polynomial to fit even a small number of samples (\cref{fig:polyfit3rd}), the fit extrapolates, \ie is closer to the desired function outside the range of training samples, better than when we use a \engordnumber{20}-order polynomial to fit the same data (\cref{fig:polyfit20th}). While the number of samples can help the fit of the higher-order function, even with a large number of samples the \engordnumber{20}-order polynomial fit  (\cref{fig:polyfit20thlots}) will not extrapolate as well as the polynomial with a more appropriate lower number of parameters (\cref{fig:polyfit3rdlots}). Similarly, a neural network\index{neural network} with a large number of parameters may not generalize as well as a neural network\index{neural network} with fewer, more salient, parameters.
    
    More recently \citet{caruana2001overfitting} further explored the analogy by training neural networks\index{neural network} to fit polynomials, showing that overfitting in neural networks\index{neural network} does not seem to be as serious a problem as in polynomials. The greatly over-parametrized neural networks\index{neural network} still found relatively good fits. The authors suggest that neural networks\index{neural network} trained with backpropagation\index{backpropagation} may be biased towards ``smoother approximations''.
	
	%% NOTE: Polynomial fits are a special case of linear regression where we've used a polynomial basis
	%% Use example of sin wave? We can fit it with a polynomial, but better to re-parametrize into a basis
	
	\subsection{Generalization and Parameters in Neural Networks}
	There has been a lot of research into the relationship between the number of parameters of a network and its generalization, aside from theoretical work presented in \cref{vcdim}, there were many empirical studies, especially in the earlier years of connectionist research~\citet{denker1987large, giles1987learning, hinton1987learning, lecun1989generalization, ahmad1989scaling,hanson1989comparing}.
	\begin{figure}[tbp]
		\centering
		\large
        \renewcommand{\ttdefault}{pcr}
		\begin{subfigure}[t]{0.45\textwidth}
			\begin{center}
			\texttt{00\textbf{11111}00}\\
			\texttt{00\textbf{111}0000}\\
			\texttt{0000\textbf{1}0000}
			\end{center}
			\caption{Binary sequences with one clump}
			\label{fig:oneclump}
		\end{subfigure}
		~
		\begin{subfigure}[t]{0.45\textwidth}
			\begin{center}
			\texttt{00\textbf{11}0\textbf{11}00}\\
			\texttt{00\textbf{1}0\textbf{1}0\textbf{1}00}\\
			\texttt{00\textbf{111}0\textbf{1}00}
			\end{center}
			\caption{Binary sequences with two-or-more clumps}
			\label{fig:twoclumps}
		\end{subfigure}
		
		\caption[Two-or-more clumps predicate]{\textbf{Two-or-more clumps predicate.} The two-or-more clumps predicate asks for the network to classify (padded) binary input sequences as having one or two-or-more contiguous strings of ones.}
		\label{fig:tomclumps}
	\end{figure}
	\afterpage{
		\begin{landscape}
		\begin{figure}[tbp]
			\centering
			\begin{subfigure}[t]{0.45\linewidth}
				\resizebox{\linewidth}{!}{\input{Figs/matplotlib/tomplotmintrainloss.pgf}}
				\caption{Training Loss}
				\label{fig:tomplotmintrainloss}
			\end{subfigure}
			~
			\begin{subfigure}[t]{0.45\linewidth}
				\resizebox{\linewidth}{!}{\input{Figs/matplotlib/tomplotminvalloss.pgf}}
				\caption{Validation Loss}
				\label{fig:tomplotminvalloss}
			\end{subfigure}\\
			\begin{subfigure}[t]{0.45\linewidth}
				\resizebox{\linewidth}{!}{\input{Figs/matplotlib/tomplotmaxtrainacc.pgf}}
				\caption{Training Accuracy}
				\label{fig:tomplotmaxtrainacc}
			\end{subfigure}
			~
			\begin{subfigure}[t]{0.45\linewidth}
				\resizebox{\linewidth}{!}{\input{Figs/matplotlib/tomplotmaxvalacc.pgf}}
				\caption{Validation Accuracy}
				\label{fig:tomplotmaxvalacc}
			\end{subfigure}
			\caption[Two-or-more clumps problem and structural priors]{\textbf{Two-or-more clumps problem and structural priors.} Our results for of the learning curves for the experiment of \citet{denker1987large}. A small \gls{cnn} converges quicker than a much larger, fully-connected network that contains the super-set of weights not present in the sparser \gls{cnn}. The class-imbalance due to the numbers of samples for the two classes in a fixed-length binary string seems to cause instability in the convergence of both networks.}
			\label{fig:tomplot}
		\end{figure}
	\end{landscape}
	}
	\citet{denker1987large,giles1987learning} explore the relationship between network architecture and generalization by evaluating networks for solving the \emph{two-or-more clumps} predicate (truth statement). The two-or-more clumps predicate asks for the network to classify binary input sequences as having one or two-or-more contiguous strings of ones, some examples of which are shown in \cref{fig:tomclumps}. The predicate is largely based on the more general predicate of \emph{connectedness} explored by \citet{minsky1988perceptrons}, and is shown to be a problem not linearly separable, or more specifically solvable by a locally connected perceptron\index{perceptron}. 
	
	The authors illustrate some surprising properties of the generalization of fully-connected neural networks\index{neural network} learned with backpropagation\index{backpropagation}. First, a human-preferred `geometric' solution is manually hard-coded into the weights of a fully-connected network. While this weight configuration is a valid solution, and is intuitive to humans, the authors show that it is not a solution that the network would ever settle upon when trained with backpropagation\index{backpropagation}. By using the geometric solution as an initialization, and training the network further, the authors show that the error-surface around the region is not stable,

	In \cref{fig:tomplot} we show the learning curves for this problem, using contemporary training methods. The small \gls{cnn} converges quicker than a much larger, fully-connected network that contains the super-set of weights not present in the sparser \gls{cnn} due to weight sharing and missing weights. Something not observed in the original paper however is that the class-imbalance due to the numbers of samples for the two classes in a fixed-length binary string seems to cause instability in the convergence of both networks.

	Work on trying to understand generalization of neural networks is not the domain of the past, and is still an active area of research. One of the more interesting recent papers on generalization in \gls{dnn} highlights that \glspl{dnn} may not always be learning the type of representations we assume. \Citet{rethinking2016} looked at the effect of training \glspl{dnn} where they replaced the labels of the \gls{ilsvrc} dataset with random labels. Surprisingly, \glspl{dnn} can learn the randomly labelled datasets perfectly (\ie zero training error), despite the intrinsic relationship between the labels and images being destroyed. This flies in the face of the commonly accepted explanation that the network is learning to represent the natural data in a low-dimensional manifold in a higher dimension, random labels are unlikely to be learned this way. Instead it seems that \glspl{dnn} are doing a lot more memorization of training data than previously thought. The authors also show that many forms of regularization currently used, including weight decay\index{weight decay} and dropout, are less effective than expected.

	On the subject of the importance of structure for learning 
	glspl{dnn}, these results show that structural priors may be more important than previously thought for generalization than even strong forms of regularization.

\subsection{No Free Lunch Theorem}
\label{nofreelunch}
The \emph{No Free Lunch} theorem (NFL)~\citep{wolpert1996lack} states that no algorithm performs better than any other when averaged over all possible problems (\ie all possible data-generating distributions) of a particular type, as each algorithm makes assumptions which will bias it towards different types of data distributions. Intuitively, if you are tasked with predicting the unobserved members of a large set, only from a few samples, and without using any assumptions whatsoever, you cannot do better than random on average. Any assumptions you do make on the pattern or relationship between members of the set will give you better performance for some data types, and worse for others. The NFL theorem simply formalizes the notion that we cannot infer a rule describing an entire set without either information or assumptions on every member of the set.

\subsubsection{Formal Definition}
Here we present a formal definition of the theorem specialized for supervised learning, as described by \citet{lattimore2013no}: Let $X$ and $Y$ be the input and label sets representing the input and outputs space of the classification problem, respectively. The classification itself is defined as a mapping $f: X \to Y$, where $y=f(x)$ is the desired class label of input $x$. Let the training set $X_m$ be the subset of the input space $X$, on which our classification algorithm is trained, and the unseen input space, $X_u = X - X_m$. Let the classification algorithm be defined $A(f_{X_{m}}, x)$, where for $x \in X_u$, $A(f_{X_{m}}, x)$ is the guess for the class label of input $x$, and $f_{X_{m}}(x) = f(x)$ where $x\in X_m$. Let the loss function be defined,
%
\begin{align}
    L_A(f, X_m) = \frac{1}{X_u} \sum_{x\in X_u} 1_{A(f_{X_m}, x) \neq f(x)},
\end{align}
%
where $1_{t}$ is the indicator function, returning 1 when $t$ is true, and 0 otherwise. The loss function measures the number of misclassifications on the training data of the classification algorithm. The expected loss on all functions $\mathcal{M}: X \to Y$ is then,
%
\begin{align}
    E\left[ L_A\left(P, X_m\right) \right] = \sum_{f\in \mathcal{M}} P(f)L_A(f, X_m)
\end{align} 
%
where $P$ is a probability distribution on $\mathcal{M}$.

\begin{theorem}[No Free Lunch]\label{NFL}
Let $P$ be a uniform probability distribution on $\mathcal{M}$. For any algorithm $A$ and training data $X_m\subseteq X$,
\begin{align}
    E\left[ L_A\left(P, X_m\right) \right] = | Y - 1 |/|Y|,
\end{align}
\end{theorem}

\ie no algorithm can do better than random on all possible problems, given only the training data with no other biases.

The NFL theorem leads many to claim that there is no such thing as a universal learner, however it is important to note that in practice the data distributions that we are interested in represent a small number of all possible data distributions, so it's not inconceivable that a learning algorithm with assumptions may be able to do well on many different real world datasets. Formalizing this argument, \citet{lattimore2013no} claim that without contradicting NFL, biased learning algorithms may exist that do well on universal problems. Their main insight is that the assumption of a uniform probability function $P$ on data distributions may be unrealistic, and under other distributions, a biased learner may do well even averaged over all distributions.

\subsubsection{No Free Lunch and Structural Priors}
The NFL theorem highlights the need to focus on designing learning algorithms that work well for the real-world data distributions of interest. At first this may be seem to be the \emph{raison d'\^{e}tre} of structural priors, however this discounts the generality of the theorem. All neural networks\index{neural network} make implicit assumptions even without structural priors. For example, using a single hidden layer neural network\index{neural network} assumes that the problem being learned is not linear. If the problem is linear, instead a single-layer network would suffice, and likely generalize better. The NFL is, however, important in pointing out that structural priors in neural networks\index{neural network} are not a `hack'. In fact well-informed assumptions are the reason machine learning works in practice.

\section{Bayesian Model Selection}
Model selection is the problem of choosing, with generalization in mind, between several model architectures with different numbers of layers, neurons, etc. 
\citet{MacKay91,mackay1992practical,mackay1995} proposed a Bayesian approach to model selection\index{Bayesian model selection|textbf}. \citeauthor{mackay1992practical} points to the relationship of \gls{occam} to Bayesian approaches and, rather than performing model selection by using cross-validation, \citeauthor{mackay1992practical} proposes to predict the likelihood of models given only the training data. Assume we have several trained models $\mathcal{M}_i$, and a training dataset $\gls{X}$. In the problem of model selection we are interested in solving:
\begin{equation}
	\argmax_i p(\mathcal{M}_i|\gls{X}),
\end{equation}
where $p(\mathcal{M}_i|\gls{X})$ is the probability of the model conditioned on the training data.

The basic principle behind Bayesian model selection\index{Bayesian model selection} is explained by Bayes' rule. For a neural network model $\mathcal{M}_i$ with parameters $\mathbf{\theta}$,
\begin{equation}
	p(\mathbf{\theta}|\mathcal{M}_i,\gls{X}) = \frac{p(\gls{X}|\mathbf{\theta},\mathcal{M}_i)\,p(\mathbf{\theta}|\mathcal{M}_i)}{p(\gls{X}|\mathcal{M}_i)},
\end{equation}
where $p(\mathbf{\theta}|\mathcal{M}_i,\gls{X})$, or the \emph{posterior}\index{posterior}, is what we commonly attempt to solve for using gradient descent when training a neural network.
The \emph{prior}\index{prior}, $p(\mathbf{\theta}|\mathcal{M}_i)$, are the probabilities on what values we expect the weights of the model to be. The normalization term, $p(\gls{X}|\mathcal{M}_i)$, is the probability of the data given the model, or `evidence', for the model.

In the case of model selection, we are interested in,
\begin{equation}
	p(\mathcal{M}_i|\gls{X}) = \frac{p(\gls{X}|\mathcal{M}_i)\,p(\mathcal{M}_i)}{p(\gls{X})},
\end{equation}
where $p(\mathcal{M}_i)$ is the prior (subjective) probability we assign to model $\mathcal{M}_i$. If all models are considered to be of equal probability, then inference is based wholly on the evidence.

The evidence itself is given by the marginalization,
\begin{equation}
	p(\gls{X}|\mathcal{M}_i) = \int p(\gls{X}|\mathbf{\theta},\mathcal{M}_i)\,p(\mathbf{\theta}|\mathcal{M}_i)\,d\mathbf{\theta}.
\end{equation}
\Citet{mackay1992practical} proposes to use the Hessian\index{Hessian} to evaluate the curvature of the error surface around the point on the error surface represented by the parameters learned in the model, in order to gain knowledge of the uncertainty of the parameters, \ie $p\left(\mathbf{\theta}|\gls{X},\mathcal{M}_i\right)$, however as explained in \cref{pathological}, the computation or storage of the Hessian\index{Hessian} for contemporary deep networks is infeasible.

\subsection{Occam's Razor}
Of particular interest to the question of the effect of structure and regularization on generalization, are \citeauthor{MacKay91}'s empirical results showing that \gls{occam} is very much a principle on which neural network design should be structured. \citet[\textsection{3.4}]{MacKay91}, shows that regularization is not enough to make an over-parameterized network generalize as well as a network with a more appropriate parameterization.

In fact, \citeauthor{mackay1992practical} makes the argument that the Bayesian approach naturally encodes the \gls{occam} approach, since given two models that predict the training data, the simpler model with fewer parameters will have less flexibility. This means that the model will explain only a more narrow range of data points (and probably only those), as compared to a more complex model which, with more parameters, will be able to explain a wider range of data. Given that both of these models are assigning probabilities, the simpler model will necessarily assign higher probability to the narrow range in which the data of interest lies\footnote{Figure 1 in \citet{mackay1995} explains this concept particularly well}.

\subsection{Practical Implementation}
The practical implementation of this method is rather more difficult~\citep{chipman2001} than might be expected. In particular, like all Bayesian approaches, having the correct prior probabilities for both the models and weights is important, and yet this is practically difficult in large neural networks.

Besides the difficulty of assigning correct priors, \citet{mackay1992practical} proposes to use the Hessian\index{Hessian} to evaluate the curvature of the error surface around the point on the error surface represented by the parameters learned in the model, in order to gain knowledge of the uncertainty of the parameters, \ie $p(\mathbf{\theta}|\gls{X},\mathcal{M}_i$. As explained in \cref{pathological}, the Hessian is difficult to represent for even reasonably-sized contemporary \glspl{dnn}.

\section{Constructive Neural Network Algorithms}
	%http://blog.otoro.net/2016/09/28/hyper-networks/
	Given the effect of structure and network design on generalization, what if instead of designing a neural network\index{neural network} before training, we could build networks from the ground up based on data? This appealing direction of research was prominent in the late 80's and early 90's, but has started to regain focus. 	
	\subsection{Perfect Binary Training Classification}
Constructive approaches to binary classification neural network architectures were the approach taken by \citet{mezard1989learning,Fahlman1989,frean1990upstart} among others. A survey of such methods is presented by \citet{parekh2000constructive}. These methods share one issue however, they  present algorithms to build neural networks that can classify a binary training set \emph{perfectly}, as pointed out by~\citet{Bishop1995}. Rather, our interest is usually in a classifier that generalizes well --- neural networks that classify the training set perfectly are likely instead to have overfit the training set.
	
	\subsubsection{Tiling Algorithm}\label{tilingalgo}
	The earliest proposals for building neural network\index{neural network} structures from the data itself were proposed for \emph{boolean classification} networks, \ie classifying binary patterns and returning a binary result. The \emph{tiling algorithm}\index{tiling algorithm|textbf}~\citep{mezard1989learning} is guaranteed to build a network that can achieve perfect classification of the training set. For each layer in the network, a master neuron\index{neuron} is first used to provide the best linear separation of the data (trained using the \emph{pocket algorithm}\index{pocket algorithm|textbf}~\citep{gallant1986optimal}), and then ancillary neurons\index{neuron} are added until perfect classification of the training data is achieved. 
	
	\subsubsection{Cascade Correlation}
	Cascade correlation\index{cascade correlation|textbf}~\citep{Fahlman1989} is a constructive algorithm for building general neural networks\index{neural network} from only a basic two-layer network of input and output nodes. It is based on two key ideas, a \emph{cascade} where hidden units are added to the network one at a time, and \emph{correlation}\footnote{In fact it is covariance that is used rather than correlation, since no normalizing factors are used, as noted by \citet{Fahlman1989}.} where the output of the new hidden unit is learned such that it is highly correlated with the error residual being minimized in the existing network.
	
	% \begin{figure}[tbp]
	% 	\centering
	% 	\includegraphics{Figs/PDF/fahlmancascade}
	% 	\caption[Cascade correlation]{\textbf{Cascade Correlation.} Connections marked with a $\square$ are frozen, while $\times$ connections are learned continuously. \copyright \citet{Fahlman1989}}
	% 	\label{fig:cascadecorrelation}
	% 	\mynote{TODO: replace this with our own figure, or remove?}
	% \end{figure}
	
	At the beginning of training, only the input and output layers exist,
	% as shown in \cref{fig:cascadecorrelation} %
	and since this is a single-layer network with no hidden units, any of the simpler perceptron training rules can be used. After sufficient training, which is heuristically determined, the network is expanded by a single new hidden unit at a time.
	Each new (candidate) hidden unit that is added to the network is connected to all of the network inputs, in addition to all of the outputs of the previous hidden units. The candidate unit is treated as a new hidden layer, and since all other connections are frozen, can be trained as if it was a single layer --- \ie with the delta rule\index{delta rule}, or other similar single-layer training algorithm, rather than backpropagation\index{backpropagation}. The candidate unit's input weights are adjusted so as to maximize the covariance of the candidate unit's output with the error output of the network:
	% http://www.ra.cs.uni-tuebingen.de/SNNS/UserManual/node167.html#SECTION001081200000000000000
	\begin{equation}
	    C = \sum_o \left|\sum_p (y_{po} - \overline{y_o})(E_{po} - \overline{E_o})\right|,
	\end{equation}
	where $y_{po}$ is the output of the output unit $o$ with input pattern $p$, and $E_{po}$ is the error. The trained unit's input weights are then frozen after training, and if an error threshold hasn't been met, a new layer is again added.
	
	Since each new neuron\index{neuron} produces a new layer, cascade correlation\index{cascade correlation} leads to very large \glspl{dnn}\index{DNN} with low throughput, since each layer is only one neuron\index{neuron} wide, making them very inefficient computationally. Due to their greedy training strategy, cascade correlation\index{cascade correlation} networks are trained to perfectly fit the training data, and thus typically overfit. This means they are not likely to generalize as well as standard neural networks\index{neural network} trained with backpropagation\index{backpropagation}, and where no weights are frozen during training.

	\subsubsection{Upstart Algorithm}
	The \emph{upstart algorithm}\index{upstart algorithm|textbf} is a method for building binary classification networks, and is guaranteed to find an architecture with perfect training classification. At a high level it sounds like the tiling algorithm\index{tiling algorithm} of \cref{tilingalgo}, but instead of building up layers from the input, in the upstart algorithm\index{upstart algorithm} all of the neurons in the network are connected directly to the inputs, and new `child' neurons are added to compensate for the misclassification errors of existing `parent' neurons.
	
	Initially the network consists of one neuron that is trained with the \emph{pocket algorithm}\index{pocket algorithm}~\citep{gallant1986optimal}. This `parent' neuron, now frozen, will misclassify some of the training samples and, to correct these, two `child' neurons are added. Since the neurons in the upstart algorithm are binary threshold units, the `parent' neuron will either misclassify samples as negative or positive. The two `child' neurons are then trained (also by the pocket algorithm\index{pocket algorithm}) to provide enough of a negative/positive signal to cause the parent neuron to give correct classifications for one or more of the misclassified samples. The `child' neurons become `parents', and their offspring also learn to correct their mistakes. Empirical results showed that the upstart algorithm produces networks with fewer neurons than the tiling algorithm\index{tiling algorithm}.
	
%	\subsection{Hyper-Networks}
%	More recently \citet{hypernetworks} looked at trying to learn a compact representation of a network, much like weight sharing in an \gls{rnn} allows the concise summary of a potentially infinite depth \gls{rnn}.
	
	\section{Pruning Algorithms}
	Rather than the progressive construction of networks, an alternative approach to learning efficient networks is \emph{pruning}, the removal of unimportant network weights after training.

	Amongst the earliest work\footnote{Rumelhart is noted by \citet{hanson1989comparing} to have worked on a similar, albeit unpublished, method.} on pruning was by \citet{sietsma1988neural}, who investigated the effect of pruning weights in very small perceptron networks. The pruning rules were summarized as ``If the output of a unit does not change for any input pattern that unit is not contributing to the solution. If the outputs of any two units are the same or opposite across all patterns the two units duplicate and one can be removed.''~\citep{sietsma1988neural}. Even in these small networks, the complexity of pruning emerges. The authors propose removing weights close to zero, or weights that similarly do not affect the output of the neuron, and removing complementary but opposite (\ie duplicate) weights. %It is claimed that aside from being faster, pruned networks are also less noise-sensitive.
	
	There have been many proposals of methods of pruning network connections since~\citep{hanson1989comparing, lecun1989optimal, mozer1989using, mozer1989skeletonization, gorodkin1993quantitative, setiono1997penalty,castellano1997iterative,han2015learning,han2015deep,han2016dsd,ullrich2017soft,} differing mostly on the method of evaluating the saliency of weights, \ie how important each weight is to maintaining generalization.  Here we will cover only a small selection of the most relevant methods.

	As demonstrated even in the early approach of \citeauthor{sietsma1988neural}, a naive saliency measure, such as weight magnitude, is overly simplistic in practice since it does not account for the larger distributed representation learned in neural networks\index{neural network} and can lead to a large reduction in accuracy if used for pruning.

    In a typical pruning algorithm, after having trained the network, a saliency measure for each network weight is calculated. The weights are then sorted by saliency, and the lowest saliency parameters are deleted. After this pruning step, the network must again be re-trained from scratch. This iterative re-training means that such pruning methods do not scale well to contemporary neural networks\index{neural network} where training times can be measured in weeks.

	The simplest saliency measure would be to simply measure the effect of removing a weight on the training error of the network. For each weight in the network, the training error can be evaluated, and the difference in error used as the saliency measure for that weight. In practice this straight-forward method is infeasible however, since we must evaluate over the training set for each weight of the network. For a network with $n$ weights, and $p$ training samples, calculating the saliencies for the whole network in this manner is $\gls{bigoh}(np)$~\citep{hanson1989comparing}.
	
	\subsection{Optimal Brain Damage}
	\citet{lecun1989optimal} proposed \gls{obd} to iteratively remove neurons\index{neuron} after training based on a saliency measure that judges the parameters that have the least effect on training error. The authors point out that simply using the magnitude of the weights themselves is equivalent in the limit to having trained with a form of non-proportional weight decay, instead the authors propose a more theoretically justified saliency measure --- they propose measuring the change in the objective function caused by deleting a parameter. Rather than performing the computationally intensive task of re-evaluating the objective for every possible parameter deletion, the second derivative of the objective with respect to the weights is instead used. The authors approximate the derivative of an objective function with respect to the weights using the \engordnumber{2}-order Taylor series expansion, $f(a) = \sum_{n=0}^{2} \frac{f^{(n)}(a)}{n!} (x - a)^n$, of the error:
    % From optimal brain surgeon paper
    % https://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf
    % also look at:
    % http://cslt.riit.tsinghua.edu.cn/mediawiki/images/9/9e/131125-OBD-LC-01.pdf
    % http://web.engr.oregonstate.edu/~tgd/classes/534/slides/part10.pdf
    \begin{align}
       \delta \gls{E} &= \sum_i \frac{\partial \gls{E}}{\partial \gls{w}_i} \delta \gls{w}_i + \frac{1}{2} \sum_i \frac{\partial^2 \gls{E}}{\partial \gls{w}_i^2} \delta \gls{w}_i^2 + \frac{1}{2} \sum_{i\neq j} \frac{\partial^2 \gls{E}}{\partial \gls{w}_i \partial \gls{w}_j} \delta \gls{w}_i \delta \gls{w}_j +\gls{bigoh}(\|\delta \gls{vectorw}\|^3),
    \end{align}
	%
    where $\gls{E}$ is the objective function, $\gls{w}_i$ is the $i^{\textrm{th}}$ weight, and here $\delta$ represents a finite difference rather the `delta' defined in \cref{deltarule}. We can write this more compactly in matrix notation,
    \begin{align}
       \delta \gls{E} &= \left( \frac{\partial \gls{E}}{\partial \gls{vectorw}} \right)^T \delta \gls{vectorw} +\frac{1}{2}\delta \gls{vectorw}^T \gls{hessian}\, \delta \gls{vectorw} +\gls{bigoh}(\|\delta \gls{vectorw}\|^3),
    \end{align}
	where $\gls{vectorw}$ is the vector of network weights, and %$h_{ij}=\partial^2 \gls{E} / \partial \gls{w}_i \partial \gls{w}_j$
	$\gls{hessian}$ is the Hessian\index{Hessian} matrix of \engordnumber{2}-order derivatives. 

    This approximation is infeasible to calculate in practice, mostly due to the size of the Hessian\index{Hessian} matrix which is square in the number of weights. The authors use several assumptions to approximate this in a more computationally efficient manner. Most importantly they use the ``diagonal approximation'' of the Hessian\index{Hessian}, using only the diagonal terms, $h_{i,i}=\partial^2 \gls{E} / \partial \gls{w}_i^2$. Further, the ``extremal approximation'' assumes that pruning is only done once the training has converged, and so it can be assumed that the first term is zero. Finally they assume that the function is approximately quadratic, and thus higher-order terms (\ie $\gls{bigoh}(\|\delta \gls{vectorw}\|^3)$) can be ignored. In all, the final approximation of the objective function's change is simply:

    \begin{align}
       \delta \gls{E} &\approx \frac{1}{2} \sum_i \frac{\partial^2 \gls{E}}{\partial \gls{w}_i^2} \delta \gls{w}_i^2.
    \end{align}
	%
    To calculate the second derivative, the chain rule is used, and the second derivatives are back-propagated through the network. This approximation is used directly as the saliency measure for each network weight $\gls{w}_i$, and after having trained the network, the weights are sorted by saliency, and the lowest saliency parameters are deleted.
	
	%\subsection{Gating}
    %\citet{hanson1989comparing} proposed using a per-neuron\index{neuron} gating mechanism to measure saliency
    
	%,Xing2009??

	\subsection{Learning both Weights and Connections}
	\citet{han2015learning} begins by pruning a trained model using a naive weight magnitude saliency, and then re-training the network in a layer-wise manner. Although the novelty of this method is arguable given the past work on pruning, it presents results on contemporary deep networks. Despite using a naive weight magnitude-based saliency the results are reasonable with a 9-fold decrease in parameters, with only a small loss in accuracy. However, the experiments were performed on AlexNet which had a very large fully-connected layer composing most of its parameters.

	\section{Compression \& Quantization}
	In order to use \gls{dnn}\index{neural network} on embedded devices, and in many applications, first a more compact representation and perhaps even faster inference is often required. Although mostly a matter of engineering, compression of neural networks\index{neural network} also tells us something about the information-theoretic capacity of the models we train, and therefore gives us hints about the internal representation learned in \glspl{dnn}\index{DNN}. There have been many papers on the topic \citep{han2015deep,han2015learning,han2016dsd,Kim2016,rastegari2016xnor,ullrich2017soft}, here we will only cover a few of the most relevant papers.

	\subsection{Deep Compression}
	\citet{han2015deep} extends the method of \citet{han2015learning} with both quantization\index{quantization} and Huffman coding. The quantization\index{quantization} uses a look-up table to store a limited number of weights (within each layer) which are indexed by multiple weights --- in effect a form of weight sharing. Interestingly the model is further trained after the quantization, using backpropagation with the quantized weights. Finally, the resulting quantized model is compressed losslessly using a Huffman encoding. The results are impressive, a large VGG-16 model of 552MB parameters is reduced in size to 11.3MB, a decrease by a factor of 40.

	\subsection{Deep-Sparse-Dense Training}
	One of the most interesting recent papers on pruning is that of \citet{han2016dsd} who do something quite different. The authors propose to train a model in distinct initial, sparse and dense steps:
	\begin{description}
		\item[Intial] Train a \gls{dnn} as normal,
		\item[Sparse] Next prune\index{pruning} the network, as in \citet{han2015learning}, with the difference that a different threshold is learned for each layer. The network is then retrained with the pruned connections removed, recovering the original accuracy of the initial network. The authors claim this is akin to a `regularization' of the network,
		\item[Dense] Finally the pruned connections are re-introduced and initialzed to zero\footnote{This is considered to be a bad initialization when training neural networks from scratch due to the requirement to break symmetry, yet the authors do not explain the use here. Presumably, because there are already non-zero weights in the network however, symmetry is not as big of an issue.}, and the network is \gls{finetuned}\index{finetuning} at $1/10$ the original learning rate.
	\end{description}

	The results of this training method are that, even in an efficient large-scale deep network trained on \gls{ilsvrc}\index{ILSVRC} such as \gls{googlenet}\index{GoogLeNet}, the top-5 error is decreased by almost 1\%, which is significant.
	
	\subsection{XOR-Net: Binary Convolutional Neural Networks}
	\citet{rastegari2016xnor} proposed two methods, one in which filter parameters were quantized to binary values, from the typical 16-bit floating point representation. Surprisingly even when the method was applied to models trained on \gls{ilsvrc}, such as \gls{alexnet}, the accuracy was little effected.

	A further method, XNOR networks\index{XNOR networks}, is proposed where both the filter parameters and \glspl{featuremap} are reduced binary representations. XNOR networks claim to be 58$\times$ faster and have 32$\times$ smaller convolutional layers. The evaluation for \gls{ilsvrc} is based on the AlexNet architecture however, which does not represent a good accuracy/model size tradeoff compared to more recent deep network architectures.
	
	% \subsection{Soft Weight Sharing}
	% \citet{ullrich2017soft,}
	% \mynote{TODO}

\section{Structural Priors}
	\Cref{background,motivation} have shown that the concept we have denoted `structural priors' is not new, and has varied interpretations. Here we will attempt to summarize the concepts related to regularization, network architecture, and generalization covered by this large body of work in a compact nomenclature, much like \citet{rethinking2016} attempted to define types of regularization. While, like with any compact nomenclature, there is a danger of overly simplifying the details, it can also help in seeing the big picture.
    
	To return to the example of fitting a polynomial curve presented in \cref{modelsize}, in practice when fitting such a curve, we have little idea of which order polynomial would best fit the data. Necessarily, we must use a relatively higher-order curve to fit the data. Similarly, with a neural network\index{neural network}, we rarely have knowledge of the underlying structure of the solution (but when we do, we should use it to parametrize our models appropriately~\citep{jain2016structural}). Instead we must use networks with more parameters than necessary to ensure that there is enough capacity to learn the underlying, but likely sparse solution. The problem with this approach is that over-parametrization of a model generally leads to poor generalization due to overfitting. To prevent this, there are two general types of methods in which we can relate our prior knowledge that the model is over-parametrized to the optimization:
	\begin{description}
	\item[Weak Structural Prior: Regularization]
	Knowing only that our model is over-parametrized is a relatively weak prior, however we can encode this into the fit by using a regularization penalty. This restricts the model to effectively use only a small number of the parameters by adding a penalty, for example on the $\ell_2$ norm of the model weights. For polynomial regression, this is called \emph{ridge regression}, while for neural networks it is called \emph{weight decay}~\citep{hinton1987learning}. In neural networks, early stopping\index{early stopping} during training is another method for doing this.
	
	\item[Strong Structural Prior: Restricted Connectivity]
	With more prior information on the task, \ie when fitting a polynomial, we may ascertain that a certain order polynomial is more appropriate from the convexity of the polynomial, and restrict learning to that order. For example, given that samples from a polynomial appear to be convex, we can surmise that the polynomial is likely to be of an even or \engordnumber{2}-order, and restrict our fit to be of that order. 

	In neural networks, as we have seen in \cref{cnns}, a similar effect can be achieved by \emph{removing parameters} that we know are not needed (\ie  in \glspl{cnn}\index{CNN}, using filters with local connectivity), or \emph{sharing parameters} we know are redundant (\ie in \glspl{cnn}\index{CNN}, using the same filters for all pixels).
    \end{description}
    
	Although neural networks\index{neural network} are usually posed as general learning machines, time and again it has been demonstrated that they only truly stand out as a learning method when we use strong structural priors, encoding our prior knowledge of the task in the architecture itself. As observed by \citet{denker1987large} this may be considered closer to modifying the problem to be solved, rather than changing the learning method. For example, by asking a \gls{cnn} to learn to classify a dataset, we are asking the network to ``classify these images'', whereas by asking a fully-connected network to classify the same dataset, we are asking the network ``classify this data''. The first task is inherently easier because of the assumptions it makes.

    Structural priors may be considered one of the greatest contributions to the success of deep learning, but arguably can also be considered the cause of its greatest failure. Structural priors allow deep models to more easily learn specific tasks, \ie image classification or speech recognition. At the same time, by specializing our networks, we are moving further away from the goal of general artificial intelligence\index{general AI}, \ie learning models that can do both tasks.
    
%\section{Other Notable Works}
%\subsection{Pruning}
%\citep{mozer1989using, mozer1989skeletonization, castellano1997iterative,gorodkin1993quantitative,han2016dsd,han2015learning,setiono1997penalty,}

%\subsection{Network Size and Generalization}
%\citep{denker1987large, giles1987learning, hinton1987learning, lecun1989generalization, ahmad1989scaling,hanson1989comparing,}

%\subsection{Structural Priors}
%\citep{Fuk80,lecun1989backpropagation,}

%\subsection{Compression}
%\citep{han2015deep,Kim2016,ullrich2017soft,}

%\subsection{TODO}
%\citep{schwartz1990exhaustive,}

%\mynote{Use these references!}
    % \section{Quotes}
    
	% \begin{chapquote}{David Rumelhart, \textit{in personal communication with \citet{hanson1989comparing}, 1987}}
	% ``\ldots the simplest most robust network which accounts for a data set will, on average, lead to the best generalization to the population from which the training set has been drawn''
	% \end{chapquote}
	
	
%	\begin{chapquote}{John Denker \etal, \textit{Large Automatic Learning, Complex Systems, 1987}}
%		``A general tabula rasa network is a fine subject for the abstract, formal studies, but one should not try to use it to solve practical problems. \ldots One should pre-program the network with all available information about the structure of the problem, especially information about the symmetry and topology of the data.''
%	\end{chapquote}
	
	
	% \begin{chapquote}{Mozer \etal, \textit{Using Relevance to Reduce Network Size Automatically, 1989}}
	% ``One thing that connectionist networks have in common with brains is that if you open them up and peer inside, all you can see is a big pile of goo.''
	% \end{chapquote}
	
	
	% \begin{chapquote}{David MacKay, \textit{A Practical Bayesian Framework for Backprop Networks, 1991}}
	% 	``There are many knobs on the black box of `backprop' (learning by backpropagation\index{backpropagation} of
	% 	errors). Generally these knobs are set by rules of thumb, trial and error, and the use of reserved test data to assess generalization ability (or more sophisticated cross-validation).''
	% \end{chapquote}
	
    
	% \begin{chapquote}{Kunihiko Fukushima, \textit{Neocognitron, 
	% 			%A Self-organizing Neural Network Model
	% 			%for a Mechanism of Pattern Recognition
	% 			%Unaffected by Shift in Position , 
	% 			Biol.\ Cybernetics, 1980}}
	% 	`` One of the largest and long-standing difficulties in designing a pattern-recognizing machine has been the problem how to cope with the shift in position and the distortion in shape of the input patterns. The Neocognitron proposed in this paper gives a drastic solution to this difficulty.
	% 	''
	% \end{chapquote}
	
%	\begin{chapquote}{Hubel \& Wiesel ,\textit{Receptive Fields of Single Neurones in the Cat's Striate Cortex, 1959}}
%	``The phenomena of summation and antagonism within receptive fields seem
%to provide a basis for the specificity of stimuli, in shape, size and orientation.
%Units activated by slits and boundaries may converge upon units of higher
%order which require still more complex stimuli for their activation. Most units
%presented in this paper have had receptive fields with clearly separable
%excitatory and inhibitory areas. However, a number of units recorded in the
%striate cortex could not be understood solely in these terms. These units with
%more complex properties are now under study.''
%	\end{chapquote}

	% \begin{chapquote}{Yann LeCun, \textit{Backpropagation Applied to Handwritten Zip Code Recognition, 1989}}
	% 	``Classical work in visual pattern recognition has demonstrated the advantage of extracting local features and combining them to form higher-order features. Such knowledge can be easily built into the network by forcing the hidden units to combine only local sources of information. Distinctive features of an object can appear at various location on the input image. Therefore it seems judicious to have a set of feature detectors that can detect a particular instance of a feature anywhere on the input place. Since the precise location of a feature is not relevant to the classification, we can afford to lose some position information in the process. Nevertheless, approximate position information must be preserved, to allow the next levels to detect higher-order, more complex features.''
	% \end{chapquote}
	
	
% 	\mynote{Fit this into our talk of learning basis functions/filters!}
% 	\begin{chapquote}{Chris Bishop, \textit{Neural Networks for Pattern Recognition, Page 98, 1996}}
% 	    ``We have already seen that a network with a single-layer of weights has very
% limited capabilities. To improve the performance of the perceptron, Rosenblatt
% used a layer of fixed processing elements to transform the raw input data, as
% shown in Figure 3.10. These processing elements can be regarded as the basis
% functions of a generalized linear discriminant.''
% 	\end{chapquote}
\end{document}
