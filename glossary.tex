% From here: https://en.m.wikibooks.org/wiki/LaTeX/Glossary
% Example use:
%\newdualentry{OWD} % label
%{OWD}            % abbreviation
%{One-Way Delay}  % long form
%{The time a packet uses through a network from one host to another} % description
\usepackage{xparse}
\DeclareDocumentCommand{\newdualentry}{ O{} O{} m m m m } {
  \newglossaryentry{gls-#3}{name={#5},text={#5\glsadd{#3}},
    description={#6},#1
  }
  \makenoidxglossaries
  \newacronym[see={[Glossary:]{gls-#3}},#2]{#3}{#4}{#5\glsadd{gls-#3}}
}
%https://tex.stackexchange.com/questions/127648/how-can-i-add-every-glossary-entry-to-the-index#127671
%\defglsdisplayfirst[\acronymtype]{#1#4\index{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% GLOSSARY
\newdualentry{cnn}{CNN}{Convolutional Neural Network}{A neural network designed for learning representations of image inputs, with shared parameters in the form of a set of convolutional filters}
\newdualentry{dnn}{DNN}{Deep Neural Network}{A neural network with two or more hidden layers}
\newdualentry{rnn}{RNN}{Recurrent Neural Network}{A neural network designed for sequences, with shared parameters in the form of a recurrence}
\newdualentry{cifar}{CIFAR}{Canadian Institute for Advanced Research}{Government agency behind the funding of several prominent researchers in Canada, notably the lab of Geoffery Hinton who released two popular datasets, \acrshort{cifar10} and CIFAR-100}
\newdualentry{mnist}{MNIST}{Modified National Institute of Standards and Technology}{Dataset of handwritten numerical digits commonly used as a 60,000 image training/10,000 image testing dataset for machine learning algorithms}
\newdualentry{mlp}{MLP}{Multi-layer Perceptron}{An established misnomyer for a neural network with one or more hidden layers, not related to a Perceptron}
\newdualentry{vgg}{VGG}{Visual Geometry Group}{A research group at the University of Oxford from which the popular VGG network architecture was proposed by \citet{Simonyan2014verydeep}}
\newdualentry{nin}{NiN}{Network in Network}{A neural network architecture proposed by \citet{Lin2013NiN} which introduced \acrshort{gap} and \acrshort{lde}}
\newdualentry{blas}{BLAS}{Basic Linear Algebra Subprograms}{A common \acrshort{api} for accelerating linear algebra operations, notably matrix multiplication, on hardware. Typically a heavily optimized version is provided by the hardware company.}
\newdualentry{cublas}{CuBLAS}{CUDA BLAS}{Nvidia's implementation of \acrshort{blas} for the \acrshort{cuda} \acrshort{gpu} programming \acrshort{api}}
\newdualentry{cuda}{CUDA}{CUDA BLAS}{Nvidia's \acrshort{gpu} programming \acrshort{api}}
\newdualentry{mkl}{MKL}{Intel's Matrix Kernel Library}{\Gls{blas} implementation for Intel \glspl{cpu}}

\newglossaryentry{cifar10}{name=CIFAR-10, description={An image recognition dataset funded by \acrshort{cifar} and created by \citet{CIFAR10} consisting of 60,000 32$\times$32 colour images of 10 classes of objects}}
%\newglossaryentry{cifar100}{name=CIFAR-100, description={An image recognition dataset created by \citet{CIFAR10} consisting of 60,000 32$\times$32 colour images of 100 classes of objects}}
\newglossaryentry{alexnet}{name=AlexNet, description={A neural network architecture proposed by \citet{Krizhevsky2012} that revolutionized computer vision, and renewed interest in neural networks}}
\newglossaryentry{googlenet}{name=GoogLeNet, description={A neural network architecture proposed by \citet{Szegedy2014going} and since extended in the Inception v1--4 refinements}}
\newglossaryentry{inception}{name=Inception, description={A building-block of the GoogLeNet neural network architecture designed for efficient state-of-the-art image recognition}}
\newglossaryentry{resnet}{name=ResNet, description={Residual network, a network architecture proposed by \citet{He2015} that uses residual connections to improve generalization and training of very deep architectures.}}
\newglossaryentry{structuralprior}{name=Structural Prior, description={The encoding of prior knowledge of the problem into a neural network by architecture design, \ie a \acrshort{cnn}, what some might call ``infinitely strong regularization''~\citep{goodfellow2016deep}}}
\newglossaryentry{cudnn}{name=CuDNN, description={Nvidia's Deep Neural Network acceleration library}}
\newglossaryentry{occam}{name={Occam's razor}, description={A general principle in hypothesis selection; given several hypothesis that match the evidence, the simplest hypothesis, \ie{}the one with the least assumptions, should be selected}}
\newglossaryentry{finetuned}{name={finetuned}, description={A method of continuing the training of a pre-trained network, with varied definitions. Typically a subset or all of the layers of a pre-trained \gls{dnn} are trained at a greatly reduced learning rate}}
\newglossaryentry{featuremap}{name={feature map}, description={The input/output of a convolutional layer, a 3D tensor with two spatial dimensions and a third dimension corresponding to the output image from a single convolutional filter}}
\newglossaryentry{filter}{name={filter}, description={A convolutional filter, or kernel, of spatial dimensions $w$$\times$$h$, and depth $c$, where $c$ is the number of channels in the input \gls{featuremap}}}
\newglossaryentry{regularization}{name={regularization}, description={a broadly-used, but relatively ill-defined term --- often its usage implies a definition of `anything that improves generalization', instead we define regularization as any modification of the training algorithm that modifies, explicitly or implicitly, the error surface such that it is smoother, the prototypical method being weight decay\index{weight decay}~\citep{hinton1987learning}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ACRONYMS
\newacronym{relu}{ReLU}{Rectified Linear Unit}
\newacronym{pca}{PCA}{Principled Component Analysis}
\newacronym{gpu}{GPU}{Graphical Processing Unit}
\newacronym{cpu}{CPU}{Computer Processing Unit}
\newacronym{rgb}{RGB}{Red-Green-Blue}
\newacronym{ilsvrc}{ILSVRC}{Imagenet Large-Scale Visual Recognition Challenge}
\newacronym{sgd}{SGD}{Stochastic Gradient Descent}
\newacronym{msr}{MSR}{Microsoft Research}
\newacronym{cvpr}{CVPR}{Computer Vision and Pattern Recognition}
\newacronym{iclr}{ICLR}{International Conference on Learning Representations}
\newacronym{lde}{LDE}{Low-Dimensional Embeddings}
\newacronym{gap}{GAP}{Global Average Pooling}
\newacronym{gmp}{GMP}{Global Max Pooling}
\newacronym{vc}{VC}{Vapnik-Chervonenkis}
\newacronym{nfl}{NFL}{No Free Lunch theorem}
\newacronym{zca}{ZCA}{Zero Component Analysis}
\newacronym{ma}{MA}{Multiply-Accumulate}
\newacronym{flops}{FLOPS}{Floating Point Operations}
\newacronym{api}{API}{Application Programming Interface}
\newacronym{doi}{DOI}{Digital Object Identifier}
\newacronym{sift}{SIFT}{Scale-Invariant Feature Transform}
\newacronym{ltu}{LTU}{Linear Threshold Unit}
\newacronym{obd}{OBD}{Optimal Brain Damage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% SYMBOLS
\glsxtrnewsymbol[description={input of a neuron}]{x}{\ensuremath{x}}
\glsxtrnewsymbol[description={output of a neuron}]{y}{\ensuremath{y}}
\glsxtrnewsymbol[description={bias of a neuron}]{b}{\ensuremath{b}}
\glsxtrnewsymbol[description={weight of a neuron}]{w}{\ensuremath{w}}
\glsxtrnewsymbol[description={net activation of a neuron (see \cref{eqn:weightsum})}]{a}{\ensuremath{a}}
\glsxtrnewsymbol[description={error for an output neuron with a given sample}]{e}{\ensuremath{e}}
\glsxtrnewsymbol[description={target label for an output neuron with a given sample}]{t}{\ensuremath{t}}
\glsxtrnewsymbol[description={error function}]{E}{\ensuremath{E}}
\glsxtrnewsymbol[description={surogate loss function}]{L}{\ensuremath{\mathcal{L}}}
\glsxtrnewsymbol[description={learning rate}]{lr}{\ensuremath{\lambda}}
\glsxtrnewsymbol[description={training set}]{X}{\ensuremath{X}}
\glsxtrnewsymbol[description={activation function of a neuron}]{f}{\ensuremath{f}}
\glsxtrnewsymbol[description={vector of inputs to a neuron, \ensuremath{\mathbf{x} = \left\{x_0, x_1, \ldots\right\}}}]{vectorx}{\ensuremath{\mathbf{x}}}
\glsxtrnewsymbol[description={vector of weights for a neuron, \ensuremath{\mathbf{w} = \left\{w_0, w_1, \ldots\right\}}}]{vectorw}{\ensuremath{\mathbf{w}}}
\glsxtrnewsymbol[description={`local gradient'/`error'/delta, $\delta \equiv \frac{\partial E}{\partial a}$}]{delta}{\ensuremath{\delta}}
\glsxtrnewsymbol[description={velocity vector, used in momentum}]{velocity}{\ensuremath{\mathbf{v}}}
\glsxtrnewsymbol[description={training iteration}]{iteration}{\ensuremath{t}}
\glsxtrnewsymbol[description={height of a convolutional \gls{featuremap}}]{H}{\ensuremath{H}}
\glsxtrnewsymbol[description={width of a convolutional \gls{featuremap}}]{W}{\ensuremath{W}}
\glsxtrnewsymbol[description={height of a convolutional \gls{filter}}]{filterh}{\ensuremath{h}}
\glsxtrnewsymbol[description={width of a convolutional \gls{filter}}]{filterw}{\ensuremath{w}}
\glsxtrnewsymbol[description={channels of a convolutional \gls{filter}}]{c}{\ensuremath{c}}
\glsxtrnewsymbol[description={covariance}]{covar}{\ensuremath{\operatorname{covar}}}
\glsxtrnewsymbol[description={convolution operator}]{convolution}{\ensuremath{\mathop{\convolution}}}
\glsxtrnewsymbol[description={function composition operator}]{composition}{\ensuremath{\mathop{\circ}}}
\glsxtrnewsymbol[description={response/gradient per-layer scaling factor}]{beta}{\ensuremath{\beta}}
\glsxtrnewsymbol[description={expected value}]{expected}{\ensuremath{\mathrm{E}}}
\glsxtrnewsymbol[description={standard deviation}]{stddev}{\ensuremath{\sigma}}
\glsxtrnewsymbol[description={mean}]{mean}{\ensuremath{\mu}}
\glsxtrnewsymbol[description={normalized response}]{normx}{\ensuremath{\hat{x}}}
\glsxtrnewsymbol[description={Hessian matrix of second-order derivatives}]{hessian}{\ensuremath{\mathbf{H}}}