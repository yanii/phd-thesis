% !TEX root = thesis.tex
\documentclass[thesis]{subfiles}

\begin{document}
%*******************************************************************************
%*********************************** Epilogue *****************************
%*******************************************************************************

\chapter{Bibliographic Epilogue}
\label{epilogue}
%********************************** %First Section  **************************************
Research in deep learning has taken on a new rapidity since the adoption of pre-prints, and the explosion of interest in the field. Rather than being bound to the annual conference schedule, new research is released on a weekly basis. Presenting a paper at a contemporary conference, one is now in the odd situation of having to relate the `new' research being presented to the 6--12 months of follow-up research in the field. Compare this to even a few years ago, when publication of new research was withheld until a conference paper acceptance, perhaps a couple of months before the conference.

This dissertation represents the ultimate presentation of the research we have undertaken, and just as in a conference, it must also be presented in the context and timeline of the follow-up research and applications that it has already inspired.

In this section, we will briefly outline the significant derivative papers published after the original pre-print publication of the research we have presented here, along with their pre-print dates. For reference, the initial public release of the papers behind the work presented in this dissertation are outlined below:

\begin{itemize}
    \item Training \glspl{cnn} with Low-Rank Filters for Efficient Image Classification~\citep{Ioannou2016}
    \begin{description}
        \item[Pre-print] arXiv:1511.06744 (30 Nov.\ 2015)
        \item[Peer-reviewed publication date] May 2, 2016
    \end{description}
    \item Decision Forests, Convolutional Networks and the Models in-Between~\citep{Ioannou2015}
    \begin{description}
        \item[Pre-print] 3 Mar.\ 2016
        %\item[MSR internal technical report date] April, 2015
    \end{description}
    \item Deep Roots: Improving \gls{cnn} Efficiency with Hierarchical Filter Groups~\citep{ioannou2016e}
    \begin{description}
        \item[Pre-print] arXiv:1605.06489 (20 May 2016)
        \item[Peer-reviewed publication date] 21 July, 2017
    \end{description}
\end{itemize}

\section*{Rethinking the Inception Architecture for Computer Vision}
\begin{description}
    \item[Pre-print] arxiv:1512.00567 (2 Dec.\ 2015)
\end{description}
The Google Brain research team published an update to the \Gls{inception} architecture around 3 weeks after our publication, making it likely that their work was independent. Nevertheless, the method they present is identical to our proposal~\citep{Ioannou2016} --- the training of the \Gls{inception} architecture with low-rank (\ie 1$\times$3 and 3$\times$1) filters to reduce computation and improve generalization. They call these `factorized filters' which we disagree with, since being simply concatenated they are not a factorization (\ie separation of a multiplication), rather we argue they are linearly combined, and so represent a basis.

Nevertheless, the method proposed in this paper (and identical to our proposed method) forms the basis of all current \Gls{inception} architectures, and are now used in all Google deep learning backed products, for example \copyright{}Google Photos.

\section*{Xception: Deep Learning with Depthwise Separable Convolutions}
\begin{description}
    \item[Pre-print] arXiv:1610.02357 (7 Oct.\ 2016)
\end{description}
\mynote{TODO}

\section*{Aggregated Residual Transformations for Deep Neural Networks}
\begin{description}
    \item[Pre-print] arXiv:1611.05431 (16 Nov.\ 2016)
\end{description}
The ResNeXt architecture, as proposed in this paper, cites our work~\citep{ioannou2016e}, and while the method is identical to ours, they explore a different compute-generalization trade-off. Rather than using the better representation to save parameters and computation as in \citet{ioannou2016e}, the authors propose to maintain the original computational footprint of the model, and increase the number of filters learned.
\mynote{TODO}

\section*{Interleaved Group Convolutions for Deep Neural Networks}
\begin{description}
    \item[Pre-print] arXiv:1707.02725 (7 Oct.\ 2016)
    \item[Peer-reviewed publication date] October 22, 2017
\end{description}
\citet{zhang2017primal}
\mynote{TODO}

\section*{The Power of Sparsity in Convolutional Neural Networks}
\begin{description}
    \item[Pre-print] arXiv:1702.06257 (21 Feb.\ 2017)
\end{description}
\citet{changpinyo2017power}
\mynote{TODO}

\end{document}
