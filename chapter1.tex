\documentclass[thesis]{subfiles}

\begin{document}
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}  %Title of the First Chapter

\ifpdf
    \graphicspath{{Figs/Raster/}{Figs/PDF/}{Figs/}}
\else
    \graphicspath{{Figs/Vector/}{Figs/}}
\fi


%********************************** %First Section  **************************************
\section{On Methods of Discriminative Classification} %Section - 1.1 

Two methods of discriminative classification, \emph{Neural Networks} and \emph{Decision Forests}, have dominated Artificial Intelligence, and the sub-field of Computer Vision in particular. Much work has been done on improving both methods and exploring their applications - with much academic and even commercial success. However, the important fact that these two methods are related often seems to be all but forgotten. In the early 1990s Sethi et al.~\cite{Sethi1990} showed that any decision tree can be represented as a neural network with one hidden layer, however the converse does not necessarily hold true.

Despite this fundamental relationship, decision forests and neural networks have such distinct and mutually exclusive strengths and weaknesses that it is not surprising that they are considered themselves distinct. Decision forests require vast amounts of labelled data, proportional to the number of classes and tree depth, since samples are ``diluted'' down the tree, while, with appropriate regularization, neural networks can be trained with far more parameters than actual samples. At test time neural networks are opaque giving little understanding, while decision forests are more intuitive - each node having an explicit decision on the input data and even describe per-class statistics. The routing of decision forests makes it easy to distribute computation, while the high connectivity of neural networks makes model parallelism difficult and inefficient. Decision forests are extremely fast at test time, due to sample routing only a small part of a tree need be computed, neural networks must on the other hand compute the response at every node.  

The ideal discriminative model desired for most tasks would have all the advantages of both neural networks and decision forests, and none of the weaknesses. It would have good generalization with computational efficiency, semantic understanding with sufficient functional complexity. This proposal attempts to explore the continuum of discriminative models that exist between decision forests and neural networks to try and find such a balance.

The rest of this proposal is organised as follows: Chapter \ref{background} explores the background of decision forests, neural networks and their applications, with particular emphasis on image classification. Chapter \ref{methodology} details the methods used to train convolutional neural networks. Chapter \ref{firstyear} presents the key contribution of our first year's work, conditional networks. Chapter \ref{nexttwoyears} proposes a research plan based on further exploring methods for conditional networks, and applications thereof.

%The prototypical neural network is trained with back-propogration, a gradient based form of global optimization. Tree training is typically a greedy layer-wise construction.
%consists of several levels of nodes, each of which is fully-connected to all the nodes of a previous layer. These neurons perform   

\end{document}
